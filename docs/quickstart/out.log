Attaching to quickstart_postgres_1, quickstart_zookeeper_1, quickstart_mysql-db_1, quickstart_kafka_1, quickstart_mysql-admin_1, quickstart_schema-registry_1, quickstart_ksql-datagen-users_1, quickstart_connect_1, quickstart_ksql-cli_1, quickstart_elasticsearch_1
[33mzookeeper_1           |[0m 
[36mpostgres_1            |[0m The files belonging to this database system will be owned by user "postgres".
[33mzookeeper_1           |[0m ===> ENV Variables ...
[36mpostgres_1            |[0m This user must also own the server process.
[36mpostgres_1            |[0m 
[32mmysql-db_1            |[0m Initializing database
[33mzookeeper_1           |[0m echo "===> ENV Variables ..."
[35mkafka_1               |[0m ===> ENV Variables ...
[31mmysql-admin_1         |[0m PHP 7.0.24 Development Server started at Wed Oct 25 05:25:26 2017
[36mpostgres_1            |[0m The database cluster will be initialized with locale "en_US.utf8".
[36mpostgres_1            |[0m The default database encoding has accordingly been set to "UTF8".
[36mpostgres_1            |[0m The default text search configuration will be set to "english".
[32mmysql-db_1            |[0m 2017-10-25T05:25:24.612279Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
[35mkafka_1               |[0m 
[33mzookeeper_1           |[0m + echo '===> ENV Variables ...'
[36mpostgres_1            |[0m 
[35mkafka_1               |[0m echo "===> ENV Variables ..."
[33mzookeeper_1           |[0m env | sort
[32mmysql-db_1            |[0m 2017-10-25T05:25:25.666695Z 0 [Warning] InnoDB: New log files created, LSN=45790
[36;1mksql-datagen-users_1  |[0m Waiting for Kafka to be ready...
[36mpostgres_1            |[0m Data page checksums are disabled.
[35mkafka_1               |[0m + echo '===> ENV Variables ...'
[33mzookeeper_1           |[0m + env
[34mschema-registry_1     |[0m 
[36mpostgres_1            |[0m 
[36;1mksql-datagen-users_1  |[0m MetadataClientConfig values: 
[35mkafka_1               |[0m env | sort
[33mzookeeper_1           |[0m + sort
[33;1mconnect_1             |[0m 
[34mschema-registry_1     |[0m . /etc/confluent/docker/apply-mesos-overrides
[36mpostgres_1            |[0m fixing permissions on existing directory /var/lib/postgresql/data ... ok
[32mmysql-db_1            |[0m 2017-10-25T05:25:25.983934Z 0 [Warning] InnoDB: Creating foreign key constraint system tables.
[36;1mksql-datagen-users_1  |[0m 	bootstrap.servers = [kafka:29092]
[35mkafka_1               |[0m + env
[33;1mconnect_1             |[0m . /etc/confluent/docker/apply-mesos-overrides
[34mschema-registry_1     |[0m + . /etc/confluent/docker/apply-mesos-overrides
[36;1mksql-datagen-users_1  |[0m 	max.poll.timeout.ms = 5000
[32mmysql-db_1            |[0m 2017-10-25T05:25:26.269704Z 0 [Warning] No existing UUID has been found, so we assume that this is the first time that this server has been started. Generating a new UUID: e8338d10-b944-11e7-bfe4-0242ac150002.
[36mpostgres_1            |[0m creating subdirectories ... ok
[35mkafka_1               |[0m + sort
[33mzookeeper_1           |[0m ALLOW_UNSIGNED=false
[33;1mconnect_1             |[0m + . /etc/confluent/docker/apply-mesos-overrides
[34mschema-registry_1     |[0m #!/usr/bin/env bash
[36;1mksql-datagen-users_1  |[0m 	sasl.jaas.config = null
[32mmysql-db_1            |[0m 2017-10-25T05:25:26.316888Z 0 [Warning] Gtid table is not ready to be used. Table 'mysql.gtid_executed' cannot be opened.
[33mzookeeper_1           |[0m COMPONENT=zookeeper
[35mkafka_1               |[0m ALLOW_UNSIGNED=false
[36mpostgres_1            |[0m selecting default max_connections ... 100
[33;1mconnect_1             |[0m #!/usr/bin/env bash
[34mschema-registry_1     |[0m #
[36;1mksql-datagen-users_1  |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[33mzookeeper_1           |[0m CONFLUENT_DEB_VERSION=1
[35mkafka_1               |[0m COMPONENT=kafka
[33;1mconnect_1             |[0m #
[34mschema-registry_1     |[0m # Copyright 2016 Confluent Inc.
[36;1mksql-datagen-users_1  |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[33mzookeeper_1           |[0m CONFLUENT_MAJOR_VERSION=3
[35mkafka_1               |[0m CONFLUENT_DEB_VERSION=1
[33;1mconnect_1             |[0m # Copyright 2016 Confluent Inc.
[32mmysql-db_1            |[0m 2017-10-25T05:25:26.317697Z 1 [Warning] root@localhost is created with an empty password ! Please consider switching off the --initialize-insecure option.
[36mpostgres_1            |[0m selecting default shared_buffers ... 128MB
[34mschema-registry_1     |[0m #
[36;1mksql-datagen-users_1  |[0m 	sasl.kerberos.service.name = null
[33mzookeeper_1           |[0m CONFLUENT_MINOR_VERSION=3
[35mkafka_1               |[0m CONFLUENT_MAJOR_VERSION=3
[33;1mconnect_1             |[0m #
[36mpostgres_1            |[0m selecting dynamic shared memory implementation ... posix
[34mschema-registry_1     |[0m # Licensed under the Apache License, Version 2.0 (the "License");
[36;1mksql-datagen-users_1  |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[33mzookeeper_1           |[0m CONFLUENT_MVN_LABEL=
[35mkafka_1               |[0m CONFLUENT_METRICS_ENABLE=true
[33;1mconnect_1             |[0m # Licensed under the Apache License, Version 2.0 (the "License");
[34mschema-registry_1     |[0m # you may not use this file except in compliance with the License.
[36;1mksql-datagen-users_1  |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[33mzookeeper_1           |[0m CONFLUENT_PATCH_VERSION=0
[33;1mconnect_1             |[0m # you may not use this file except in compliance with the License.
[35mkafka_1               |[0m CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS=kafka:29092
[36mpostgres_1            |[0m creating configuration files ... ok
[34mschema-registry_1     |[0m # You may obtain a copy of the License at
[36;1mksql-datagen-users_1  |[0m 	sasl.mechanism = GSSAPI
[33mzookeeper_1           |[0m CONFLUENT_PLATFORM_LABEL=
[33;1mconnect_1             |[0m # You may obtain a copy of the License at
[35mkafka_1               |[0m CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS=1
[34mschema-registry_1     |[0m #
[36mpostgres_1            |[0m running bootstrap script ... ok
[36;1mksql-datagen-users_1  |[0m 	security.protocol = PLAINTEXT
[33mzookeeper_1           |[0m CONFLUENT_VERSION=3.3.0
[33;1mconnect_1             |[0m #
[35mkafka_1               |[0m CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT=zookeeper:32181
[34mschema-registry_1     |[0m # http://www.apache.org/licenses/LICENSE-2.0
[36;1mksql-datagen-users_1  |[0m 	ssl.cipher.suites = null
[33mzookeeper_1           |[0m HOME=/root
[36mpostgres_1            |[0m performing post-bootstrap initialization ... ok
[33;1mconnect_1             |[0m # http://www.apache.org/licenses/LICENSE-2.0
[35mkafka_1               |[0m CONFLUENT_MINOR_VERSION=3
[34mschema-registry_1     |[0m #
[36;1mksql-datagen-users_1  |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[33mzookeeper_1           |[0m HOSTNAME=zookeeper
[33;1mconnect_1             |[0m #
[35mkafka_1               |[0m CONFLUENT_MVN_LABEL=
[34mschema-registry_1     |[0m # Unless required by applicable law or agreed to in writing, software
[36;1mksql-datagen-users_1  |[0m 	ssl.endpoint.identification.algorithm = null
[33mzookeeper_1           |[0m KAFKA_VERSION=0.11.0.0
[33;1mconnect_1             |[0m # Unless required by applicable law or agreed to in writing, software
[35mkafka_1               |[0m CONFLUENT_PATCH_VERSION=0
[34mschema-registry_1     |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[36;1mksql-datagen-users_1  |[0m 	ssl.key.password = null
[33mzookeeper_1           |[0m LANG=C.UTF-8
[33;1mconnect_1             |[0m # distributed under the License is distributed on an "AS IS" BASIS,
[35mkafka_1               |[0m CONFLUENT_PLATFORM_LABEL=
[33mzookeeper_1           |[0m PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
[33;1mconnect_1             |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[35mkafka_1               |[0m CONFLUENT_SUPPORT_CUSTOMER_ID=anonymous
[36;1mksql-datagen-users_1  |[0m 	ssl.keymanager.algorithm = SunX509
[33mzookeeper_1           |[0m PWD=/
[33;1mconnect_1             |[0m # See the License for the specific language governing permissions and
[34mschema-registry_1     |[0m # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
[35mkafka_1               |[0m CONFLUENT_VERSION=3.3.0
[36;1mksql-datagen-users_1  |[0m 	ssl.keystore.location = null
[33mzookeeper_1           |[0m PYTHON_PIP_VERSION=8.1.2
[33;1mconnect_1             |[0m # limitations under the License.
[34mschema-registry_1     |[0m # See the License for the specific language governing permissions and
[35mkafka_1               |[0m HOME=/root
[36;1mksql-datagen-users_1  |[0m 	ssl.keystore.password = null
[33mzookeeper_1           |[0m PYTHON_VERSION=2.7.9-1
[33;1mconnect_1             |[0m 
[35mkafka_1               |[0m HOSTNAME=kafka
[36;1mksql-datagen-users_1  |[0m 	ssl.keystore.type = JKS
[33mzookeeper_1           |[0m SCALA_VERSION=2.11
[33;1mconnect_1             |[0m # Mesos DC/OS docker deployments will have HOST and PORT0 
[34mschema-registry_1     |[0m # limitations under the License.
[35mkafka_1               |[0m KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
[36;1mksql-datagen-users_1  |[0m 	ssl.protocol = TLS
[33mzookeeper_1           |[0m SHLVL=1
[33;1mconnect_1             |[0m # set for the proxying of the service.
[34mschema-registry_1     |[0m 
[35mkafka_1               |[0m KAFKA_AUTO_CREATE_TOPICS_ENABLE=true
[36;1mksql-datagen-users_1  |[0m 	ssl.provider = null
[33mzookeeper_1           |[0m ZOOKEEPER_CLIENT_PORT=32181
[33;1mconnect_1             |[0m # 
[34mschema-registry_1     |[0m # Mesos DC/OS docker deployments will have HOST and PORT0 
[35mkafka_1               |[0m KAFKA_BROKER_ID=1
[36;1mksql-datagen-users_1  |[0m 	ssl.secure.random.implementation = null
[33mzookeeper_1           |[0m ZOOKEEPER_TICK_TIME=2000
[33;1mconnect_1             |[0m # Use those values provide things we know we'll need.
[34mschema-registry_1     |[0m # set for the proxying of the service.
[35mkafka_1               |[0m KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT
[36;1mksql-datagen-users_1  |[0m 	ssl.trustmanager.algorithm = PKIX
[33mzookeeper_1           |[0m ZULU_OPENJDK_VERSION=8=8.17.0.3
[33;1mconnect_1             |[0m 
[34mschema-registry_1     |[0m # 
[35mkafka_1               |[0m KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
[36;1mksql-datagen-users_1  |[0m 	ssl.truststore.location = null
[33mzookeeper_1           |[0m _=/usr/bin/env
[33;1mconnect_1             |[0m [ -n "${HOST:-}" ] && [ -z "${CONNECT_REST_ADVERTISED_HOST_NAME:-}" ] && \
[34mschema-registry_1     |[0m # Use those values provide things we know we'll need.
[35mkafka_1               |[0m KAFKA_METRIC_REPORTERS=io.confluent.metrics.reporter.ConfluentMetricsReporter
[36;1mksql-datagen-users_1  |[0m 	ssl.truststore.password = null
[33;1mconnect_1             |[0m 	export CONNECT_REST_ADVERTISED_HOST_NAME=$HOST || true
[33mzookeeper_1           |[0m 
[34mschema-registry_1     |[0m 
[35mkafka_1               |[0m KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
[36;1mksql-datagen-users_1  |[0m 	ssl.truststore.type = JKS
[33;1mconnect_1             |[0m ++ '[' -n '' ']'
[33mzookeeper_1           |[0m echo "===> User"
[34mschema-registry_1     |[0m [ -n "${HOST:-}" ] && [ -z "${SCHEMA_REGISTRY_HOST_NAME:-}" ] && \
[35mkafka_1               |[0m KAFKA_VERSION=0.11.0.0
[36;1mksql-datagen-users_1  |[0m 
[33;1mconnect_1             |[0m ++ true
[33mzookeeper_1           |[0m + echo '===> User'
[34mschema-registry_1     |[0m 	export SCHEMA_REGISTRY_HOST_NAME=$HOST || true # we don't want the setup to fail if not on Mesos
[35mkafka_1               |[0m KAFKA_ZOOKEEPER_CONNECT=zookeeper:32181
[33;1mconnect_1             |[0m 
[35mkafka_1               |[0m LANG=C.UTF-8
[33;1mconnect_1             |[0m [ -n "${PORT0:-}" ] && [ -z "${CONNECT_REST_ADVERTISED_PORT:-}" ] && \
[33mzookeeper_1           |[0m ===> User
[35mkafka_1               |[0m PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
[33;1mconnect_1             |[0m 	export CONNECT_REST_ADVERTISED_PORT=$PORT0 || true
[34mschema-registry_1     |[0m ===> ENV Variables ...
[35mkafka_1               |[0m PWD=/
[33;1mconnect_1             |[0m ++ '[' -n '' ']'
[33mzookeeper_1           |[0m id
[35mkafka_1               |[0m PYTHON_PIP_VERSION=8.1.2
[34mschema-registry_1     |[0m ++ '[' -n '' ']'
[33;1mconnect_1             |[0m ++ true
[33mzookeeper_1           |[0m + id
[35mkafka_1               |[0m PYTHON_VERSION=2.7.9-1
[34mschema-registry_1     |[0m ++ true
[33;1mconnect_1             |[0m 
[35mkafka_1               |[0m SCALA_VERSION=2.11
[34mschema-registry_1     |[0m 
[34mschema-registry_1     |[0m 
[33;1mconnect_1             |[0m # And default to 8083, which MUST match the containerPort specification
[35mkafka_1               |[0m SHLVL=1
[33mzookeeper_1           |[0m uid=0(root) gid=0(root) groups=0(root)
[34mschema-registry_1     |[0m echo "===> ENV Variables ..."
[33;1mconnect_1             |[0m # in the Mesos package for this service.
[35mkafka_1               |[0m ZULU_OPENJDK_VERSION=8=8.17.0.3
[34mschema-registry_1     |[0m + echo '===> ENV Variables ...'
[33;1mconnect_1             |[0m [ -z "${CONNECT_REST_PORT:-}" ] && \
[35mkafka_1               |[0m _=/usr/bin/env
[34mschema-registry_1     |[0m env | sort
[33mzookeeper_1           |[0m 
[33;1mconnect_1             |[0m 	export CONNECT_REST_PORT=8083 || true
[34mschema-registry_1     |[0m + env
[33mzookeeper_1           |[0m echo "===> Configuring ..."
[33;1mconnect_1             |[0m ++ '[' -z 8083 ']'
[34mschema-registry_1     |[0m + sort
[33mzookeeper_1           |[0m + echo '===> Configuring ...'
[33;1mconnect_1             |[0m ++ true
[35mkafka_1               |[0m ===> User
[34mschema-registry_1     |[0m ALLOW_UNSIGNED=false
[33;1mconnect_1             |[0m 
[34mschema-registry_1     |[0m COMPONENT=schema-registry
[35mkafka_1               |[0m 
[33;1mconnect_1             |[0m 
[34mschema-registry_1     |[0m CONFLUENT_DEB_VERSION=1
[34mschema-registry_1     |[0m CONFLUENT_MAJOR_VERSION=3
[33mzookeeper_1           |[0m ===> Configuring ...
[33;1mconnect_1             |[0m echo "===> ENV Variables ..."
[35mkafka_1               |[0m echo "===> User"
[34mschema-registry_1     |[0m CONFLUENT_MINOR_VERSION=3
[33;1mconnect_1             |[0m + echo '===> ENV Variables ...'
[35mkafka_1               |[0m + echo '===> User'
[34mschema-registry_1     |[0m CONFLUENT_MVN_LABEL=
[33;1mconnect_1             |[0m env | sort
[35mkafka_1               |[0m id
[34mschema-registry_1     |[0m CONFLUENT_PATCH_VERSION=0
[35mkafka_1               |[0m + id
[34mschema-registry_1     |[0m CONFLUENT_PLATFORM_LABEL=
[33mzookeeper_1           |[0m /etc/confluent/docker/configure
[33;1mconnect_1             |[0m ===> ENV Variables ...
[35mkafka_1               |[0m uid=0(root) gid=0(root) groups=0(root)
[34mschema-registry_1     |[0m CONFLUENT_VERSION=3.3.0
[33mzookeeper_1           |[0m + /etc/confluent/docker/configure
[33;1mconnect_1             |[0m + sort
[34mschema-registry_1     |[0m HOME=/root
[35mkafka_1               |[0m 
[34mschema-registry_1     |[0m HOSTNAME=schema-registry
[35mkafka_1               |[0m echo "===> Configuring ..."
[34mschema-registry_1     |[0m KAFKA_VERSION=0.11.0.0
[35mkafka_1               |[0m + echo '===> Configuring ...'
[33mzookeeper_1           |[0m 
[34mschema-registry_1     |[0m LANG=C.UTF-8
[33;1mconnect_1             |[0m + env
[33mzookeeper_1           |[0m 
[34mschema-registry_1     |[0m PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
[33mzookeeper_1           |[0m dub ensure ZOOKEEPER_CLIENT_PORT
[34mschema-registry_1     |[0m PWD=/
[35mkafka_1               |[0m /etc/confluent/docker/configure
[33;1mconnect_1             |[0m ALLOW_UNSIGNED=false
[33mzookeeper_1           |[0m + dub ensure ZOOKEEPER_CLIENT_PORT
[34mschema-registry_1     |[0m PYTHON_PIP_VERSION=8.1.2
[35mkafka_1               |[0m + /etc/confluent/docker/configure
[33;1mconnect_1             |[0m CLASSPATH=/usr/share/java/monitoring-interceptors/monitoring-interceptors-3.3.0.jar
[34mschema-registry_1     |[0m PYTHON_VERSION=2.7.9-1
[33mzookeeper_1           |[0m 
[33;1mconnect_1             |[0m COMPONENT=kafka-connect
[34mschema-registry_1     |[0m SCALA_VERSION=2.11
[33mzookeeper_1           |[0m dub path /etc/kafka/ writable
[33;1mconnect_1             |[0m CONFLUENT_DEB_VERSION=1
[34mschema-registry_1     |[0m SCHEMA_REGISTRY_HOST_NAME=schema-registry
[33;1mconnect_1             |[0m CONFLUENT_MAJOR_VERSION=3
[34mschema-registry_1     |[0m SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL=zookeeper:32181
[33mzookeeper_1           |[0m + dub path /etc/kafka/ writable
[35mkafka_1               |[0m ===> Configuring ...
[33;1mconnect_1             |[0m CONFLUENT_MINOR_VERSION=3
[34mschema-registry_1     |[0m SHLVL=1
[33;1mconnect_1             |[0m CONFLUENT_MVN_LABEL=
[34mschema-registry_1     |[0m ZULU_OPENJDK_VERSION=8=8.17.0.3
[33;1mconnect_1             |[0m CONFLUENT_PATCH_VERSION=0
[35mkafka_1               |[0m 
[33mzookeeper_1           |[0m 
[36;1mksql-datagen-users_1  |[0m Expected 1 brokers but found only 0. Trying to query Kafka for metadata again ...
[34mschema-registry_1     |[0m _=/usr/bin/env
[33;1mconnect_1             |[0m CONFLUENT_PLATFORM_LABEL=
[35mkafka_1               |[0m dub ensure KAFKA_ZOOKEEPER_CONNECT
[33mzookeeper_1           |[0m # myid is required for clusters
[34mschema-registry_1     |[0m 
[33;1mconnect_1             |[0m CONFLUENT_VERSION=3.3.0
[33mzookeeper_1           |[0m if [[ -n "${ZOOKEEPER_SERVERS-}" ]]
[34mschema-registry_1     |[0m echo "===> User"
[33;1mconnect_1             |[0m CONNECT_BOOTSTRAP_SERVERS=kafka:29092
[35mkafka_1               |[0m + dub ensure KAFKA_ZOOKEEPER_CONNECT
[34mschema-registry_1     |[0m + echo '===> User'
[33;1mconnect_1             |[0m CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR=1
[33mzookeeper_1           |[0m then
[34mschema-registry_1     |[0m id
[33;1mconnect_1             |[0m CONNECT_CONFIG_STORAGE_TOPIC=connect-config
[33mzookeeper_1           |[0m   dub ensure ZOOKEEPER_SERVER_ID
[34mschema-registry_1     |[0m + id
[33;1mconnect_1             |[0m CONNECT_CONSUMER_INTERCEPTOR_CLASSES=io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor
[33mzookeeper_1           |[0m   export ZOOKEEPER_INIT_LIMIT=${ZOOKEEPER_INIT_LIMIT:-"10"}
[33;1mconnect_1             |[0m CONNECT_GROUP_ID=connect
[33mzookeeper_1           |[0m   export ZOOKEEPER_SYNC_LIMIT=${ZOOKEEPER_SYNC_LIMIT:-"5"}
[35mkafka_1               |[0m dub ensure KAFKA_ADVERTISED_LISTENERS
[34mschema-registry_1     |[0m ===> User
[33;1mconnect_1             |[0m CONNECT_INTERNAL_KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
[33mzookeeper_1           |[0m fi
[35mkafka_1               |[0m + dub ensure KAFKA_ADVERTISED_LISTENERS
[34mschema-registry_1     |[0m uid=0(root) gid=0(root) groups=0(root)
[33;1mconnect_1             |[0m CONNECT_INTERNAL_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
[33;1mconnect_1             |[0m CONNECT_KEY_CONVERTER=org.apache.kafka.connect.storage.StringConverter
[33;1mconnect_1             |[0m CONNECT_LOG4J_LOGGERS=org.reflections=ERROR
[33;1mconnect_1             |[0m CONNECT_LOG4J_ROOT_LOGLEVEL=INFO
[33;1mconnect_1             |[0m CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR=1
[33;1mconnect_1             |[0m CONNECT_OFFSET_STORAGE_TOPIC=connect-offsets
[33;1mconnect_1             |[0m CONNECT_PLUGIN_PATH=/connect-plugins
[33;1mconnect_1             |[0m CONNECT_PRODUCER_INTERCEPTOR_CLASSES=io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor
[33;1mconnect_1             |[0m CONNECT_REPLICATION_FACTOR=1
[33mzookeeper_1           |[0m + [[ -n '' ]]
[33;1mconnect_1             |[0m CONNECT_REST_ADVERTISED_HOST_NAME=connect
[33mzookeeper_1           |[0m 
[34mschema-registry_1     |[0m 
[35mkafka_1               |[0m 
[33;1mconnect_1             |[0m CONNECT_REST_PORT=8083
[33mzookeeper_1           |[0m if [[ -n "${ZOOKEEPER_SERVER_ID-}" ]]
[34mschema-registry_1     |[0m echo "===> Configuring ..."
[35mkafka_1               |[0m # By default, LISTENERS is derived from ADVERTISED_LISTENERS by replacing
[33;1mconnect_1             |[0m CONNECT_STATUS_STORAGE_REPLICATION_FACTOR=1
[33mzookeeper_1           |[0m then
[35mkafka_1               |[0m # hosts with 0.0.0.0. This is good default as it ensures that the broker
[33;1mconnect_1             |[0m CONNECT_STATUS_STORAGE_TOPIC=connect-status
[33mzookeeper_1           |[0m   dub template "/etc/confluent/docker/myid.template" "/var/lib/${COMPONENT}/data/myid"
[35mkafka_1               |[0m # process listens on all ports.
[34mschema-registry_1     |[0m + echo '===> Configuring ...'
[33;1mconnect_1             |[0m CONNECT_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
[33mzookeeper_1           |[0m fi
[35mkafka_1               |[0m if [[ -z "${KAFKA_LISTENERS-}" ]]
[34mschema-registry_1     |[0m /etc/confluent/docker/configure
[33;1mconnect_1             |[0m CONNECT_ZOOKEEPER_CONNECT=zookeeper:32181
[33mzookeeper_1           |[0m + [[ -n '' ]]
[35mkafka_1               |[0m then
[34mschema-registry_1     |[0m + /etc/confluent/docker/configure
[33;1mconnect_1             |[0m HOME=/root
[33mzookeeper_1           |[0m 
[35mkafka_1               |[0m   export KAFKA_LISTENERS
[33;1mconnect_1             |[0m HOSTNAME=5dee5800a78d
[33mzookeeper_1           |[0m if [[ -n "${KAFKA_JMX_OPTS-}" ]]
[35mkafka_1               |[0m   KAFKA_LISTENERS=$(cub listeners "$KAFKA_ADVERTISED_LISTENERS")
[33;1mconnect_1             |[0m KAFKA_VERSION=0.11.0.0
[33mzookeeper_1           |[0m then
[35mkafka_1               |[0m fi
[34mschema-registry_1     |[0m ===> Configuring ...
[33mzookeeper_1           |[0m   if [[ ! $KAFKA_JMX_OPTS == *"com.sun.management.jmxremote.rmi.port"*  ]]
[35mkafka_1               |[0m + [[ -z '' ]]
[33mzookeeper_1           |[0m   then
[33;1mconnect_1             |[0m LANG=C.UTF-8
[35mkafka_1               |[0m + export KAFKA_LISTENERS
[33mzookeeper_1           |[0m     echo "KAFKA_OPTS should contain 'com.sun.management.jmxremote.rmi.port' property. It is required for accessing the JMX metrics externally."
[33;1mconnect_1             |[0m PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
[33mzookeeper_1           |[0m   fi
[35mkafka_1               |[0m cub listeners "$KAFKA_ADVERTISED_LISTENERS"
[33mzookeeper_1           |[0m fi
[34mschema-registry_1     |[0m 
[33;1mconnect_1             |[0m PWD=/
[35mkafka_1               |[0m ++ cub listeners PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
[34mschema-registry_1     |[0m dub ensure SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL
[33;1mconnect_1             |[0m PYTHON_PIP_VERSION=8.1.2
[33mzookeeper_1           |[0m + [[ -n '' ]]
[34mschema-registry_1     |[0m + dub ensure SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL
[33;1mconnect_1             |[0m PYTHON_VERSION=2.7.9-1
[35mkafka_1               |[0m + KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
[33mzookeeper_1           |[0m 
[33;1mconnect_1             |[0m SCALA_VERSION=2.11
[35mkafka_1               |[0m 
[33mzookeeper_1           |[0m dub template "/etc/confluent/docker/${COMPONENT}.properties.template" "/etc/kafka/${COMPONENT}.properties"
[34mschema-registry_1     |[0m dub ensure SCHEMA_REGISTRY_HOST_NAME
[33;1mconnect_1             |[0m SHLVL=1
[35mkafka_1               |[0m dub path /etc/kafka/ writable
[34mschema-registry_1     |[0m + dub ensure SCHEMA_REGISTRY_HOST_NAME
[33;1mconnect_1             |[0m ZULU_OPENJDK_VERSION=8=8.17.0.3
[33mzookeeper_1           |[0m + dub template /etc/confluent/docker/zookeeper.properties.template /etc/kafka/zookeeper.properties
[35mkafka_1               |[0m + dub path /etc/kafka/ writable
[33;1mconnect_1             |[0m _=/usr/bin/env
[34mschema-registry_1     |[0m dub path /etc/"${COMPONENT}"/ writable
[35mkafka_1               |[0m 
[34mschema-registry_1     |[0m + dub path /etc/schema-registry/ writable
[35mkafka_1               |[0m if [[ -z "${KAFKA_LOG_DIRS-}" ]]
[35mkafka_1               |[0m then
[33mzookeeper_1           |[0m dub template "/etc/confluent/docker/log4j.properties.template" "/etc/kafka/log4j.properties"
[33;1mconnect_1             |[0m 
[35mkafka_1               |[0m   export KAFKA_LOG_DIRS
[33mzookeeper_1           |[0m + dub template /etc/confluent/docker/log4j.properties.template /etc/kafka/log4j.properties
[34mschema-registry_1     |[0m 
[33;1mconnect_1             |[0m echo "===> User"
[35mkafka_1               |[0m   KAFKA_LOG_DIRS="/var/lib/kafka/data"
[34mschema-registry_1     |[0m if [[ -n "${SCHEMA_REGISTRY_PORT-}" ]]
[33mzookeeper_1           |[0m dub template "/etc/confluent/docker/tools-log4j.properties.template" "/etc/kafka/tools-log4j.properties"
[35mkafka_1               |[0m fi
[34mschema-registry_1     |[0m then
[33;1mconnect_1             |[0m + echo '===> User'
[33mzookeeper_1           |[0m + dub template /etc/confluent/docker/tools-log4j.properties.template /etc/kafka/tools-log4j.properties
[35mkafka_1               |[0m + [[ -z '' ]]
[34mschema-registry_1     |[0m   echo "PORT is deprecated. Please use SCHEMA_REGISTRY_LISTENERS instead."
[33;1mconnect_1             |[0m id
[35mkafka_1               |[0m + export KAFKA_LOG_DIRS
[33mzookeeper_1           |[0m 
[34mschema-registry_1     |[0m   exit 1
[33;1mconnect_1             |[0m + id
[35mkafka_1               |[0m + KAFKA_LOG_DIRS=/var/lib/kafka/data
[33mzookeeper_1           |[0m echo "===> Running preflight checks ... "
[34mschema-registry_1     |[0m fi
[35mkafka_1               |[0m 
[33;1mconnect_1             |[0m ===> User
[34mschema-registry_1     |[0m + [[ -n '' ]]
[33mzookeeper_1           |[0m ===> Running preflight checks ... 
[35mkafka_1               |[0m # advertised.host, advertised.port, host and port are deprecated. Exit if these properties are set.
[35mkafka_1               |[0m if [[ -n "${KAFKA_ADVERTISED_PORT-}" ]]
[34mschema-registry_1     |[0m 
[33mzookeeper_1           |[0m + echo '===> Running preflight checks ... '
[33;1mconnect_1             |[0m uid=0(root) gid=0(root) groups=0(root)
[35mkafka_1               |[0m then
[34mschema-registry_1     |[0m if [[ -n "${SCHEMA_REGISTRY_JMX_OPTS-}" ]]
[33mzookeeper_1           |[0m /etc/confluent/docker/ensure
[35mkafka_1               |[0m   echo "advertised.port is deprecated. Please use KAFKA_ADVERTISED_LISTENERS instead."
[34mschema-registry_1     |[0m then
[33mzookeeper_1           |[0m + /etc/confluent/docker/ensure
[35mkafka_1               |[0m   exit 1
[34mschema-registry_1     |[0m   if [[ ! $SCHEMA_REGISTRY_JMX_OPTS == *"com.sun.management.jmxremote.rmi.port"*  ]]
[35mkafka_1               |[0m fi
[34mschema-registry_1     |[0m   then
[33;1mconnect_1             |[0m 
[35mkafka_1               |[0m + [[ -n '' ]]
[33;1mconnect_1             |[0m echo "===> Configuring ..."
[33mzookeeper_1           |[0m 
[35mkafka_1               |[0m 
[34mschema-registry_1     |[0m     echo "SCHEMA_REGISTRY_OPTS should contain 'com.sun.management.jmxremote.rmi.port' property. It is required for accessing the JMX metrics externally."
[33mzookeeper_1           |[0m 
[35mkafka_1               |[0m if [[ -n "${KAFKA_ADVERTISED_HOST-}" ]]
[33mzookeeper_1           |[0m echo "===> Check if /var/lib/zookeeper/data is writable ..."
[33;1mconnect_1             |[0m + echo '===> Configuring ...'
[33mzookeeper_1           |[0m + echo '===> Check if /var/lib/zookeeper/data is writable ...'
[34mschema-registry_1     |[0m   fi
[35mkafka_1               |[0m then
[33;1mconnect_1             |[0m ===> Configuring ...
[34mschema-registry_1     |[0m fi
[35mkafka_1               |[0m   echo "advertised.host is deprecated. Please use KAFKA_ADVERTISED_LISTENERS instead."
[34mschema-registry_1     |[0m + [[ -n '' ]]
[33;1mconnect_1             |[0m /etc/confluent/docker/configure
[35mkafka_1               |[0m   exit 1
[34mschema-registry_1     |[0m 
[33mzookeeper_1           |[0m ===> Check if /var/lib/zookeeper/data is writable ...
[33;1mconnect_1             |[0m + /etc/confluent/docker/configure
[35mkafka_1               |[0m fi
[34mschema-registry_1     |[0m dub template "/etc/confluent/docker/${COMPONENT}.properties.template" "/etc/${COMPONENT}/${COMPONENT}.properties"
[33mzookeeper_1           |[0m dub path /var/lib/zookeeper/data writable
[35mkafka_1               |[0m + [[ -n '' ]]
[34mschema-registry_1     |[0m + dub template /etc/confluent/docker/schema-registry.properties.template /etc/schema-registry/schema-registry.properties
[33mzookeeper_1           |[0m + dub path /var/lib/zookeeper/data writable
[33;1mconnect_1             |[0m 
[35mkafka_1               |[0m 
[33mzookeeper_1           |[0m 
[35mkafka_1               |[0m if [[ -n "${KAFKA_HOST-}" ]]
[33;1mconnect_1             |[0m dub ensure CONNECT_BOOTSTRAP_SERVERS
[33mzookeeper_1           |[0m echo "===> Check if /var/lib/zookeeper/log is writable ..."
[35mkafka_1               |[0m then
[33;1mconnect_1             |[0m + dub ensure CONNECT_BOOTSTRAP_SERVERS
[34mschema-registry_1     |[0m dub template "/etc/confluent/docker/log4j.properties.template" "/etc/${COMPONENT}/log4j.properties"
[34mschema-registry_1     |[0m + dub template /etc/confluent/docker/log4j.properties.template /etc/schema-registry/log4j.properties
[35mkafka_1               |[0m   echo "host is deprecated. Please use KAFKA_ADVERTISED_LISTENERS instead."
[35mkafka_1               |[0m   exit 1
[33mzookeeper_1           |[0m ===> Check if /var/lib/zookeeper/log is writable ...
[34mschema-registry_1     |[0m dub template "/etc/confluent/docker/admin.properties.template" "/etc/${COMPONENT}/admin.properties"
[35mkafka_1               |[0m fi
[33;1mconnect_1             |[0m dub ensure CONNECT_GROUP_ID
[34mschema-registry_1     |[0m + dub template /etc/confluent/docker/admin.properties.template /etc/schema-registry/admin.properties
[33mzookeeper_1           |[0m + echo '===> Check if /var/lib/zookeeper/log is writable ...'
[33;1mconnect_1             |[0m + dub ensure CONNECT_GROUP_ID
[35mkafka_1               |[0m + [[ -n '' ]]
[34mschema-registry_1     |[0m 
[33mzookeeper_1           |[0m dub path /var/lib/zookeeper/log writable
[35mkafka_1               |[0m 
[34mschema-registry_1     |[0m echo "===> Running preflight checks ... "
[33;1mconnect_1             |[0m dub ensure CONNECT_CONFIG_STORAGE_TOPIC
[33mzookeeper_1           |[0m + dub path /var/lib/zookeeper/log writable
[35mkafka_1               |[0m if [[ -n "${KAFKA_PORT-}" ]]
[34mschema-registry_1     |[0m + echo '===> Running preflight checks ... '
[33;1mconnect_1             |[0m + dub ensure CONNECT_CONFIG_STORAGE_TOPIC
[35mkafka_1               |[0m then
[34mschema-registry_1     |[0m /etc/confluent/docker/ensure
[33mzookeeper_1           |[0m 
[35mkafka_1               |[0m   echo "port is deprecated. Please use KAFKA_ADVERTISED_LISTENERS instead."
[33;1mconnect_1             |[0m dub ensure CONNECT_OFFSET_STORAGE_TOPIC
[33mzookeeper_1           |[0m echo "===> Launching ... "
[35mkafka_1               |[0m   exit 1
[33;1mconnect_1             |[0m + dub ensure CONNECT_OFFSET_STORAGE_TOPIC
[34mschema-registry_1     |[0m ===> Running preflight checks ... 
[35mkafka_1               |[0m fi
[34mschema-registry_1     |[0m + /etc/confluent/docker/ensure
[33;1mconnect_1             |[0m dub ensure CONNECT_STATUS_STORAGE_TOPIC
[35mkafka_1               |[0m + [[ -n '' ]]
[33mzookeeper_1           |[0m + echo '===> Launching ... '
[34mschema-registry_1     |[0m 
[35mkafka_1               |[0m 
[33mzookeeper_1           |[0m exec /etc/confluent/docker/launch
[35mkafka_1               |[0m # Set if ADVERTISED_LISTENERS has SSL:// or SASL_SSL:// endpoints.
[33mzookeeper_1           |[0m + exec /etc/confluent/docker/launch
[33;1mconnect_1             |[0m + dub ensure CONNECT_STATUS_STORAGE_TOPIC
[35mkafka_1               |[0m if [[ $KAFKA_ADVERTISED_LISTENERS == *"SSL://"* ]]
[34mschema-registry_1     |[0m echo "===> Check if Zookeeper is healthy ..."
[35mkafka_1               |[0m then
[33;1mconnect_1             |[0m dub ensure CONNECT_KEY_CONVERTER
[33mzookeeper_1           |[0m ===> Launching ... 
[34mschema-registry_1     |[0m + echo '===> Check if Zookeeper is healthy ...'
[35mkafka_1               |[0m   echo "SSL is enabled."
[33;1mconnect_1             |[0m + dub ensure CONNECT_KEY_CONVERTER
[33mzookeeper_1           |[0m ===> Launching zookeeper ... 
[35mkafka_1               |[0m 
[34mschema-registry_1     |[0m ===> Check if Zookeeper is healthy ...
[35mkafka_1               |[0m   dub ensure KAFKA_SSL_KEYSTORE_FILENAME
[34mschema-registry_1     |[0m cub zk-ready "$SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL" "${SCHEMA_REGISTRY_CUB_ZK_TIMEOUT:-40}"
[33mzookeeper_1           |[0m [2017-10-25 05:25:24,985] INFO Reading configuration from: /etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[35mkafka_1               |[0m   export KAFKA_SSL_KEYSTORE_LOCATION="/etc/kafka/secrets/$KAFKA_SSL_KEYSTORE_FILENAME"
[34mschema-registry_1     |[0m + cub zk-ready zookeeper:32181 40
[33;1mconnect_1             |[0m dub ensure CONNECT_VALUE_CONVERTER
[33mzookeeper_1           |[0m [2017-10-25 05:25:24,989] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[35mkafka_1               |[0m   dub path "$KAFKA_SSL_KEYSTORE_LOCATION" exists
[33;1mconnect_1             |[0m + dub ensure CONNECT_VALUE_CONVERTER
[35mkafka_1               |[0m 
[34mschema-registry_1     |[0m Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
[33mzookeeper_1           |[0m [2017-10-25 05:25:24,989] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[35mkafka_1               |[0m   dub ensure KAFKA_SSL_KEY_CREDENTIALS
[34mschema-registry_1     |[0m Client environment:host.name=schema-registry
[35mkafka_1               |[0m   KAFKA_SSL_KEY_CREDENTIALS_LOCATION="/etc/kafka/secrets/$KAFKA_SSL_KEY_CREDENTIALS"
[34mschema-registry_1     |[0m Client environment:java.version=1.8.0_102
[33;1mconnect_1             |[0m dub ensure CONNECT_INTERNAL_KEY_CONVERTER
[34mschema-registry_1     |[0m Client environment:java.vendor=Azul Systems, Inc.
[33;1mconnect_1             |[0m + dub ensure CONNECT_INTERNAL_KEY_CONVERTER
[34mschema-registry_1     |[0m Client environment:java.home=/usr/lib/jvm/zulu-8-amd64/jre
[35mkafka_1               |[0m   dub path "$KAFKA_SSL_KEY_CREDENTIALS_LOCATION" exists
[33mzookeeper_1           |[0m [2017-10-25 05:25:24,989] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[34mschema-registry_1     |[0m Client environment:java.class.path=/etc/confluent/docker/docker-utils.jar
[35mkafka_1               |[0m   export KAFKA_SSL_KEY_PASSWORD
[33;1mconnect_1             |[0m dub ensure CONNECT_INTERNAL_VALUE_CONVERTER
[34mschema-registry_1     |[0m Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[35mkafka_1               |[0m   KAFKA_SSL_KEY_PASSWORD=$(cat "$KAFKA_SSL_KEY_CREDENTIALS_LOCATION")
[33;1mconnect_1             |[0m + dub ensure CONNECT_INTERNAL_VALUE_CONVERTER
[34mschema-registry_1     |[0m Client environment:java.io.tmpdir=/tmp
[35mkafka_1               |[0m 
[34mschema-registry_1     |[0m Client environment:java.compiler=<NA>
[35mkafka_1               |[0m   dub ensure KAFKA_SSL_KEYSTORE_CREDENTIALS
[34mschema-registry_1     |[0m Client environment:os.name=Linux
[33mzookeeper_1           |[0m [2017-10-25 05:25:24,989] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[35mkafka_1               |[0m   KAFKA_SSL_KEYSTORE_CREDENTIALS_LOCATION="/etc/kafka/secrets/$KAFKA_SSL_KEYSTORE_CREDENTIALS"
[34mschema-registry_1     |[0m Client environment:os.arch=amd64
[33;1mconnect_1             |[0m # This is required to avoid config bugs. You should set this to a value that is
[35mkafka_1               |[0m   dub path "$KAFKA_SSL_KEYSTORE_CREDENTIALS_LOCATION" exists
[34mschema-registry_1     |[0m Client environment:os.version=4.9.49-moby
[35mkafka_1               |[0m   export KAFKA_SSL_KEYSTORE_PASSWORD
[34mschema-registry_1     |[0m Client environment:user.name=root
[33;1mconnect_1             |[0m # resolvable by all containers.
[33mzookeeper_1           |[0m [2017-10-25 05:25:25,011] INFO Reading configuration from: /etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[35mkafka_1               |[0m   KAFKA_SSL_KEYSTORE_PASSWORD=$(cat "$KAFKA_SSL_KEYSTORE_CREDENTIALS_LOCATION")
[34mschema-registry_1     |[0m Client environment:user.home=/root
[33;1mconnect_1             |[0m dub ensure CONNECT_REST_ADVERTISED_HOST_NAME
[35mkafka_1               |[0m 
[34mschema-registry_1     |[0m Client environment:user.dir=/
[33;1mconnect_1             |[0m + dub ensure CONNECT_REST_ADVERTISED_HOST_NAME
[35mkafka_1               |[0m   dub ensure KAFKA_SSL_TRUSTSTORE_FILENAME
[34mschema-registry_1     |[0m Initiating client connection, connectString=zookeeper:32181 sessionTimeout=40000 watcher=io.confluent.admin.utils.ZookeeperConnectionWatcher@5a2e4553
[33mzookeeper_1           |[0m [2017-10-25 05:25:25,011] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[35mkafka_1               |[0m   export KAFKA_SSL_TRUSTSTORE_LOCATION="/etc/kafka/secrets/$KAFKA_SSL_TRUSTSTORE_FILENAME"
[33;1mconnect_1             |[0m 
[33mzookeeper_1           |[0m [2017-10-25 05:25:25,023] INFO Server environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[35mkafka_1               |[0m   dub path "$KAFKA_SSL_TRUSTSTORE_LOCATION" exists
[33;1mconnect_1             |[0m # Default to 8083, which matches the mesos-overrides. This is here in case we extend the containers to remove the mesos overrides.
[34mschema-registry_1     |[0m Opening socket connection to server quickstart_zookeeper_1.quickstart_default/172.21.0.4:32181. Will not attempt to authenticate using SASL (unknown error)
[33mzookeeper_1           |[0m [2017-10-25 05:25:25,023] INFO Server environment:host.name=zookeeper (org.apache.zookeeper.server.ZooKeeperServer)
[35mkafka_1               |[0m 
[33;1mconnect_1             |[0m if [ -z "$CONNECT_REST_PORT" ]; then
[34mschema-registry_1     |[0m Socket connection established to quickstart_zookeeper_1.quickstart_default/172.21.0.4:32181, initiating session
[33mzookeeper_1           |[0m [2017-10-25 05:25:25,023] INFO Server environment:java.version=1.8.0_102 (org.apache.zookeeper.server.ZooKeeperServer)
[35mkafka_1               |[0m   dub ensure KAFKA_SSL_TRUSTSTORE_CREDENTIALS
[33;1mconnect_1             |[0m   export CONNECT_REST_PORT=8083
[34mschema-registry_1     |[0m Session establishment complete on server quickstart_zookeeper_1.quickstart_default/172.21.0.4:32181, sessionid = 0x15f51fe0e370002, negotiated timeout = 40000
[33mzookeeper_1           |[0m [2017-10-25 05:25:25,023] INFO Server environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.server.ZooKeeperServer)
[35mkafka_1               |[0m   KAFKA_SSL_TRUSTSTORE_CREDENTIALS_LOCATION="/etc/kafka/secrets/$KAFKA_SSL_TRUSTSTORE_CREDENTIALS"
[33;1mconnect_1             |[0m fi
[33mzookeeper_1           |[0m [2017-10-25 05:25:25,024] INFO Server environment:java.home=/usr/lib/jvm/zulu-8-amd64/jre (org.apache.zookeeper.server.ZooKeeperServer)
[34mschema-registry_1     |[0m Session: 0x15f51fe0e370002 closed
[35mkafka_1               |[0m   dub path "$KAFKA_SSL_TRUSTSTORE_CREDENTIALS_LOCATION" exists
[33;1mconnect_1             |[0m + '[' -z 8083 ']'
[33mzookeeper_1           |[0m [2017-10-25 05:25:25,024] INFO Server environment:java.class.path=:/usr/bin/../share/java/kafka/kafka_2.11-0.11.0.0-cp1-javadoc.jar:/usr/bin/../share/java/kafka/xz-1.5.jar:/usr/bin/../share/java/kafka/commons-digester-1.8.1.jar:/usr/bin/../share/java/kafka/kafka-clients-0.11.0.0-cp1.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.1.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.8.5.jar:/usr/bin/../share/java/kafka/commons-beanutils-1.8.3.jar:/usr/bin/../share/java/kafka/jetty-security-9.2.15.v20160210.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.3.jar:/usr/bin/../share/java/kafka/commons-logging-1.2.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/kafka-streams-0.11.0.0-cp1.jar:/usr/bin/../share/java/kafka/support-metrics-common-3.3.0.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.8.5.jar:/usr/bin/../share/java/kafka/jersey-media-jaxb-2.24.jar:/usr/bin/../share/java/kafka/javax.inject-1.jar:/usr/bin/../share/java/kafka/connect-json-0.11.0.0-cp1.jar:/usr/bin/../share/java/kafka/slf4j-log4j12-1.7.25.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.5.0-b05.jar:/usr/bin/../share/java/kafka/lz4-1.3.0.jar:/usr/bin/../share/java/kafka/kafka_2.11-0.11.0.0-cp1-sources.jar:/usr/bin/../share/java/kafka/zookeeper-3.4.10.jar:/usr/bin/../share/java/kafka/commons-lang3-3.5.jar:/usr/bin/../share/java/kafka/httpmime-4.5.2.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-0.11.0.0-cp1.jar:/usr/bin/../share/java/kafka/scala-library-2.11.11.jar:/usr/bin/../share/java/kafka/jersey-common-2.24.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/plexus-utils-3.0.24.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.0.1.jar:/usr/bin/../share/java/kafka/commons-lang3-3.1.jar:/usr/bin/../share/java/kafka/rocksdbjni-5.0.1.jar:/usr/bin/../share/java/kafka/guava-20.0.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.25.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.2.15.v20160210.jar:/usr/bin/../share/java/kafka/connect-api-0.11.0.0-cp1.jar:/usr/bin/../share/java/kafka/jetty-io-9.2.15.v20160210.jar:/usr/bin/../share/java/kafka/commons-collections-3.2.1.jar:/usr/bin/../share/java/kafka/avro-1.8.2.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.24.jar:/usr/bin/../share/java/kafka/jackson-databind-2.8.5.jar:/usr/bin/../share/java/kafka/kafka_2.11-0.11.0.0-cp1-test.jar:/usr/bin/../share/java/kafka/maven-artifact-3.5.0.jar:/usr/bin/../share/java/kafka/jackson-core-asl-1.9.13.jar:/usr/bin/../share/java/kafka/jackson-mapper-asl-1.9.13.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.8.5.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.2.15.v20160210.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.24.jar:/usr/bin/../share/java/kafka/jetty-util-9.2.15.v20160210.jar:/usr/bin/../share/java/kafka/support-metrics-client-3.3.0.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.2.6.jar:/usr/bin/../share/java/kafka/javax.inject-2.5.0-b05.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.2.15.v20160210.jar:/usr/bin/../share/java/kafka/zkclient-0.10.jar:/usr/bin/../share/java/kafka/jersey-server-2.24.jar:/usr/bin/../share/java/kafka/jetty-server-9.2.15.v20160210.jar:/usr/bin/../share/java/kafka/hk2-locator-2.5.0-b05.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.8.5.jar:/usr/bin/../share/java/kafka/kafka_2.11-0.11.0.0-cp1-test-sources.jar:/usr/bin/../share/java/kafka/javax.annotation-api-1.2.jar:/usr/bin/../share/java/kafka/kafka_2.11-0.11.0.0-cp1-scaladoc.jar:/usr/bin/../share/java/kafka/commons-compress-1.8.1.jar:/usr/bin/../share/java/kafka/jersey-guava-2.24.jar:/usr/bin/../share/java/kafka/scala-parser-combinators_2.11-1.0.4.jar:/usr/bin/../share/java/kafka/log4j-1.2.17.jar:/usr/bin/../share/java/kafka/paranamer-2.7.jar:/usr/bin/../share/java/kafka/jersey-client-2.24.jar:/usr/bin/../share/java/kafka/hk2-utils-2.5.0-b05.jar:/usr/bin/../share/java/kafka/connect-runtime-0.11.0.0-cp1.jar:/usr/bin/../share/java/kafka/validation-api-1.1.0.Final.jar:/usr/bin/../share/java/kafka/reflections-0.9.11.jar:/usr/bin/../share/java/kafka/commons-codec-1.9.jar:/usr/bin/../share/java/kafka/connect-transforms-0.11.0.0-cp1.jar:/usr/bin/../share/java/kafka/javassist-3.21.0-GA.jar:/usr/bin/../share/java/kafka/kafka-tools-0.11.0.0-cp1.jar:/usr/bin/../share/java/kafka/commons-validator-1.4.1.jar:/usr/bin/../share/java/kafka/hk2-api-2.5.0-b05.jar:/usr/bin/../share/java/kafka/jackson-core-2.8.5.jar:/usr/bin/../share/java/kafka/httpclient-4.5.2.jar:/usr/bin/../share/java/kafka/httpcore-4.4.4.jar:/usr/bin/../share/java/kafka/connect-file-0.11.0.0-cp1.jar:/usr/bin/../share/java/kafka/kafka_2.11-0.11.0.0-cp1.jar:/usr/bin/../share/java/kafka/jetty-http-9.2.15.v20160210.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-0.11.0.0-cp1.jar:/usr/bin/../share/java/confluent-support-metrics/*:/usr/share/java/confluent-support-metrics/* (org.apache.zookeeper.server.ZooKeeperServer)
[34mschema-registry_1     |[0m EventThread shut down
[35mkafka_1               |[0m   export KAFKA_SSL_TRUSTSTORE_PASSWORD
[33;1mconnect_1             |[0m 
[33mzookeeper_1           |[0m [2017-10-25 05:25:25,024] INFO Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.server.ZooKeeperServer)
[35mkafka_1               |[0m   KAFKA_SSL_TRUSTSTORE_PASSWORD=$(cat "$KAFKA_SSL_TRUSTSTORE_CREDENTIALS_LOCATION")
[33;1mconnect_1             |[0m # Fix for https://issues.apache.org/jira/browse/KAFKA-3988
[35mkafka_1               |[0m 
[34mschema-registry_1     |[0m ===> Check if Kafka is healthy ...
[33;1mconnect_1             |[0m if [[ $CONNECT_INTERNAL_KEY_CONVERTER == "org.apache.kafka.connect.json.JsonConverter" ]] || [[ $CONNECT_INTERNAL_VALUE_CONVERTER == "org.apache.kafka.connect.json.JsonConverter" ]]
[35mkafka_1               |[0m fi
[35mkafka_1               |[0m + [[ PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092 == *\S\S\L\:\/\/* ]]
[33mzookeeper_1           |[0m [2017-10-25 05:25:25,024] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer)
[33;1mconnect_1             |[0m then
[34mschema-registry_1     |[0m 
[35mkafka_1               |[0m 
[33mzookeeper_1           |[0m [2017-10-25 05:25:25,024] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[33;1mconnect_1             |[0m   export CONNECT_INTERNAL_KEY_CONVERTER_SCHEMAS_ENABLE=false
[34mschema-registry_1     |[0m echo "===> Check if Kafka is healthy ..."
[35mkafka_1               |[0m # Set if KAFKA_ADVERTISED_LISTENERS has SASL_PLAINTEXT:// or SASL_SSL:// endpoints.
[33mzookeeper_1           |[0m [2017-10-25 05:25:25,024] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer)
[33;1mconnect_1             |[0m   export CONNECT_INTERNAL_VALUE_CONVERTER_SCHEMAS_ENABLE=false
[34mschema-registry_1     |[0m + echo '===> Check if Kafka is healthy ...'
[35mkafka_1               |[0m if [[ $KAFKA_ADVERTISED_LISTENERS =~ .*SASL_.*://.* ]]
[33mzookeeper_1           |[0m [2017-10-25 05:25:25,024] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[34mschema-registry_1     |[0m 
[33;1mconnect_1             |[0m fi
[35mkafka_1               |[0m then
[33mzookeeper_1           |[0m [2017-10-25 05:25:25,024] INFO Server environment:os.version=4.9.49-moby (org.apache.zookeeper.server.ZooKeeperServer)
[34mschema-registry_1     |[0m if [[ -n "${SCHEMA_REGISTRY_KAFKASTORE_SECURITY_PROTOCOL-}" ]] && [[ $SCHEMA_REGISTRY_KAFKASTORE_SECURITY_PROTOCOL = "SSL" ]]
[33;1mconnect_1             |[0m + [[ org.apache.kafka.connect.json.JsonConverter == \o\r\g\.\a\p\a\c\h\e\.\k\a\f\k\a\.\c\o\n\n\e\c\t\.\j\s\o\n\.\J\s\o\n\C\o\n\v\e\r\t\e\r ]]
[35mkafka_1               |[0m   echo "SASL" is enabled.
[33mzookeeper_1           |[0m [2017-10-25 05:25:25,024] INFO Server environment:user.name=root (org.apache.zookeeper.server.ZooKeeperServer)
[34mschema-registry_1     |[0m then
[33;1mconnect_1             |[0m + export CONNECT_INTERNAL_KEY_CONVERTER_SCHEMAS_ENABLE=false
[35mkafka_1               |[0m 
[33mzookeeper_1           |[0m [2017-10-25 05:25:25,024] INFO Server environment:user.home=/root (org.apache.zookeeper.server.ZooKeeperServer)
[34mschema-registry_1     |[0m     cub kafka-ready \
[33;1mconnect_1             |[0m + CONNECT_INTERNAL_KEY_CONVERTER_SCHEMAS_ENABLE=false
[35mkafka_1               |[0m   dub ensure KAFKA_OPTS
[33mzookeeper_1           |[0m [2017-10-25 05:25:25,024] INFO Server environment:user.dir=/ (org.apache.zookeeper.server.ZooKeeperServer)
[34mschema-registry_1     |[0m         "${SCHEMA_REGISTRY_CUB_KAFKA_MIN_BROKERS:-1}" \
[33;1mconnect_1             |[0m + export CONNECT_INTERNAL_VALUE_CONVERTER_SCHEMAS_ENABLE=false
[35mkafka_1               |[0m 
[34mschema-registry_1     |[0m         "${SCHEMA_REGISTRY_CUB_KAFKA_TIMEOUT:-40}" \
[33;1mconnect_1             |[0m + CONNECT_INTERNAL_VALUE_CONVERTER_SCHEMAS_ENABLE=false
[35mkafka_1               |[0m   if [[ ! $KAFKA_OPTS == *"java.security.auth.login.config"*  ]]
[34mschema-registry_1     |[0m         -b "${SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS}" \
[33;1mconnect_1             |[0m 
[35mkafka_1               |[0m   then
[34mschema-registry_1     |[0m         --config /etc/"${COMPONENT}"/admin.properties
[33mzookeeper_1           |[0m [2017-10-25 05:25:25,130] INFO tickTime set to 2000 (org.apache.zookeeper.server.ZooKeeperServer)
[33;1mconnect_1             |[0m if [[ $CONNECT_KEY_CONVERTER == "io.confluent.connect.avro.AvroConverter" ]]
[33;1mconnect_1             |[0m then
[35mkafka_1               |[0m     echo "KAFKA_OPTS should contain 'java.security.auth.login.config' property."
[34mschema-registry_1     |[0m else
[33mzookeeper_1           |[0m [2017-10-25 05:25:25,130] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[33;1mconnect_1             |[0m   dub ensure CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL
[35mkafka_1               |[0m   fi
[34mschema-registry_1     |[0m     cub kafka-ready \
[33mzookeeper_1           |[0m [2017-10-25 05:25:25,131] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[33;1mconnect_1             |[0m fi
[35mkafka_1               |[0m fi
[34mschema-registry_1     |[0m         "${SCHEMA_REGISTRY_CUB_KAFKA_MIN_BROKERS:-1}" \
[33;1mconnect_1             |[0m + [[ org.apache.kafka.connect.storage.StringConverter == \i\o\.\c\o\n\f\l\u\e\n\t\.\c\o\n\n\e\c\t\.\a\v\r\o\.\A\v\r\o\C\o\n\v\e\r\t\e\r ]]
[35mkafka_1               |[0m + [[ PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092 =~ .*SASL_.*://.* ]]
[34mschema-registry_1     |[0m         "${SCHEMA_REGISTRY_CUB_KAFKA_TIMEOUT:-40}" \
[33mzookeeper_1           |[0m [2017-10-25 05:25:25,158] INFO binding to port 0.0.0.0/0.0.0.0:32181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[33;1mconnect_1             |[0m 
[35mkafka_1               |[0m 
[33;1mconnect_1             |[0m if [[ $CONNECT_VALUE_CONVERTER == "io.confluent.connect.avro.AvroConverter" ]]
[35mkafka_1               |[0m if [[ -n "${KAFKA_JMX_OPTS-}" ]]
[33;1mconnect_1             |[0m then
[34mschema-registry_1     |[0m         -z "$SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL"
[35mkafka_1               |[0m then
[33;1mconnect_1             |[0m   dub ensure CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL
[34mschema-registry_1     |[0m fi
[33mzookeeper_1           |[0m [2017-10-25 05:25:26,775] INFO Accepted socket connection from /172.21.0.5:40118 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[35mkafka_1               |[0m   if [[ ! $KAFKA_JMX_OPTS == *"com.sun.management.jmxremote.rmi.port"*  ]]
[33;1mconnect_1             |[0m fi
[34mschema-registry_1     |[0m + [[ -n '' ]]
[35mkafka_1               |[0m   then
[33mzookeeper_1           |[0m [2017-10-25 05:25:26,846] INFO Client attempting to establish new session at /172.21.0.5:40118 (org.apache.zookeeper.server.ZooKeeperServer)
[33;1mconnect_1             |[0m + [[ org.apache.kafka.connect.json.JsonConverter == \i\o\.\c\o\n\f\l\u\e\n\t\.\c\o\n\n\e\c\t\.\a\v\r\o\.\A\v\r\o\C\o\n\v\e\r\t\e\r ]]
[34mschema-registry_1     |[0m + cub kafka-ready 1 40 -z zookeeper:32181
[35mkafka_1               |[0m     echo "KAFKA_OPTS should contain 'com.sun.management.jmxremote.rmi.port' property. It is required for accessing the JMX metrics externally."
[33;1mconnect_1             |[0m 
[33mzookeeper_1           |[0m [2017-10-25 05:25:26,850] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)
[35mkafka_1               |[0m   fi
[33;1mconnect_1             |[0m dub path /etc/"${COMPONENT}"/ writable
[34mschema-registry_1     |[0m Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
[35mkafka_1               |[0m fi
[33mzookeeper_1           |[0m [2017-10-25 05:25:26,930] INFO Established session 0x15f51fe0e370000 with negotiated timeout 40000 for client /172.21.0.5:40118 (org.apache.zookeeper.server.ZooKeeperServer)
[33;1mconnect_1             |[0m + dub path /etc/kafka-connect/ writable
[35mkafka_1               |[0m + [[ -n '' ]]
[34mschema-registry_1     |[0m Client environment:host.name=schema-registry
[33mzookeeper_1           |[0m [2017-10-25 05:25:26,939] INFO Processed session termination for sessionid: 0x15f51fe0e370000 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m 
[33;1mconnect_1             |[0m 
[35mkafka_1               |[0m dub template "/etc/confluent/docker/${COMPONENT}.properties.template" "/etc/${COMPONENT}/${COMPONENT}.properties"
[33;1mconnect_1             |[0m dub template "/etc/confluent/docker/${COMPONENT}.properties.template" "/etc/${COMPONENT}/${COMPONENT}.properties"
[35mkafka_1               |[0m + dub template /etc/confluent/docker/kafka.properties.template /etc/kafka/kafka.properties
[34mschema-registry_1     |[0m Client environment:java.version=1.8.0_102
[33;1mconnect_1             |[0m + dub template /etc/confluent/docker/kafka-connect.properties.template /etc/kafka-connect/kafka-connect.properties
[35mkafka_1               |[0m dub template "/etc/confluent/docker/log4j.properties.template" "/etc/${COMPONENT}/log4j.properties"
[33mzookeeper_1           |[0m [2017-10-25 05:25:26,969] INFO Closed socket connection for client /172.21.0.5:40118 which had sessionid 0x15f51fe0e370000 (org.apache.zookeeper.server.NIOServerCnxn)
[35mkafka_1               |[0m + dub template /etc/confluent/docker/log4j.properties.template /etc/kafka/log4j.properties
[33;1mconnect_1             |[0m 
[35mkafka_1               |[0m dub template "/etc/confluent/docker/tools-log4j.properties.template" "/etc/${COMPONENT}/tools-log4j.properties"
[33;1mconnect_1             |[0m # The connect-distributed script expects the log4j config at /etc/kafka/connect-log4j.properties.
[33;1mconnect_1             |[0m dub template "/etc/confluent/docker/log4j.properties.template" "/etc/kafka/connect-log4j.properties"
[35mkafka_1               |[0m + dub template /etc/confluent/docker/tools-log4j.properties.template /etc/kafka/tools-log4j.properties
[33;1mconnect_1             |[0m + dub template /etc/confluent/docker/log4j.properties.template /etc/kafka/connect-log4j.properties
[35mkafka_1               |[0m 
[33mzookeeper_1           |[0m [2017-10-25 05:25:28,118] INFO Accepted socket connection from /172.21.0.5:40120 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[34mschema-registry_1     |[0m Client environment:java.vendor=Azul Systems, Inc.
[35mkafka_1               |[0m echo "===> Running preflight checks ... "
[34mschema-registry_1     |[0m Client environment:java.home=/usr/lib/jvm/zulu-8-amd64/jre
[35mkafka_1               |[0m + echo '===> Running preflight checks ... '
[33mzookeeper_1           |[0m [2017-10-25 05:25:28,123] INFO Client attempting to establish new session at /172.21.0.5:40120 (org.apache.zookeeper.server.ZooKeeperServer)
[33;1mconnect_1             |[0m 
[35mkafka_1               |[0m /etc/confluent/docker/ensure
[33;1mconnect_1             |[0m echo "===> Running preflight checks ... "
[35mkafka_1               |[0m + /etc/confluent/docker/ensure
[34mschema-registry_1     |[0m Client environment:java.class.path=/etc/confluent/docker/docker-utils.jar
[33mzookeeper_1           |[0m [2017-10-25 05:25:28,154] INFO Established session 0x15f51fe0e370001 with negotiated timeout 6000 for client /172.21.0.5:40120 (org.apache.zookeeper.server.ZooKeeperServer)
[33;1mconnect_1             |[0m + echo '===> Running preflight checks ... '
[35mkafka_1               |[0m 
[33;1mconnect_1             |[0m /etc/confluent/docker/ensure
[35mkafka_1               |[0m export KAFKA_DATA_DIRS=${KAFKA_DATA_DIRS:-"/var/lib/kafka/data"}
[33;1mconnect_1             |[0m + /etc/confluent/docker/ensure
[35mkafka_1               |[0m + export KAFKA_DATA_DIRS=/var/lib/kafka/data
[35mkafka_1               |[0m + KAFKA_DATA_DIRS=/var/lib/kafka/data
[34mschema-registry_1     |[0m Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[33mzookeeper_1           |[0m [2017-10-25 05:25:28,238] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x5 zxid:0x5 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m echo "===> Check if $KAFKA_DATA_DIRS is writable ..."
[35mkafka_1               |[0m + echo '===> Check if /var/lib/kafka/data is writable ...'
[34mschema-registry_1     |[0m Client environment:java.io.tmpdir=/tmp
[33mzookeeper_1           |[0m [2017-10-25 05:25:28,285] INFO Accepted socket connection from /172.21.0.7:58328 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[33;1mconnect_1             |[0m ===> Running preflight checks ... 
[35mkafka_1               |[0m dub path "$KAFKA_DATA_DIRS" writable
[35mkafka_1               |[0m + dub path /var/lib/kafka/data writable
[33mzookeeper_1           |[0m [2017-10-25 05:25:28,293] INFO Client attempting to establish new session at /172.21.0.7:58328 (org.apache.zookeeper.server.ZooKeeperServer)
[34mschema-registry_1     |[0m Client environment:java.compiler=<NA>
[33;1mconnect_1             |[0m ===> Check if Kafka is healthy ...
[33mzookeeper_1           |[0m [2017-10-25 05:25:28,325] INFO Established session 0x15f51fe0e370002 with negotiated timeout 40000 for client /172.21.0.7:58328 (org.apache.zookeeper.server.ZooKeeperServer)
[34mschema-registry_1     |[0m Client environment:os.name=Linux
[35mkafka_1               |[0m ===> Running preflight checks ... 
[33;1mconnect_1             |[0m 
[35mkafka_1               |[0m ===> Check if /var/lib/kafka/data is writable ...
[34mschema-registry_1     |[0m Client environment:os.arch=amd64
[33mzookeeper_1           |[0m [2017-10-25 05:25:28,337] INFO Processed session termination for sessionid: 0x15f51fe0e370002 (org.apache.zookeeper.server.PrepRequestProcessor)
[33;1mconnect_1             |[0m echo "===> Check if Kafka is healthy ..."
[34mschema-registry_1     |[0m Client environment:os.version=4.9.49-moby
[35mkafka_1               |[0m 
[33;1mconnect_1             |[0m + echo '===> Check if Kafka is healthy ...'
[33mzookeeper_1           |[0m [2017-10-25 05:25:28,373] INFO Closed socket connection for client /172.21.0.7:58328 which had sessionid 0x15f51fe0e370002 (org.apache.zookeeper.server.NIOServerCnxn)
[34mschema-registry_1     |[0m Client environment:user.name=root
[35mkafka_1               |[0m echo "===> Check if Zookeeper is healthy ..."
[33;1mconnect_1             |[0m 
[33;1mconnect_1             |[0m if [[ -n "${CONNECT_SECURITY_PROTOCOL-}" ]] && [[ $CONNECT_SECURITY_PROTOCOL = "SSL" ]]
[33mzookeeper_1           |[0m [2017-10-25 05:25:28,430] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0xb zxid:0xb txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config (org.apache.zookeeper.server.PrepRequestProcessor)
[34mschema-registry_1     |[0m Client environment:user.home=/root
[33;1mconnect_1             |[0m then
[35mkafka_1               |[0m + echo '===> Check if Zookeeper is healthy ...'
[34mschema-registry_1     |[0m Client environment:user.dir=/
[33;1mconnect_1             |[0m 
[33mzookeeper_1           |[0m [2017-10-25 05:25:28,565] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x13 zxid:0x10 txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin (org.apache.zookeeper.server.PrepRequestProcessor)
[34mschema-registry_1     |[0m Initiating client connection, connectString=zookeeper:32181 sessionTimeout=40000 watcher=io.confluent.admin.utils.ZookeeperConnectionWatcher@6d5380c2
[33;1mconnect_1             |[0m     cub kafka-ready \
[33;1mconnect_1             |[0m         "${CONNECT_CUB_KAFKA_MIN_BROKERS:-1}" \
[33;1mconnect_1             |[0m         "${CONNECT_CUB_KAFKA_TIMEOUT:-40}" \
[35mkafka_1               |[0m ===> Check if Zookeeper is healthy ...
[34mschema-registry_1     |[0m Opening socket connection to server quickstart_zookeeper_1.quickstart_default/172.21.0.4:32181. Will not attempt to authenticate using SASL (unknown error)
[33mzookeeper_1           |[0m [2017-10-25 05:25:28,770] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x1d zxid:0x16 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster (org.apache.zookeeper.server.PrepRequestProcessor)
[33;1mconnect_1             |[0m         -b "$CONNECT_BOOTSTRAP_SERVERS" \
[35mkafka_1               |[0m cub zk-ready "$KAFKA_ZOOKEEPER_CONNECT" "${KAFKA_CUB_ZK_TIMEOUT:-40}"
[34mschema-registry_1     |[0m Socket connection established to quickstart_zookeeper_1.quickstart_default/172.21.0.4:32181, initiating session
[33;1mconnect_1             |[0m         --config /etc/"${COMPONENT}"/kafka-connect.properties
[35mkafka_1               |[0m + cub zk-ready zookeeper:32181 40
[33mzookeeper_1           |[0m [2017-10-25 05:25:28,942] INFO Accepted socket connection from /172.21.0.7:58330 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[33;1mconnect_1             |[0m else
[34mschema-registry_1     |[0m Session establishment complete on server quickstart_zookeeper_1.quickstart_default/172.21.0.4:32181, sessionid = 0x15f51fe0e370003, negotiated timeout = 40000
[35mkafka_1               |[0m Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
[33;1mconnect_1             |[0m 
[33mzookeeper_1           |[0m [2017-10-25 05:25:28,949] INFO Client attempting to establish new session at /172.21.0.7:58330 (org.apache.zookeeper.server.ZooKeeperServer)
[34mschema-registry_1     |[0m Session: 0x15f51fe0e370003 closed
[35mkafka_1               |[0m Client environment:host.name=kafka
[33;1mconnect_1             |[0m     cub kafka-ready \
[33mzookeeper_1           |[0m [2017-10-25 05:25:28,983] INFO Established session 0x15f51fe0e370003 with negotiated timeout 40000 for client /172.21.0.7:58330 (org.apache.zookeeper.server.ZooKeeperServer)
[33;1mconnect_1             |[0m         "${CONNECT_CUB_KAFKA_MIN_BROKERS:-1}" \
[34mschema-registry_1     |[0m EventThread shut down
[35mkafka_1               |[0m Client environment:java.version=1.8.0_102
[33;1mconnect_1             |[0m         "${CONNECT_CUB_KAFKA_TIMEOUT:-40}" \
[35mkafka_1               |[0m Client environment:java.vendor=Azul Systems, Inc.
[33;1mconnect_1             |[0m         -b "$CONNECT_BOOTSTRAP_SERVERS"
[34mschema-registry_1     |[0m Initiating client connection, connectString=zookeeper:32181 sessionTimeout=40000 watcher=io.confluent.admin.utils.ZookeeperConnectionWatcher@5d6f64b1
[35mkafka_1               |[0m Client environment:java.home=/usr/lib/jvm/zulu-8-amd64/jre
[33mzookeeper_1           |[0m [2017-10-25 05:25:28,991] INFO Processed session termination for sessionid: 0x15f51fe0e370003 (org.apache.zookeeper.server.PrepRequestProcessor)
[33;1mconnect_1             |[0m fi
[35mkafka_1               |[0m Client environment:java.class.path=/etc/confluent/docker/docker-utils.jar
[34mschema-registry_1     |[0m Opening socket connection to server quickstart_zookeeper_1.quickstart_default/172.21.0.4:32181. Will not attempt to authenticate using SASL (unknown error)
[33;1mconnect_1             |[0m + [[ -n '' ]]
[33mzookeeper_1           |[0m [2017-10-25 05:25:29,015] INFO Closed socket connection for client /172.21.0.7:58330 which had sessionid 0x15f51fe0e370003 (org.apache.zookeeper.server.NIOServerCnxn)
[33;1mconnect_1             |[0m + cub kafka-ready 1 40 -b kafka:29092
[35mkafka_1               |[0m Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[34mschema-registry_1     |[0m Socket connection established to quickstart_zookeeper_1.quickstart_default/172.21.0.4:32181, initiating session
[33mzookeeper_1           |[0m [2017-10-25 05:25:29,020] INFO Accepted socket connection from /172.21.0.7:58332 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[35mkafka_1               |[0m Client environment:java.io.tmpdir=/tmp
[33;1mconnect_1             |[0m MetadataClientConfig values: 
[33;1mconnect_1             |[0m 	bootstrap.servers = [kafka:29092]
[33;1mconnect_1             |[0m 	max.poll.timeout.ms = 5000
[35mkafka_1               |[0m Client environment:java.compiler=<NA>
[33;1mconnect_1             |[0m 	sasl.jaas.config = null
[35mkafka_1               |[0m Client environment:os.name=Linux
[33;1mconnect_1             |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[33mzookeeper_1           |[0m [2017-10-25 05:25:29,021] INFO Client attempting to establish new session at /172.21.0.7:58332 (org.apache.zookeeper.server.ZooKeeperServer)
[34mschema-registry_1     |[0m Session establishment complete on server quickstart_zookeeper_1.quickstart_default/172.21.0.4:32181, sessionid = 0x15f51fe0e370004, negotiated timeout = 40000
[33;1mconnect_1             |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[33;1mconnect_1             |[0m 	sasl.kerberos.service.name = null
[33mzookeeper_1           |[0m [2017-10-25 05:25:29,045] INFO Established session 0x15f51fe0e370004 with negotiated timeout 40000 for client /172.21.0.7:58332 (org.apache.zookeeper.server.ZooKeeperServer)
[35mkafka_1               |[0m Client environment:os.arch=amd64
[33;1mconnect_1             |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[33mzookeeper_1           |[0m [2017-10-25 05:25:29,546] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:setData cxid:0x29 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m Client environment:os.version=4.9.49-moby
[33;1mconnect_1             |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36;1mksql-datagen-users_1  |[0m Waiting for Confluent Schema Registry to be ready...
[34mschema-registry_1     |[0m Session: 0x15f51fe0e370004 closed
[35mkafka_1               |[0m Client environment:user.name=root
[33;1mconnect_1             |[0m 	sasl.mechanism = GSSAPI
[33mzookeeper_1           |[0m [2017-10-25 05:25:30,011] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:delete cxid:0x43 zxid:0x20 txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election (org.apache.zookeeper.server.PrepRequestProcessor)
[34mschema-registry_1     |[0m EventThread shut down
[33;1mconnect_1             |[0m 	security.protocol = PLAINTEXT
[33;1mconnect_1             |[0m 	ssl.cipher.suites = null
[35mkafka_1               |[0m Client environment:user.home=/root
[33;1mconnect_1             |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[33mzookeeper_1           |[0m [2017-10-25 05:25:30,094] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x46 zxid:0x21 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m Client environment:user.dir=/
[34mschema-registry_1     |[0m MetadataClientConfig values: 
[33;1mconnect_1             |[0m 	ssl.endpoint.identification.algorithm = null
[34mschema-registry_1     |[0m 	bootstrap.servers = [kafka:29092]
[33;1mconnect_1             |[0m 	ssl.key.password = null
[35mkafka_1               |[0m Initiating client connection, connectString=zookeeper:32181 sessionTimeout=40000 watcher=io.confluent.admin.utils.ZookeeperConnectionWatcher@5a2e4553
[33mzookeeper_1           |[0m [2017-10-25 05:25:30,095] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x47 zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[34mschema-registry_1     |[0m 	max.poll.timeout.ms = 5000
[33;1mconnect_1             |[0m 	ssl.keymanager.algorithm = SunX509
[34mschema-registry_1     |[0m 	sasl.jaas.config = null
[35mkafka_1               |[0m Opening socket connection to server quickstart_zookeeper_1.quickstart_default/172.21.0.4:32181. Will not attempt to authenticate using SASL (unknown error)
[33mzookeeper_1           |[0m [2017-10-25 05:25:30,192] INFO Processed session termination for sessionid: 0x15f51fe0e370004 (org.apache.zookeeper.server.PrepRequestProcessor)
[33;1mconnect_1             |[0m 	ssl.keystore.location = null
[34mschema-registry_1     |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[33;1mconnect_1             |[0m 	ssl.keystore.password = null
[35mkafka_1               |[0m Socket connection established to quickstart_zookeeper_1.quickstart_default/172.21.0.4:32181, initiating session
[33mzookeeper_1           |[0m [2017-10-25 05:25:30,226] INFO Closed socket connection for client /172.21.0.7:58332 which had sessionid 0x15f51fe0e370004 (org.apache.zookeeper.server.NIOServerCnxn)
[34mschema-registry_1     |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[33;1mconnect_1             |[0m 	ssl.keystore.type = JKS
[34mschema-registry_1     |[0m 	sasl.kerberos.service.name = null
[35mkafka_1               |[0m Session establishment complete on server quickstart_zookeeper_1.quickstart_default/172.21.0.4:32181, sessionid = 0x15f51fe0e370000, negotiated timeout = 40000
[33;1mconnect_1             |[0m 	ssl.protocol = TLS
[34mschema-registry_1     |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[33;1mconnect_1             |[0m 	ssl.provider = null
[35mkafka_1               |[0m Session: 0x15f51fe0e370000 closed
[34mschema-registry_1     |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[33;1mconnect_1             |[0m 	ssl.secure.random.implementation = null
[34mschema-registry_1     |[0m 	sasl.mechanism = GSSAPI
[33;1mconnect_1             |[0m 	ssl.trustmanager.algorithm = PKIX
[34mschema-registry_1     |[0m 	security.protocol = PLAINTEXT
[33;1mconnect_1             |[0m 	ssl.truststore.location = null
[34mschema-registry_1     |[0m 	ssl.cipher.suites = null
[33;1mconnect_1             |[0m 	ssl.truststore.password = null
[35mkafka_1               |[0m EventThread shut down
[34mschema-registry_1     |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[33;1mconnect_1             |[0m 	ssl.truststore.type = JKS
[34mschema-registry_1     |[0m 	ssl.endpoint.identification.algorithm = null
[33;1mconnect_1             |[0m 
[34mschema-registry_1     |[0m 	ssl.key.password = null
[35mkafka_1               |[0m ===> Launching ... 
[34mschema-registry_1     |[0m 	ssl.keymanager.algorithm = SunX509
[34mschema-registry_1     |[0m 	ssl.keystore.location = null
[34mschema-registry_1     |[0m 	ssl.keystore.password = null
[34mschema-registry_1     |[0m 	ssl.keystore.type = JKS
[34mschema-registry_1     |[0m 	ssl.protocol = TLS
[34mschema-registry_1     |[0m 	ssl.provider = null
[34mschema-registry_1     |[0m 	ssl.secure.random.implementation = null
[34mschema-registry_1     |[0m 	ssl.trustmanager.algorithm = PKIX
[34mschema-registry_1     |[0m 	ssl.truststore.location = null
[34mschema-registry_1     |[0m 	ssl.truststore.password = null
[34mschema-registry_1     |[0m 	ssl.truststore.type = JKS
[34mschema-registry_1     |[0m 
[35mkafka_1               |[0m 
[34mschema-registry_1     |[0m 
[35mkafka_1               |[0m echo "===> Launching ... "
[34mschema-registry_1     |[0m echo "===> Launching ... "
[35mkafka_1               |[0m + echo '===> Launching ... '
[35mkafka_1               |[0m exec /etc/confluent/docker/launch
[35mkafka_1               |[0m + exec /etc/confluent/docker/launch
[34mschema-registry_1     |[0m + echo '===> Launching ... '
[34mschema-registry_1     |[0m exec /etc/confluent/docker/launch
[34mschema-registry_1     |[0m + exec /etc/confluent/docker/launch
[35mkafka_1               |[0m ===> Launching kafka ... 
[34mschema-registry_1     |[0m ===> Launching ... 
[35mkafka_1               |[0m [2017-10-25 05:25:27,910] INFO KafkaConfig values: 
[35mkafka_1               |[0m 	advertised.host.name = null
[35mkafka_1               |[0m 	advertised.listeners = PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
[34mschema-registry_1     |[0m ===> Launching schema-registry ... 
[35mkafka_1               |[0m 	advertised.port = null
[35mkafka_1               |[0m 	alter.config.policy.class.name = null
[34mschema-registry_1     |[0m [2017-10-25 05:25:30,983] INFO SchemaRegistryConfig values: 
[35mkafka_1               |[0m 	authorizer.class.name = 
[34mschema-registry_1     |[0m 	metric.reporters = []
[35mkafka_1               |[0m 	auto.create.topics.enable = true
[34mschema-registry_1     |[0m 	kafkastore.sasl.kerberos.kinit.cmd = /usr/bin/kinit
[35mkafka_1               |[0m 	auto.leader.rebalance.enable = true
[34mschema-registry_1     |[0m 	response.mediatype.default = application/vnd.schemaregistry.v1+json
[35mkafka_1               |[0m 	background.threads = 10
[34mschema-registry_1     |[0m 	kafkastore.ssl.trustmanager.algorithm = PKIX
[35mkafka_1               |[0m 	broker.id = 1
[34mschema-registry_1     |[0m 	authentication.realm = 
[35mkafka_1               |[0m 	broker.id.generation.enable = true
[34mschema-registry_1     |[0m 	ssl.keystore.type = JKS
[35mkafka_1               |[0m 	broker.rack = null
[34mschema-registry_1     |[0m 	kafkastore.topic = _schemas
[34mschema-registry_1     |[0m 	metrics.jmx.prefix = kafka.schema.registry
[34mschema-registry_1     |[0m 	kafkastore.ssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1
[34mschema-registry_1     |[0m 	kafkastore.topic.replication.factor = 3
[35mkafka_1               |[0m 	compression.type = producer
[34mschema-registry_1     |[0m 	ssl.truststore.password = 
[35mkafka_1               |[0m 	connections.max.idle.ms = 600000
[34mschema-registry_1     |[0m 	kafkastore.timeout.ms = 500
[35mkafka_1               |[0m 	controlled.shutdown.enable = true
[34mschema-registry_1     |[0m 	host.name = schema-registry
[35mkafka_1               |[0m 	controlled.shutdown.max.retries = 3
[34mschema-registry_1     |[0m 	kafkastore.bootstrap.servers = []
[35mkafka_1               |[0m 	controlled.shutdown.retry.backoff.ms = 5000
[34mschema-registry_1     |[0m 	schema.registry.zk.namespace = schema_registry
[35mkafka_1               |[0m 	controller.socket.timeout.ms = 30000
[34mschema-registry_1     |[0m 	kafkastore.sasl.kerberos.ticket.renew.window.factor = 0.8
[35mkafka_1               |[0m 	create.topic.policy.class.name = null
[34mschema-registry_1     |[0m 	kafkastore.sasl.kerberos.service.name = 
[35mkafka_1               |[0m 	default.replication.factor = 1
[34mschema-registry_1     |[0m 	ssl.endpoint.identification.algorithm = 
[35mkafka_1               |[0m 	delete.records.purgatory.purge.interval.requests = 1
[34mschema-registry_1     |[0m 	compression.enable = false
[35mkafka_1               |[0m 	delete.topic.enable = false
[34mschema-registry_1     |[0m 	kafkastore.ssl.truststore.type = JKS
[35mkafka_1               |[0m 	fetch.purgatory.purge.interval.requests = 1000
[34mschema-registry_1     |[0m 	avro.compatibility.level = backward
[35mkafka_1               |[0m 	group.initial.rebalance.delay.ms = 3000
[34mschema-registry_1     |[0m 	kafkastore.ssl.protocol = TLS
[35mkafka_1               |[0m 	group.max.session.timeout.ms = 300000
[34mschema-registry_1     |[0m 	kafkastore.ssl.provider = 
[35mkafka_1               |[0m 	group.min.session.timeout.ms = 6000
[34mschema-registry_1     |[0m 	kafkastore.ssl.truststore.location = 
[35mkafka_1               |[0m 	host.name = 
[34mschema-registry_1     |[0m 	response.mediatype.preferred = [application/vnd.schemaregistry.v1+json, application/vnd.schemaregistry+json, application/json]
[35mkafka_1               |[0m 	inter.broker.listener.name = PLAINTEXT
[34mschema-registry_1     |[0m 	kafkastore.ssl.keystore.type = JKS
[35mkafka_1               |[0m 	inter.broker.protocol.version = 0.11.0-IV2
[34mschema-registry_1     |[0m 	ssl.truststore.type = JKS
[35mkafka_1               |[0m 	leader.imbalance.check.interval.seconds = 300
[34mschema-registry_1     |[0m 	kafkastore.ssl.truststore.password = 
[35mkafka_1               |[0m 	leader.imbalance.per.broker.percentage = 10
[35mkafka_1               |[0m 	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
[34mschema-registry_1     |[0m 	access.control.allow.origin = 
[35mkafka_1               |[0m 	listeners = PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
[34mschema-registry_1     |[0m 	ssl.truststore.location = 
[35mkafka_1               |[0m 	log.cleaner.backoff.ms = 15000
[34mschema-registry_1     |[0m 	ssl.keystore.password = 
[35mkafka_1               |[0m 	log.cleaner.dedupe.buffer.size = 134217728
[34mschema-registry_1     |[0m 	port = 8081
[35mkafka_1               |[0m 	log.cleaner.delete.retention.ms = 86400000
[34mschema-registry_1     |[0m 	kafkastore.ssl.keystore.location = 
[35mkafka_1               |[0m 	log.cleaner.enable = true
[34mschema-registry_1     |[0m 	master.eligibility = true
[35mkafka_1               |[0m 	log.cleaner.io.buffer.load.factor = 0.9
[34mschema-registry_1     |[0m 	ssl.client.auth = false
[35mkafka_1               |[0m 	log.cleaner.io.buffer.size = 524288
[34mschema-registry_1     |[0m 	kafkastore.ssl.keystore.password = 
[35mkafka_1               |[0m 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
[34mschema-registry_1     |[0m 	kafkastore.security.protocol = PLAINTEXT
[35mkafka_1               |[0m 	log.cleaner.min.cleanable.ratio = 0.5
[34mschema-registry_1     |[0m 	ssl.trustmanager.algorithm = 
[35mkafka_1               |[0m 	log.cleaner.min.compaction.lag.ms = 0
[34mschema-registry_1     |[0m 	authentication.method = NONE
[35mkafka_1               |[0m 	log.cleaner.threads = 1
[34mschema-registry_1     |[0m 	request.logger.name = io.confluent.rest-utils.requests
[35mkafka_1               |[0m 	log.cleanup.policy = [delete]
[34mschema-registry_1     |[0m 	ssl.key.password = 
[35mkafka_1               |[0m 	log.dir = /tmp/kafka-logs
[34mschema-registry_1     |[0m 	kafkastore.zk.session.timeout.ms = 30000
[35mkafka_1               |[0m 	log.dirs = /var/lib/kafka/data
[34mschema-registry_1     |[0m 	kafkastore.sasl.mechanism = GSSAPI
[35mkafka_1               |[0m 	log.flush.interval.messages = 9223372036854775807
[34mschema-registry_1     |[0m 	kafkastore.sasl.kerberos.ticket.renew.jitter = 0.05
[35mkafka_1               |[0m 	log.flush.interval.ms = null
[34mschema-registry_1     |[0m 	kafkastore.ssl.key.password = 
[35mkafka_1               |[0m 	log.flush.offset.checkpoint.interval.ms = 60000
[34mschema-registry_1     |[0m 	zookeeper.set.acl = false
[35mkafka_1               |[0m 	log.flush.scheduler.interval.ms = 9223372036854775807
[34mschema-registry_1     |[0m 	authentication.roles = [*]
[35mkafka_1               |[0m 	log.flush.start.offset.checkpoint.interval.ms = 60000
[34mschema-registry_1     |[0m 	metrics.num.samples = 2
[35mkafka_1               |[0m 	log.index.interval.bytes = 4096
[34mschema-registry_1     |[0m 	ssl.protocol = TLS
[35mkafka_1               |[0m 	log.index.size.max.bytes = 10485760
[34mschema-registry_1     |[0m 	kafkastore.ssl.keymanager.algorithm = SunX509
[35mkafka_1               |[0m 	log.message.format.version = 0.11.0-IV2
[34mschema-registry_1     |[0m 	kafkastore.connection.url = zookeeper:32181
[35mkafka_1               |[0m 	log.message.timestamp.difference.max.ms = 9223372036854775807
[34mschema-registry_1     |[0m 	debug = false
[35mkafka_1               |[0m 	log.message.timestamp.type = CreateTime
[34mschema-registry_1     |[0m 	listeners = []
[35mkafka_1               |[0m 	log.preallocate = false
[34mschema-registry_1     |[0m 	kafkastore.group.id = 
[35mkafka_1               |[0m 	log.retention.bytes = -1
[34mschema-registry_1     |[0m 	ssl.provider = 
[35mkafka_1               |[0m 	log.retention.check.interval.ms = 300000
[34mschema-registry_1     |[0m 	ssl.enabled.protocols = []
[35mkafka_1               |[0m 	log.retention.hours = 168
[34mschema-registry_1     |[0m 	shutdown.graceful.ms = 1000
[35mkafka_1               |[0m 	log.retention.minutes = null
[34mschema-registry_1     |[0m 	ssl.keystore.location = 
[35mkafka_1               |[0m 	log.retention.ms = null
[34mschema-registry_1     |[0m 	ssl.cipher.suites = []
[35mkafka_1               |[0m 	log.roll.hours = 168
[34mschema-registry_1     |[0m 	kafkastore.ssl.endpoint.identification.algorithm = 
[35mkafka_1               |[0m 	log.roll.jitter.hours = 0
[34mschema-registry_1     |[0m 	kafkastore.ssl.cipher.suites = 
[35mkafka_1               |[0m 	log.roll.jitter.ms = null
[34mschema-registry_1     |[0m 	access.control.allow.methods = 
[35mkafka_1               |[0m 	log.roll.ms = null
[34mschema-registry_1     |[0m 	kafkastore.sasl.kerberos.min.time.before.relogin = 60000
[35mkafka_1               |[0m 	log.segment.bytes = 1073741824
[34mschema-registry_1     |[0m 	ssl.keymanager.algorithm = 
[35mkafka_1               |[0m 	log.segment.delete.delay.ms = 60000
[34mschema-registry_1     |[0m 	metrics.sample.window.ms = 30000
[35mkafka_1               |[0m 	max.connections.per.ip = 2147483647
[34mschema-registry_1     |[0m 	kafkastore.init.timeout.ms = 60000
[35mkafka_1               |[0m 	max.connections.per.ip.overrides = 
[34mschema-registry_1     |[0m  (io.confluent.kafka.schemaregistry.rest.SchemaRegistryConfig)
[35mkafka_1               |[0m 	message.max.bytes = 1000012
[35mkafka_1               |[0m 	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
[35mkafka_1               |[0m 	metrics.num.samples = 2
[35mkafka_1               |[0m 	metrics.recording.level = INFO
[35mkafka_1               |[0m 	metrics.sample.window.ms = 30000
[35mkafka_1               |[0m 	min.insync.replicas = 1
[35mkafka_1               |[0m 	num.io.threads = 8
[35mkafka_1               |[0m 	num.network.threads = 3
[35mkafka_1               |[0m 	num.partitions = 1
[35mkafka_1               |[0m 	num.recovery.threads.per.data.dir = 1
[35mkafka_1               |[0m 	num.replica.fetchers = 1
[35mkafka_1               |[0m 	offset.metadata.max.bytes = 4096
[35mkafka_1               |[0m 	offsets.commit.required.acks = -1
[35mkafka_1               |[0m 	offsets.commit.timeout.ms = 5000
[35mkafka_1               |[0m 	offsets.load.buffer.size = 5242880
[35mkafka_1               |[0m 	offsets.retention.check.interval.ms = 600000
[35mkafka_1               |[0m 	offsets.retention.minutes = 1440
[35mkafka_1               |[0m 	offsets.topic.compression.codec = 0
[35mkafka_1               |[0m 	offsets.topic.num.partitions = 50
[35mkafka_1               |[0m 	offsets.topic.replication.factor = 1
[35mkafka_1               |[0m 	offsets.topic.segment.bytes = 104857600
[35mkafka_1               |[0m 	port = 9092
[35mkafka_1               |[0m 	principal.builder.class = class org.apache.kafka.common.security.auth.DefaultPrincipalBuilder
[35mkafka_1               |[0m 	producer.purgatory.purge.interval.requests = 1000
[35mkafka_1               |[0m 	queued.max.requests = 500
[35mkafka_1               |[0m 	quota.consumer.default = 9223372036854775807
[35mkafka_1               |[0m 	quota.producer.default = 9223372036854775807
[35mkafka_1               |[0m 	quota.window.num = 11
[35mkafka_1               |[0m 	quota.window.size.seconds = 1
[35mkafka_1               |[0m 	replica.fetch.backoff.ms = 1000
[35mkafka_1               |[0m 	replica.fetch.max.bytes = 1048576
[35mkafka_1               |[0m 	replica.fetch.min.bytes = 1
[35mkafka_1               |[0m 	replica.fetch.response.max.bytes = 10485760
[35mkafka_1               |[0m 	replica.fetch.wait.max.ms = 500
[35mkafka_1               |[0m 	replica.high.watermark.checkpoint.interval.ms = 5000
[35mkafka_1               |[0m 	replica.lag.time.max.ms = 10000
[35mkafka_1               |[0m 	replica.socket.receive.buffer.bytes = 65536
[35mkafka_1               |[0m 	replica.socket.timeout.ms = 30000
[35mkafka_1               |[0m 	replication.quota.window.num = 11
[35mkafka_1               |[0m 	replication.quota.window.size.seconds = 1
[35mkafka_1               |[0m 	request.timeout.ms = 30000
[35mkafka_1               |[0m 	reserved.broker.max.id = 1000
[35mkafka_1               |[0m 	sasl.enabled.mechanisms = [GSSAPI]
[35mkafka_1               |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[35mkafka_1               |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[35mkafka_1               |[0m 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
[35mkafka_1               |[0m 	sasl.kerberos.service.name = null
[35mkafka_1               |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[35mkafka_1               |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[35mkafka_1               |[0m 	sasl.mechanism.inter.broker.protocol = GSSAPI
[35mkafka_1               |[0m 	security.inter.broker.protocol = PLAINTEXT
[35mkafka_1               |[0m 	socket.receive.buffer.bytes = 102400
[35mkafka_1               |[0m 	socket.request.max.bytes = 104857600
[35mkafka_1               |[0m 	socket.send.buffer.bytes = 102400
[35mkafka_1               |[0m 	ssl.cipher.suites = null
[35mkafka_1               |[0m 	ssl.client.auth = none
[35mkafka_1               |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[35mkafka_1               |[0m 	ssl.endpoint.identification.algorithm = null
[35mkafka_1               |[0m 	ssl.key.password = null
[35mkafka_1               |[0m 	ssl.keymanager.algorithm = SunX509
[35mkafka_1               |[0m 	ssl.keystore.location = null
[35mkafka_1               |[0m 	ssl.keystore.password = null
[35mkafka_1               |[0m 	ssl.keystore.type = JKS
[35mkafka_1               |[0m 	ssl.protocol = TLS
[35mkafka_1               |[0m 	ssl.provider = null
[35mkafka_1               |[0m 	ssl.secure.random.implementation = null
[35mkafka_1               |[0m 	ssl.trustmanager.algorithm = PKIX
[35mkafka_1               |[0m 	ssl.truststore.location = null
[35mkafka_1               |[0m 	ssl.truststore.password = null
[35mkafka_1               |[0m 	ssl.truststore.type = JKS
[35mkafka_1               |[0m 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
[35mkafka_1               |[0m 	transaction.max.timeout.ms = 900000
[35mkafka_1               |[0m 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
[35mkafka_1               |[0m 	transaction.state.log.load.buffer.size = 5242880
[35mkafka_1               |[0m 	transaction.state.log.min.isr = 2
[35mkafka_1               |[0m 	transaction.state.log.num.partitions = 50
[35mkafka_1               |[0m 	transaction.state.log.replication.factor = 3
[35mkafka_1               |[0m 	transaction.state.log.segment.bytes = 104857600
[35mkafka_1               |[0m 	transactional.id.expiration.ms = 604800000
[35mkafka_1               |[0m 	unclean.leader.election.enable = false
[35mkafka_1               |[0m 	zookeeper.connect = zookeeper:32181
[35mkafka_1               |[0m 	zookeeper.connection.timeout.ms = null
[35mkafka_1               |[0m 	zookeeper.session.timeout.ms = 6000
[35mkafka_1               |[0m 	zookeeper.set.acl = false
[35mkafka_1               |[0m 	zookeeper.sync.time.ms = 2000
[35mkafka_1               |[0m  (kafka.server.KafkaConfig)
[35mkafka_1               |[0m [2017-10-25 05:25:27,981] WARN The support metrics collection feature ("Metrics") of Proactive Support is disabled. (io.confluent.support.metrics.SupportedServerStartable)
[35mkafka_1               |[0m [2017-10-25 05:25:27,982] INFO starting (kafka.server.KafkaServer)
[35mkafka_1               |[0m [2017-10-25 05:25:27,984] INFO Connecting to zookeeper on zookeeper:32181 (kafka.server.KafkaServer)
[35mkafka_1               |[0m [2017-10-25 05:25:28,001] INFO Starting ZkClient event thread. (org.I0Itec.zkclient.ZkEventThread)
[35mkafka_1               |[0m [2017-10-25 05:25:28,008] INFO Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT (org.apache.zookeeper.ZooKeeper)
[35mkafka_1               |[0m [2017-10-25 05:25:28,008] INFO Client environment:host.name=kafka (org.apache.zookeeper.ZooKeeper)
[35mkafka_1               |[0m [2017-10-25 05:25:28,008] INFO Client environment:java.version=1.8.0_102 (org.apache.zookeeper.ZooKeeper)
[35mkafka_1               |[0m [2017-10-25 05:25:28,008] INFO Client environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.ZooKeeper)
[35mkafka_1               |[0m [2017-10-25 05:25:28,008] INFO Client environment:java.home=/usr/lib/jvm/zulu-8-amd64/jre (org.apache.zookeeper.ZooKeeper)
[35mkafka_1               |[0m [2017-10-25 05:25:28,009] INFO Client environment:java.class.path=:/usr/bin/../share/java/kafka/kafka_2.11-0.11.0.0-cp1-javadoc.jar:/usr/bin/../share/java/kafka/xz-1.5.jar:/usr/bin/../share/java/kafka/commons-digester-1.8.1.jar:/usr/bin/../share/java/kafka/kafka-clients-0.11.0.0-cp1.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.1.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.8.5.jar:/usr/bin/../share/java/kafka/commons-beanutils-1.8.3.jar:/usr/bin/../share/java/kafka/jetty-security-9.2.15.v20160210.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.3.jar:/usr/bin/../share/java/kafka/commons-logging-1.2.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/kafka-streams-0.11.0.0-cp1.jar:/usr/bin/../share/java/kafka/support-metrics-common-3.3.0.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.8.5.jar:/usr/bin/../share/java/kafka/jersey-media-jaxb-2.24.jar:/usr/bin/../share/java/kafka/javax.inject-1.jar:/usr/bin/../share/java/kafka/connect-json-0.11.0.0-cp1.jar:/usr/bin/../share/java/kafka/slf4j-log4j12-1.7.25.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.5.0-b05.jar:/usr/bin/../share/java/kafka/lz4-1.3.0.jar:/usr/bin/../share/java/kafka/kafka_2.11-0.11.0.0-cp1-sources.jar:/usr/bin/../share/java/kafka/zookeeper-3.4.10.jar:/usr/bin/../share/java/kafka/commons-lang3-3.5.jar:/usr/bin/../share/java/kafka/httpmime-4.5.2.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-0.11.0.0-cp1.jar:/usr/bin/../share/java/kafka/scala-library-2.11.11.jar:/usr/bin/../share/java/kafka/jersey-common-2.24.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/plexus-utils-3.0.24.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.0.1.jar:/usr/bin/../share/java/kafka/commons-lang3-3.1.jar:/usr/bin/../share/java/kafka/rocksdbjni-5.0.1.jar:/usr/bin/../share/java/kafka/guava-20.0.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.25.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.2.15.v20160210.jar:/usr/bin/../share/java/kafka/connect-api-0.11.0.0-cp1.jar:/usr/bin/../share/java/kafka/jetty-io-9.2.15.v20160210.jar:/usr/bin/../share/java/kafka/commons-collections-3.2.1.jar:/usr/bin/../share/java/kafka/avro-1.8.2.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.24.jar:/usr/bin/../share/java/kafka/jackson-databind-2.8.5.jar:/usr/bin/../share/java/kafka/kafka_2.11-0.11.0.0-cp1-test.jar:/usr/bin/../share/java/kafka/maven-artifact-3.5.0.jar:/usr/bin/../share/java/kafka/jackson-core-asl-1.9.13.jar:/usr/bin/../share/java/kafka/jackson-mapper-asl-1.9.13.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.8.5.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.2.15.v20160210.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.24.jar:/usr/bin/../share/java/kafka/jetty-util-9.2.15.v20160210.jar:/usr/bin/../share/java/kafka/support-metrics-client-3.3.0.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.2.6.jar:/usr/bin/../share/java/kafka/javax.inject-2.5.0-b05.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.2.15.v20160210.jar:/usr/bin/../share/java/kafka/zkclient-0.10.jar:/usr/bin/../share/java/kafka/jersey-server-2.24.jar:/usr/bin/../share/java/kafka/jetty-server-9.2.15.v20160210.jar:/usr/bin/../share/java/kafka/hk2-locator-2.5.0-b05.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.8.5.jar:/usr/bin/../share/java/kafka/kafka_2.11-0.11.0.0-cp1-test-sources.jar:/usr/bin/../share/java/kafka/javax.annotation-api-1.2.jar:/usr/bin/../share/java/kafka/kafka_2.11-0.11.0.0-cp1-scaladoc.jar:/usr/bin/../share/java/kafka/commons-compress-1.8.1.jar:/usr/bin/../share/java/kafka/jersey-guava-2.24.jar:/usr/bin/../share/java/kafka/scala-parser-combinators_2.11-1.0.4.jar:/usr/bin/../share/java/kafka/log4j-1.2.17.jar:/usr/bin/../share/java/kafka/paranamer-2.7.jar:/usr/bin/../share/java/kafka/jersey-client-2.24.jar:/usr/bin/../share/java/kafka/hk2-utils-2.5.0-b05.jar:/usr/bin/../share/java/kafka/connect-runtime-0.11.0.0-cp1.jar:/usr/bin/../share/java/kafka/validation-api-1.1.0.Final.jar:/usr/bin/../share/java/kafka/reflections-0.9.11.jar:/usr/bin/../share/java/kafka/commons-codec-1.9.jar:/usr/bin/../share/java/kafka/connect-transforms-0.11.0.0-cp1.jar:/usr/bin/../share/java/kafka/javassist-3.21.0-GA.jar:/usr/bin/../share/java/kafka/kafka-tools-0.11.0.0-cp1.jar:/usr/bin/../share/java/kafka/commons-validator-1.4.1.jar:/usr/bin/../share/java/kafka/hk2-api-2.5.0-b05.jar:/usr/bin/../share/java/kafka/jackson-core-2.8.5.jar:/usr/bin/../share/java/kafka/httpclient-4.5.2.jar:/usr/bin/../share/java/kafka/httpcore-4.4.4.jar:/usr/bin/../share/java/kafka/connect-file-0.11.0.0-cp1.jar:/usr/bin/../share/java/kafka/kafka_2.11-0.11.0.0-cp1.jar:/usr/bin/../share/java/kafka/jetty-http-9.2.15.v20160210.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-0.11.0.0-cp1.jar:/usr/bin/../share/java/kafka/confluent-metrics-3.3.0.jar:/usr/bin/../share/java/confluent-support-metrics/support-metrics-fullcollector-3.3.0.jar:/usr/share/java/confluent-support-metrics/support-metrics-fullcollector-3.3.0.jar (org.apache.zookeeper.ZooKeeper)
[35mkafka_1               |[0m [2017-10-25 05:25:28,009] INFO Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
[35mkafka_1               |[0m [2017-10-25 05:25:28,009] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
[35mkafka_1               |[0m [2017-10-25 05:25:28,009] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[35mkafka_1               |[0m [2017-10-25 05:25:28,010] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
[35mkafka_1               |[0m [2017-10-25 05:25:28,010] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[35mkafka_1               |[0m [2017-10-25 05:25:28,010] INFO Client environment:os.version=4.9.49-moby (org.apache.zookeeper.ZooKeeper)
[35mkafka_1               |[0m [2017-10-25 05:25:28,010] INFO Client environment:user.name=root (org.apache.zookeeper.ZooKeeper)
[35mkafka_1               |[0m [2017-10-25 05:25:28,010] INFO Client environment:user.home=/root (org.apache.zookeeper.ZooKeeper)
[35mkafka_1               |[0m [2017-10-25 05:25:28,010] INFO Client environment:user.dir=/ (org.apache.zookeeper.ZooKeeper)
[35mkafka_1               |[0m [2017-10-25 05:25:28,012] INFO Initiating client connection, connectString=zookeeper:32181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@68999068 (org.apache.zookeeper.ZooKeeper)
[35mkafka_1               |[0m [2017-10-25 05:25:28,028] INFO Waiting for keeper state SyncConnected (org.I0Itec.zkclient.ZkClient)
[35mkafka_1               |[0m [2017-10-25 05:25:28,032] INFO Opening socket connection to server quickstart_zookeeper_1.quickstart_default/172.21.0.4:32181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[35mkafka_1               |[0m [2017-10-25 05:25:28,119] INFO Socket connection established to quickstart_zookeeper_1.quickstart_default/172.21.0.4:32181, initiating session (org.apache.zookeeper.ClientCnxn)
[35mkafka_1               |[0m [2017-10-25 05:25:28,158] INFO Session establishment complete on server quickstart_zookeeper_1.quickstart_default/172.21.0.4:32181, sessionid = 0x15f51fe0e370001, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[35mkafka_1               |[0m [2017-10-25 05:25:28,161] INFO zookeeper state changed (SyncConnected) (org.I0Itec.zkclient.ZkClient)
[35mkafka_1               |[0m [2017-10-25 05:25:28,851] INFO Cluster ID = C5MgIcbyQGG-Zb34UBMPtA (kafka.server.KafkaServer)
[35mkafka_1               |[0m [2017-10-25 05:25:28,859] WARN No meta.properties file under dir /var/lib/kafka/data/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[35mkafka_1               |[0m [2017-10-25 05:25:28,943] INFO ConfluentMetricsReporterConfig values: 
[35mkafka_1               |[0m 	confluent.metrics.reporter.bootstrap.servers = kafka:29092
[35mkafka_1               |[0m 	confluent.metrics.reporter.publish.ms = 15000
[35mkafka_1               |[0m 	confluent.metrics.reporter.topic = _confluent-metrics
[35mkafka_1               |[0m 	confluent.metrics.reporter.topic.create = true
[35mkafka_1               |[0m 	confluent.metrics.reporter.topic.max.message.bytes = 10485760
[35mkafka_1               |[0m 	confluent.metrics.reporter.topic.partitions = 12
[35mkafka_1               |[0m 	confluent.metrics.reporter.topic.replicas = 1
[35mkafka_1               |[0m 	confluent.metrics.reporter.topic.retention.bytes = -1
[34mschema-registry_1     |[0m [2017-10-25 05:25:31,405] WARN DEPRECATION warning: `listeners` configuration is not configured. Falling back to the deprecated `port` configuration. (io.confluent.rest.Application)
[35mkafka_1               |[0m 	confluent.metrics.reporter.topic.retention.ms = 259200000
[35mkafka_1               |[0m 	confluent.metrics.reporter.topic.roll.ms = 14400000
[35mkafka_1               |[0m 	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
[35mkafka_1               |[0m 	confluent.metrics.reporter.whitelist = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|OfflinePartitionsCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TotalFetchRequestsPerSec|TotalProduceRequestsPerSec|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec).*
[35mkafka_1               |[0m  (io.confluent.metrics.reporter.ConfluentMetricsReporterConfig)
[35mkafka_1               |[0m [2017-10-25 05:25:28,980] INFO ProducerConfig values: 
[35mkafka_1               |[0m 	acks = all
[35mkafka_1               |[0m 	batch.size = 16384
[35mkafka_1               |[0m 	bootstrap.servers = [kafka:29092]
[35mkafka_1               |[0m 	buffer.memory = 33554432
[35mkafka_1               |[0m 	client.id = confluent-metrics-reporter
[35mkafka_1               |[0m 	compression.type = lz4
[35mkafka_1               |[0m 	connections.max.idle.ms = 540000
[35mkafka_1               |[0m 	enable.idempotence = false
[35mkafka_1               |[0m 	interceptor.classes = []
[35mkafka_1               |[0m 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[35mkafka_1               |[0m 	linger.ms = 500
[35mkafka_1               |[0m 	max.block.ms = 60000
[35mkafka_1               |[0m 	max.in.flight.requests.per.connection = 1
[35mkafka_1               |[0m 	max.request.size = 10485760
[35mkafka_1               |[0m 	metadata.max.age.ms = 300000
[35mkafka_1               |[0m 	metric.reporters = []
[35mkafka_1               |[0m 	metrics.num.samples = 2
[35mkafka_1               |[0m 	metrics.recording.level = INFO
[35mkafka_1               |[0m 	metrics.sample.window.ms = 30000
[35mkafka_1               |[0m 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
[35mkafka_1               |[0m 	receive.buffer.bytes = 32768
[35mkafka_1               |[0m 	reconnect.backoff.max.ms = 1000
[35mkafka_1               |[0m 	reconnect.backoff.ms = 50
[35mkafka_1               |[0m 	request.timeout.ms = 30000
[35mkafka_1               |[0m 	retries = 10
[35mkafka_1               |[0m 	retry.backoff.ms = 500
[35mkafka_1               |[0m 	sasl.jaas.config = null
[35mkafka_1               |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[35mkafka_1               |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[35mkafka_1               |[0m 	sasl.kerberos.service.name = null
[35mkafka_1               |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[35mkafka_1               |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[35mkafka_1               |[0m 	sasl.mechanism = GSSAPI
[35mkafka_1               |[0m 	security.protocol = PLAINTEXT
[35mkafka_1               |[0m 	send.buffer.bytes = 131072
[35mkafka_1               |[0m 	ssl.cipher.suites = null
[35mkafka_1               |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[35mkafka_1               |[0m 	ssl.endpoint.identification.algorithm = null
[35mkafka_1               |[0m 	ssl.key.password = null
[35mkafka_1               |[0m 	ssl.keymanager.algorithm = SunX509
[35mkafka_1               |[0m 	ssl.keystore.location = null
[35mkafka_1               |[0m 	ssl.keystore.password = null
[35mkafka_1               |[0m 	ssl.keystore.type = JKS
[35mkafka_1               |[0m 	ssl.protocol = TLS
[35mkafka_1               |[0m 	ssl.provider = null
[35mkafka_1               |[0m 	ssl.secure.random.implementation = null
[35mkafka_1               |[0m 	ssl.trustmanager.algorithm = PKIX
[35mkafka_1               |[0m 	ssl.truststore.location = null
[35mkafka_1               |[0m 	ssl.truststore.password = null
[35mkafka_1               |[0m 	ssl.truststore.type = JKS
[35mkafka_1               |[0m 	transaction.timeout.ms = 60000
[35mkafka_1               |[0m 	transactional.id = null
[35mkafka_1               |[0m 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[35mkafka_1               |[0m  (org.apache.kafka.clients.producer.ProducerConfig)
[35mkafka_1               |[0m [2017-10-25 05:25:29,030] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[35mkafka_1               |[0m [2017-10-25 05:25:29,030] WARN The configuration 'topic.replicas' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[35mkafka_1               |[0m [2017-10-25 05:25:29,033] INFO Kafka version : 0.11.0.0-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[35mkafka_1               |[0m [2017-10-25 05:25:29,034] INFO Kafka commitId : 6a8cf706ddc9ab6a (org.apache.kafka.common.utils.AppInfoParser)
[35mkafka_1               |[0m [2017-10-25 05:25:29,072] INFO [ThrottledRequestReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[35mkafka_1               |[0m [2017-10-25 05:25:29,074] INFO [ThrottledRequestReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[35mkafka_1               |[0m [2017-10-25 05:25:29,074] INFO [ThrottledRequestReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[35mkafka_1               |[0m [2017-10-25 05:25:29,085] INFO Starting Confluent metrics reporter for cluster id C5MgIcbyQGG-Zb34UBMPtA with an interval of 15000 ms (io.confluent.metrics.reporter.ConfluentMetricsReporter)
[35mkafka_1               |[0m [2017-10-25 05:25:29,176] INFO Loading logs. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:29,191] INFO Logs loading complete in 15 ms. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:29,251] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:29,254] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:29,262] INFO Starting the log cleaner (kafka.log.LogCleaner)
[35mkafka_1               |[0m [2017-10-25 05:25:29,270] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner)
[35mkafka_1               |[0m [2017-10-25 05:25:29,311] INFO Awaiting socket connections on 0.0.0.0:29092. (kafka.network.Acceptor)
[35mkafka_1               |[0m [2017-10-25 05:25:29,338] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[35mkafka_1               |[0m [2017-10-25 05:25:29,345] INFO [Socket Server on Broker 1], Started 2 acceptor threads (kafka.network.SocketServer)
[35mkafka_1               |[0m [2017-10-25 05:25:29,363] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[35mkafka_1               |[0m [2017-10-25 05:25:29,371] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[35mkafka_1               |[0m [2017-10-25 05:25:29,379] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[35mkafka_1               |[0m [2017-10-25 05:25:29,450] INFO [controller-event-thread]: Starting (kafka.controller.ControllerEventManager$ControllerEventThread)
[35mkafka_1               |[0m [2017-10-25 05:25:29,454] INFO [ExpirationReaper-1-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[35mkafka_1               |[0m [2017-10-25 05:25:29,465] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[35mkafka_1               |[0m [2017-10-25 05:25:29,475] INFO Creating /controller (is it secure? false) (kafka.utils.ZKCheckedEphemeral)
[35mkafka_1               |[0m [2017-10-25 05:25:29,477] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[35mkafka_1               |[0m [2017-10-25 05:25:29,486] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[35mkafka_1               |[0m [2017-10-25 05:25:29,487] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[35mkafka_1               |[0m [2017-10-25 05:25:29,491] INFO [Group Metadata Manager on Broker 1]: Removed 0 expired offsets in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:29,516] INFO Result of znode creation is: OK (kafka.utils.ZKCheckedEphemeral)
[35mkafka_1               |[0m [2017-10-25 05:25:29,518] INFO [Controller 1]: 1 successfully elected as the controller (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:29,521] INFO [Controller 1]: Broker 1 starting become controller state transition (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:29,720] INFO [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1 (kafka.coordinator.transaction.ProducerIdManager)
[35mkafka_1               |[0m [2017-10-25 05:25:29,745] INFO [Controller 1]: Controller 1 incremented epoch to 1 (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:29,748] DEBUG [Controller 1]: Registering IsrChangeNotificationListener (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:29,771] INFO [Transaction Coordinator 1]: Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[35mkafka_1               |[0m [2017-10-25 05:25:29,779] INFO [Transaction Coordinator 1]: Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[35mkafka_1               |[0m [2017-10-25 05:25:29,781] INFO [Controller 1]: Partitions being reassigned: Map() (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:29,782] INFO [Controller 1]: Partitions already reassigned: Set() (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:29,784] INFO [Controller 1]: Resuming reassignment of partitions: Map() (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:29,784] INFO [Transaction Marker Channel Manager 1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[35mkafka_1               |[0m [2017-10-25 05:25:29,786] INFO [Controller 1]: Currently active brokers in the cluster: Set() (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:29,787] INFO [Controller 1]: Currently shutting brokers in the cluster: Set() (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:29,788] INFO [Controller 1]: Current list of topics in the cluster: Set() (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:29,794] INFO [Controller 1]: List of topics to be deleted:  (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:29,796] INFO [Controller 1]: List of topics ineligible for deletion:  (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:29,945] INFO Will not load MX4J, mx4j-tools.jar is not in the classpath (kafka.utils.Mx4jLoader$)
[35mkafka_1               |[0m [2017-10-25 05:25:29,950] INFO [Replica state machine on controller 1]: Started replica state machine with initial state -> Map() (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:29,960] INFO [Partition state machine on Controller 1]: Started partition state machine with initial state -> Map() (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:29,969] INFO [Controller 1]: Broker 1 is ready to serve as the new controller with epoch 1 (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:29,981] INFO [Controller 1]: Partitions undergoing preferred replica election:  (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:29,981] INFO [Controller 1]: Partitions that completed preferred replica election:  (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:29,983] INFO [Controller 1]: Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:29,984] INFO [Controller 1]: Resuming preferred replica election for partitions:  (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:29,984] INFO [Controller 1]: Starting preferred replica leader election for partitions  (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:29,987] INFO [Partition state machine on Controller 1]: Invoking state change to OnlinePartition for partitions  (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:30,039] INFO [Controller 1]: starting the controller scheduler (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:30,089] INFO Creating /brokers/ids/1 (is it secure? false) (kafka.utils.ZKCheckedEphemeral)
[35mkafka_1               |[0m [2017-10-25 05:25:30,172] INFO Result of znode creation is: OK (kafka.utils.ZKCheckedEphemeral)
[35mkafka_1               |[0m [2017-10-25 05:25:30,181] INFO Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(kafka,29092,ListenerName(PLAINTEXT),PLAINTEXT),EndPoint(localhost,9092,ListenerName(PLAINTEXT_HOST),PLAINTEXT) (kafka.utils.ZkUtils)
[35mkafka_1               |[0m [2017-10-25 05:25:30,196] WARN No meta.properties file under dir /var/lib/kafka/data/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[35mkafka_1               |[0m [2017-10-25 05:25:30,250] INFO [Controller 1]: Newly added brokers: 1, deleted brokers: , all live brokers: 1 (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:30,252] DEBUG [Channel manager on controller 1]: Controller 1 trying to connect to broker 1 (kafka.controller.ControllerChannelManager)
[35mkafka_1               |[0m [2017-10-25 05:25:30,260] INFO [Controller 1]: New broker startup callback for 1 (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:30,265] INFO [Controller-1-to-broker-1-send-thread]: Starting (kafka.controller.RequestSendThread)
[35mkafka_1               |[0m [2017-10-25 05:25:30,268] INFO [Controller-1-to-broker-1-send-thread]: Controller 1 connected to kafka:29092 (id: 1 rack: null) for sending state change requests (kafka.controller.RequestSendThread)
[35mkafka_1               |[0m [2017-10-25 05:25:30,294] INFO Kafka version : 0.11.0.0-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[35mkafka_1               |[0m [2017-10-25 05:25:30,294] INFO Kafka commitId : 6a8cf706ddc9ab6a (org.apache.kafka.common.utils.AppInfoParser)
[35mkafka_1               |[0m [2017-10-25 05:25:30,295] INFO [Kafka Server 1], started (kafka.server.KafkaServer)
[35mkafka_1               |[0m [2017-10-25 05:25:30,346] TRACE Controller 1 epoch 1 received response {error_code=0} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[34mschema-registry_1     |[0m [2017-10-25 05:25:31,516] WARN DEPRECATION warning: `listeners` configuration is not configured. Falling back to the deprecated `port` configuration. (io.confluent.rest.Application)
[33;1mconnect_1             |[0m 
[33;1mconnect_1             |[0m echo "===> Launching ... "
[33;1mconnect_1             |[0m + echo '===> Launching ... '
[33;1mconnect_1             |[0m exec /etc/confluent/docker/launch
[33;1mconnect_1             |[0m + exec /etc/confluent/docker/launch
[33;1mconnect_1             |[0m ===> Launching ... 
[33;1mconnect_1             |[0m ===> Launching kafka-connect ... 
[34mschema-registry_1     |[0m [2017-10-25 05:25:31,717] INFO Starting ZkClient event thread. (org.I0Itec.zkclient.ZkEventThread)
[34mschema-registry_1     |[0m [2017-10-25 05:25:31,725] INFO Client environment:zookeeper.version=3.4.8--1, built on 02/06/2016 03:18 GMT (org.apache.zookeeper.ZooKeeper)
[34mschema-registry_1     |[0m [2017-10-25 05:25:31,725] INFO Client environment:host.name=schema-registry (org.apache.zookeeper.ZooKeeper)
[34mschema-registry_1     |[0m [2017-10-25 05:25:31,725] INFO Client environment:java.version=1.8.0_102 (org.apache.zookeeper.ZooKeeper)
[34mschema-registry_1     |[0m [2017-10-25 05:25:31,725] INFO Client environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.ZooKeeper)
[34mschema-registry_1     |[0m [2017-10-25 05:25:31,725] INFO Client environment:java.home=/usr/lib/jvm/zulu-8-amd64/jre (org.apache.zookeeper.ZooKeeper)
[34mschema-registry_1     |[0m [2017-10-25 05:25:31,726] INFO Client environment:java.class.path=:/usr/bin/../package-schema-registry/target/kafka-schema-registry-package-*-development/share/java/schema-registry/*:/usr/bin/../share/java/confluent-common/slf4j-api-1.7.21.jar:/usr/bin/../share/java/confluent-common/common-utils-3.3.0.jar:/usr/bin/../share/java/confluent-common/netty-3.7.0.Final.jar:/usr/bin/../share/java/confluent-common/jline-0.9.94.jar:/usr/bin/../share/java/confluent-common/zkclient-0.10.jar:/usr/bin/../share/java/confluent-common/log4j-1.2.17.jar:/usr/bin/../share/java/confluent-common/common-metrics-3.3.0.jar:/usr/bin/../share/java/confluent-common/common-config-3.3.0.jar:/usr/bin/../share/java/confluent-common/zookeeper-3.4.8.jar:/usr/bin/../share/java/rest-utils/jetty-util-9.2.19.v20160908.jar:/usr/bin/../share/java/rest-utils/osgi-resource-locator-1.0.1.jar:/usr/bin/../share/java/rest-utils/jackson-jaxrs-base-2.8.4.jar:/usr/bin/../share/java/rest-utils/jetty-io-9.2.19.v20160908.jar:/usr/bin/../share/java/rest-utils/jetty-continuation-9.2.19.v20160908.jar:/usr/bin/../share/java/rest-utils/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/rest-utils/jersey-common-2.25.jar:/usr/bin/../share/java/rest-utils/jackson-core-2.8.4.jar:/usr/bin/../share/java/rest-utils/jersey-client-2.25.jar:/usr/bin/../share/java/rest-utils/javax.ws.rs-api-2.0.1.jar:/usr/bin/../share/java/rest-utils/hk2-utils-2.5.0-b30.jar:/usr/bin/../share/java/rest-utils/aopalliance-repackaged-2.5.0-b30.jar:/usr/bin/../share/java/rest-utils/jetty-servlets-9.2.19.v20160908.jar:/usr/bin/../share/java/rest-utils/jackson-jaxrs-json-provider-2.8.4.jar:/usr/bin/../share/java/rest-utils/classmate-1.0.0.jar:/usr/bin/../share/java/rest-utils/hk2-api-2.5.0-b30.jar:/usr/bin/../share/java/rest-utils/javax.el-2.2.4.jar:/usr/bin/../share/java/rest-utils/javax.inject-2.5.0-b30.jar:/usr/bin/../share/java/rest-utils/jersey-server-2.25.jar:/usr/bin/../share/java/rest-utils/jackson-annotations-2.8.4.jar:/usr/bin/../share/java/rest-utils/jersey-bean-validation-2.25.jar:/usr/bin/../share/java/rest-utils/jetty-servlet-9.2.19.v20160908.jar:/usr/bin/../share/java/rest-utils/jboss-logging-3.1.3.GA.jar:/usr/bin/../share/java/rest-utils/javassist-3.20.0-GA.jar:/usr/bin/../share/java/rest-utils/jackson-module-jaxb-annotations-2.8.4.jar:/usr/bin/../share/java/rest-utils/jetty-jaas-9.2.19.v20160908.jar:/usr/bin/../share/java/rest-utils/jetty-security-9.2.19.v20160908.jar:/usr/bin/../share/java/rest-utils/hk2-locator-2.5.0-b30.jar:/usr/bin/../share/java/rest-utils/hibernate-validator-5.1.3.Final.jar:/usr/bin/../share/java/rest-utils/javax.annotation-api-1.2.jar:/usr/bin/../share/java/rest-utils/jersey-guava-2.25.jar:/usr/bin/../share/java/rest-utils/rest-utils-3.3.0.jar:/usr/bin/../share/java/rest-utils/jackson-databind-2.8.4.jar:/usr/bin/../share/java/rest-utils/validation-api-1.1.0.Final.jar:/usr/bin/../share/java/rest-utils/jersey-container-servlet-2.25.jar:/usr/bin/../share/java/rest-utils/jetty-jmx-9.2.19.v20160908.jar:/usr/bin/../share/java/rest-utils/jetty-http-9.2.19.v20160908.jar:/usr/bin/../share/java/rest-utils/jersey-container-servlet-core-2.25.jar:/usr/bin/../share/java/rest-utils/jersey-media-jaxb-2.25.jar:/usr/bin/../share/java/rest-utils/javax.el-api-2.2.4.jar:/usr/bin/../share/java/rest-utils/jetty-server-9.2.19.v20160908.jar:/usr/bin/../share/java/schema-registry/xz-1.5.jar:/usr/bin/../share/java/schema-registry/slf4j-api-1.7.21.jar:/usr/bin/../share/java/schema-registry/kafka-clients-0.11.0.0-cp1.jar:/usr/bin/../share/java/schema-registry/osgi-resource-locator-1.0.1.jar:/usr/bin/../share/java/schema-registry/metrics-core-2.2.0.jar:/usr/bin/../share/java/schema-registry/jopt-simple-5.0.3.jar:/usr/bin/../share/java/schema-registry/kafka-schema-registry-client-3.3.0.jar:/usr/bin/../share/java/schema-registry/lz4-1.3.0.jar:/usr/bin/../share/java/schema-registry/jersey-common-2.25.jar:/usr/bin/../share/java/schema-registry/scala-library-2.11.11.jar:/usr/bin/../share/java/schema-registry/jackson-core-2.8.4.jar:/usr/bin/../share/java/schema-registry/jersey-client-2.25.jar:/usr/bin/../share/java/schema-registry/javax.ws.rs-api-2.0.1.jar:/usr/bin/../share/java/schema-registry/hk2-utils-2.5.0-b30.jar:/usr/bin/../share/java/schema-registry/aopalliance-repackaged-2.5.0-b30.jar:/usr/bin/../share/java/schema-registry/classmate-1.0.0.jar:/usr/bin/../share/java/schema-registry/avro-1.8.2.jar:/usr/bin/../share/java/schema-registry/hk2-api-2.5.0-b30.jar:/usr/bin/../share/java/schema-registry/javax.el-2.2.4.jar:/usr/bin/../share/java/schema-registry/javax.inject-2.5.0-b30.jar:/usr/bin/../share/java/schema-registry/jersey-server-2.25.jar:/usr/bin/../share/java/schema-registry/slf4j-log4j12-1.7.21.jar:/usr/bin/../share/java/schema-registry/jackson-annotations-2.8.4.jar:/usr/bin/../share/java/schema-registry/jersey-bean-validation-2.25.jar:/usr/bin/../share/java/schema-registry/jackson-core-asl-1.9.13.jar:/usr/bin/../share/java/schema-registry/jboss-logging-3.1.3.GA.jar:/usr/bin/../share/java/schema-registry/javassist-3.20.0-GA.jar:/usr/bin/../share/java/schema-registry/jackson-mapper-asl-1.9.13.jar:/usr/bin/../share/java/schema-registry/hk2-locator-2.5.0-b30.jar:/usr/bin/../share/java/schema-registry/hibernate-validator-5.1.3.Final.jar:/usr/bin/../share/java/schema-registry/kafka-schema-registry-3.3.0.jar:/usr/bin/../share/java/schema-registry/javax.annotation-api-1.2.jar:/usr/bin/../share/java/schema-registry/commons-compress-1.8.1.jar:/usr/bin/../share/java/schema-registry/scala-parser-combinators_2.11-1.0.4.jar:/usr/bin/../share/java/schema-registry/log4j-1.2.17.jar:/usr/bin/../share/java/schema-registry/paranamer-2.7.jar:/usr/bin/../share/java/schema-registry/jersey-guava-2.25.jar:/usr/bin/../share/java/schema-registry/jackson-databind-2.8.4.jar:/usr/bin/../share/java/schema-registry/validation-api-1.1.0.Final.jar:/usr/bin/../share/java/schema-registry/jersey-media-jaxb-2.25.jar:/usr/bin/../share/java/schema-registry/javax.el-api-2.2.4.jar:/usr/bin/../share/java/schema-registry/snappy-java-1.1.1.3.jar:/usr/bin/../share/java/schema-registry/kafka_2.11-0.11.0.0-cp1.jar (org.apache.zookeeper.ZooKeeper)
[34mschema-registry_1     |[0m [2017-10-25 05:25:31,726] INFO Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
[34mschema-registry_1     |[0m [2017-10-25 05:25:31,726] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
[34mschema-registry_1     |[0m [2017-10-25 05:25:31,726] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[34mschema-registry_1     |[0m [2017-10-25 05:25:31,726] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
[34mschema-registry_1     |[0m [2017-10-25 05:25:31,726] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[34mschema-registry_1     |[0m [2017-10-25 05:25:31,726] INFO Client environment:os.version=4.9.49-moby (org.apache.zookeeper.ZooKeeper)
[34mschema-registry_1     |[0m [2017-10-25 05:25:31,727] INFO Client environment:user.name=root (org.apache.zookeeper.ZooKeeper)
[34mschema-registry_1     |[0m [2017-10-25 05:25:31,727] INFO Client environment:user.home=/root (org.apache.zookeeper.ZooKeeper)
[34mschema-registry_1     |[0m [2017-10-25 05:25:31,727] INFO Client environment:user.dir=/ (org.apache.zookeeper.ZooKeeper)
[34mschema-registry_1     |[0m [2017-10-25 05:25:31,728] INFO Initiating client connection, connectString=zookeeper:32181 sessionTimeout=30000 watcher=org.I0Itec.zkclient.ZkClient@20d525 (org.apache.zookeeper.ZooKeeper)
[34mschema-registry_1     |[0m [2017-10-25 05:25:31,747] INFO Waiting for keeper state SyncConnected (org.I0Itec.zkclient.ZkClient)
[34mschema-registry_1     |[0m [2017-10-25 05:25:31,753] INFO Opening socket connection to server quickstart_zookeeper_1.quickstart_default/172.21.0.4:32181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[33mzookeeper_1           |[0m [2017-10-25 05:25:31,837] INFO Accepted socket connection from /172.21.0.7:58344 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[34mschema-registry_1     |[0m [2017-10-25 05:25:31,840] INFO Socket connection established to quickstart_zookeeper_1.quickstart_default/172.21.0.4:32181, initiating session (org.apache.zookeeper.ClientCnxn)
[33mzookeeper_1           |[0m [2017-10-25 05:25:31,845] INFO Client attempting to establish new session at /172.21.0.7:58344 (org.apache.zookeeper.server.ZooKeeperServer)
[33mzookeeper_1           |[0m [2017-10-25 05:25:31,875] INFO Established session 0x15f51fe0e370005 with negotiated timeout 30000 for client /172.21.0.7:58344 (org.apache.zookeeper.server.ZooKeeperServer)
[34mschema-registry_1     |[0m [2017-10-25 05:25:31,878] INFO Session establishment complete on server quickstart_zookeeper_1.quickstart_default/172.21.0.4:32181, sessionid = 0x15f51fe0e370005, negotiated timeout = 30000 (org.apache.zookeeper.ClientCnxn)
[34mschema-registry_1     |[0m [2017-10-25 05:25:31,882] INFO zookeeper state changed (SyncConnected) (org.I0Itec.zkclient.ZkClient)
[34mschema-registry_1     |[0m [2017-10-25 05:25:32,319] INFO Initializing KafkaStore with broker endpoints: PLAINTEXT://kafka:29092,PLAINTEXT://localhost:9092 (io.confluent.kafka.schemaregistry.storage.KafkaStore)
[34mschema-registry_1     |[0m [2017-10-25 05:25:32,354] WARN Creating the schema topic _schemas using a replication factor of 1, which is less than the desired one of 3. If this is a production environment, it's crucial to add more brokers and increase the replication factor of the topic. (io.confluent.kafka.schemaregistry.storage.KafkaStore)
[33;1mconnect_1             |[0m SLF4J: Class path contains multiple SLF4J bindings.
[33;1mconnect_1             |[0m SLF4J: Found binding in [jar:file:/usr/share/java/kafka/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
[33;1mconnect_1             |[0m SLF4J: Found binding in [jar:file:/usr/share/java/kafka-connect-hdfs/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
[33;1mconnect_1             |[0m SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
[33;1mconnect_1             |[0m SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
[33mzookeeper_1           |[0m [2017-10-25 05:25:32,582] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370005 type:setData cxid:0x8 zxid:0x26 txntype:-1 reqpath:n/a Error Path:/config/topics/_schemas Error:KeeperErrorCode = NoNode for /config/topics/_schemas (org.apache.zookeeper.server.PrepRequestProcessor)
[33;1mconnect_1             |[0m [2017-10-25 05:25:32,593] INFO Loading plugin from: /connect-plugins/json-schema-transform-1.0.0-SNAPSHOT.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33mzookeeper_1           |[0m [2017-10-25 05:25:32,616] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370005 type:create cxid:0xa zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[34mschema-registry_1     |[0m [2017-10-25 05:25:32,682] INFO Topic creation {"version":1,"partitions":{"0":[1]}} (kafka.admin.AdminUtils$)
[35mkafka_1               |[0m [2017-10-25 05:25:32,723] INFO [Controller 1]: New topics: [Set(_schemas)], deleted topics: [Set()], new partition replica assignment [Map([_schemas,0] -> List(1))] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:32,724] INFO [Controller 1]: New topic creation callback for [_schemas,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:32,728] INFO [Controller 1]: New partition creation callback for [_schemas,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:32,729] INFO [Partition state machine on Controller 1]: Invoking state change to NewPartition for partitions [_schemas,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:32,732] TRACE Controller 1 epoch 1 changed partition [_schemas,0] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:32,736] INFO [Replica state machine on controller 1]: Invoking state change to NewReplica for replicas [Topic=_schemas,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:32,752] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_schemas,0] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:32,753] INFO [Partition state machine on Controller 1]: Invoking state change to OnlinePartition for partitions [_schemas,0] (kafka.controller.PartitionStateMachine)
[34mschema-registry_1     |[0m [2017-10-25 05:25:32,752] INFO ProducerConfig values: 
[34mschema-registry_1     |[0m 	acks = -1
[34mschema-registry_1     |[0m 	batch.size = 16384
[34mschema-registry_1     |[0m 	bootstrap.servers = [PLAINTEXT://kafka:29092, PLAINTEXT://localhost:9092]
[34mschema-registry_1     |[0m 	buffer.memory = 33554432
[34mschema-registry_1     |[0m 	client.id = 
[34mschema-registry_1     |[0m 	compression.type = none
[34mschema-registry_1     |[0m 	connections.max.idle.ms = 540000
[34mschema-registry_1     |[0m 	enable.idempotence = false
[34mschema-registry_1     |[0m 	interceptor.classes = null
[34mschema-registry_1     |[0m 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[34mschema-registry_1     |[0m 	linger.ms = 0
[34mschema-registry_1     |[0m 	max.block.ms = 60000
[34mschema-registry_1     |[0m 	max.in.flight.requests.per.connection = 5
[34mschema-registry_1     |[0m 	max.request.size = 1048576
[34mschema-registry_1     |[0m 	metadata.max.age.ms = 300000
[34mschema-registry_1     |[0m 	metric.reporters = []
[34mschema-registry_1     |[0m 	metrics.num.samples = 2
[34mschema-registry_1     |[0m 	metrics.recording.level = INFO
[34mschema-registry_1     |[0m 	metrics.sample.window.ms = 30000
[34mschema-registry_1     |[0m 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
[34mschema-registry_1     |[0m 	receive.buffer.bytes = 32768
[34mschema-registry_1     |[0m 	reconnect.backoff.max.ms = 1000
[34mschema-registry_1     |[0m 	reconnect.backoff.ms = 50
[34mschema-registry_1     |[0m 	request.timeout.ms = 30000
[34mschema-registry_1     |[0m 	retries = 0
[34mschema-registry_1     |[0m 	retry.backoff.ms = 100
[34mschema-registry_1     |[0m 	sasl.jaas.config = null
[34mschema-registry_1     |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[34mschema-registry_1     |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[34mschema-registry_1     |[0m 	sasl.kerberos.service.name = null
[34mschema-registry_1     |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[34mschema-registry_1     |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[34mschema-registry_1     |[0m 	sasl.mechanism = GSSAPI
[34mschema-registry_1     |[0m 	security.protocol = PLAINTEXT
[34mschema-registry_1     |[0m 	send.buffer.bytes = 131072
[34mschema-registry_1     |[0m 	ssl.cipher.suites = null
[34mschema-registry_1     |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[34mschema-registry_1     |[0m 	ssl.endpoint.identification.algorithm = null
[34mschema-registry_1     |[0m 	ssl.key.password = null
[34mschema-registry_1     |[0m 	ssl.keymanager.algorithm = SunX509
[34mschema-registry_1     |[0m 	ssl.keystore.location = null
[34mschema-registry_1     |[0m 	ssl.keystore.password = null
[34mschema-registry_1     |[0m 	ssl.keystore.type = JKS
[34mschema-registry_1     |[0m 	ssl.protocol = TLS
[34mschema-registry_1     |[0m 	ssl.provider = null
[34mschema-registry_1     |[0m 	ssl.secure.random.implementation = null
[34mschema-registry_1     |[0m 	ssl.trustmanager.algorithm = PKIX
[34mschema-registry_1     |[0m 	ssl.truststore.location = null
[34mschema-registry_1     |[0m 	ssl.truststore.password = null
[34mschema-registry_1     |[0m 	ssl.truststore.type = JKS
[34mschema-registry_1     |[0m 	transaction.timeout.ms = 60000
[34mschema-registry_1     |[0m 	transactional.id = null
[34mschema-registry_1     |[0m 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[34mschema-registry_1     |[0m  (org.apache.kafka.clients.producer.ProducerConfig)
[35mkafka_1               |[0m [2017-10-25 05:25:32,755] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [_schemas,0] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:32,760] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [_schemas,0] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:32,761] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x52 zxid:0x2a txntype:-1 reqpath:n/a Error Path:/brokers/topics/_schemas/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/_schemas/partitions/0 (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:25:32,786] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x53 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/_schemas/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/_schemas/partitions (org.apache.zookeeper.server.PrepRequestProcessor)
[34mschema-registry_1     |[0m [2017-10-25 05:25:32,861] INFO Kafka version : 0.11.0.0-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[34mschema-registry_1     |[0m [2017-10-25 05:25:32,861] INFO Kafka commitId : 6a8cf706ddc9ab6a (org.apache.kafka.common.utils.AppInfoParser)
[34mschema-registry_1     |[0m [2017-10-25 05:25:32,878] INFO ConsumerConfig values: 
[34mschema-registry_1     |[0m 	auto.commit.interval.ms = 5000
[34mschema-registry_1     |[0m 	auto.offset.reset = earliest
[34mschema-registry_1     |[0m 	bootstrap.servers = [PLAINTEXT://kafka:29092, PLAINTEXT://localhost:9092]
[34mschema-registry_1     |[0m 	check.crcs = true
[34mschema-registry_1     |[0m 	client.id = KafkaStore-reader-_schemas
[34mschema-registry_1     |[0m 	connections.max.idle.ms = 540000
[34mschema-registry_1     |[0m 	enable.auto.commit = false
[34mschema-registry_1     |[0m 	exclude.internal.topics = true
[34mschema-registry_1     |[0m 	fetch.max.bytes = 52428800
[34mschema-registry_1     |[0m 	fetch.max.wait.ms = 500
[34mschema-registry_1     |[0m 	fetch.min.bytes = 1
[34mschema-registry_1     |[0m 	group.id = schema-registry-schema-registry-8081
[34mschema-registry_1     |[0m 	heartbeat.interval.ms = 3000
[34mschema-registry_1     |[0m 	interceptor.classes = null
[34mschema-registry_1     |[0m 	internal.leave.group.on.close = true
[34mschema-registry_1     |[0m 	isolation.level = read_uncommitted
[34mschema-registry_1     |[0m 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[34mschema-registry_1     |[0m 	max.partition.fetch.bytes = 1048576
[34mschema-registry_1     |[0m 	max.poll.interval.ms = 300000
[34mschema-registry_1     |[0m 	max.poll.records = 500
[34mschema-registry_1     |[0m 	metadata.max.age.ms = 300000
[34mschema-registry_1     |[0m 	metric.reporters = []
[34mschema-registry_1     |[0m 	metrics.num.samples = 2
[34mschema-registry_1     |[0m 	metrics.recording.level = INFO
[34mschema-registry_1     |[0m 	metrics.sample.window.ms = 30000
[34mschema-registry_1     |[0m 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
[34mschema-registry_1     |[0m 	receive.buffer.bytes = 65536
[34mschema-registry_1     |[0m 	reconnect.backoff.max.ms = 1000
[34mschema-registry_1     |[0m 	reconnect.backoff.ms = 50
[34mschema-registry_1     |[0m 	request.timeout.ms = 305000
[34mschema-registry_1     |[0m 	retry.backoff.ms = 100
[34mschema-registry_1     |[0m 	sasl.jaas.config = null
[34mschema-registry_1     |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[34mschema-registry_1     |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[34mschema-registry_1     |[0m 	sasl.kerberos.service.name = null
[34mschema-registry_1     |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[34mschema-registry_1     |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[34mschema-registry_1     |[0m 	sasl.mechanism = GSSAPI
[34mschema-registry_1     |[0m 	security.protocol = PLAINTEXT
[34mschema-registry_1     |[0m 	send.buffer.bytes = 131072
[34mschema-registry_1     |[0m 	session.timeout.ms = 10000
[34mschema-registry_1     |[0m 	ssl.cipher.suites = null
[34mschema-registry_1     |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[34mschema-registry_1     |[0m 	ssl.endpoint.identification.algorithm = null
[34mschema-registry_1     |[0m 	ssl.key.password = null
[34mschema-registry_1     |[0m 	ssl.keymanager.algorithm = SunX509
[34mschema-registry_1     |[0m 	ssl.keystore.location = null
[34mschema-registry_1     |[0m 	ssl.keystore.password = null
[34mschema-registry_1     |[0m 	ssl.keystore.type = JKS
[34mschema-registry_1     |[0m 	ssl.protocol = TLS
[34mschema-registry_1     |[0m 	ssl.provider = null
[34mschema-registry_1     |[0m 	ssl.secure.random.implementation = null
[34mschema-registry_1     |[0m 	ssl.trustmanager.algorithm = PKIX
[34mschema-registry_1     |[0m 	ssl.truststore.location = null
[34mschema-registry_1     |[0m 	ssl.truststore.password = null
[34mschema-registry_1     |[0m 	ssl.truststore.type = JKS
[34mschema-registry_1     |[0m 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[34mschema-registry_1     |[0m  (org.apache.kafka.clients.consumer.ConsumerConfig)
[35mkafka_1               |[0m [2017-10-25 05:25:32,915] TRACE Controller 1 epoch 1 changed partition [_schemas,0] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:32,919] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [_schemas,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:32,928] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition _schemas-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:32,929] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 1 from controller 1 epoch 1 for partition [_schemas,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:32,931] INFO [Replica state machine on controller 1]: Invoking state change to OnlineReplica for replicas [Topic=_schemas,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:32,932] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_schemas,0] from NewReplica to OnlineReplica (state.change.logger)
[34mschema-registry_1     |[0m [2017-10-25 05:25:32,960] INFO Kafka version : 0.11.0.0-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[34mschema-registry_1     |[0m [2017-10-25 05:25:32,960] INFO Kafka commitId : 6a8cf706ddc9ab6a (org.apache.kafka.common.utils.AppInfoParser)
[35mkafka_1               |[0m [2017-10-25 05:25:32,966] TRACE Broker 1 handling LeaderAndIsr request correlationId 1 from controller 1 epoch 1 starting the become-leader transition for partition _schemas-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:32,970] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions _schemas-0 (kafka.server.ReplicaFetcherManager)
[34mschema-registry_1     |[0m [2017-10-25 05:25:32,977] WARN Connection to node -2 could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
[33;1mconnect_1             |[0m [2017-10-25 05:25:33,079] INFO Registered loader: PluginClassLoader{pluginLocation=file:/connect-plugins/json-schema-transform-1.0.0-SNAPSHOT.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:33,079] INFO Added plugin 'io.confluent.examples.sling.json.connect.DummySourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:33,080] INFO Added plugin 'io.confluent.examples.sling.json.connect.JsonSchemaTransformation$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:33,080] INFO Added plugin 'io.confluent.examples.sling.json.connect.JsonSchemaTransformation$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:33,080] INFO Loading plugin from: /connect-plugins/kafka-connect-transform-nullfilter (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[35mkafka_1               |[0m [2017-10-25 05:25:33,089] INFO Loading producer state from offset 0 for partition _schemas-0 with message format version 2 (kafka.log.Log)
[33;1mconnect_1             |[0m [2017-10-25 05:25:33,098] INFO Registered loader: PluginClassLoader{pluginLocation=file:/connect-plugins/kafka-connect-transform-nullfilter/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:33,098] INFO Loading plugin from: /connect-plugins/mysql-connector-java-5.1.44-bin.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[34mschema-registry_1     |[0m [2017-10-25 05:25:33,103] WARN Connection to node -2 could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
[35mkafka_1               |[0m [2017-10-25 05:25:33,118] INFO Completed load of log _schemas-0 with 1 log segments, log start offset 0 and log end offset 0 in 102 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:33,128] INFO Created log for partition [_schemas,0] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:33,129] INFO Partition [_schemas,0] on broker 1: No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:33,135] INFO Partition [_schemas,0] on broker 1: _schemas-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:33,153] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 1 for partition _schemas-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:33,157] TRACE Broker 1 completed LeaderAndIsr request correlationId 1 from controller 1 epoch 1 for the become-leader transition for partition _schemas-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:33,163] TRACE Controller 1 epoch 1 received response {error_code=0,partitions=[{topic=_schemas,partition=0,error_code=0}]} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:33,166] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition _schemas-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:33,168] TRACE Controller 1 epoch 1 received response {error_code=0} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[34mschema-registry_1     |[0m [2017-10-25 05:25:33,206] WARN Connection to node -2 could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
[34mschema-registry_1     |[0m [2017-10-25 05:25:33,334] WARN Connection to node -2 could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
[33;1mconnect_1             |[0m [2017-10-25 05:25:33,358] INFO Registered loader: PluginClassLoader{pluginLocation=file:/connect-plugins/mysql-connector-java-5.1.44-bin.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[34mschema-registry_1     |[0m [2017-10-25 05:25:33,397] WARN Connection to node -2 could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
[34mschema-registry_1     |[0m [2017-10-25 05:25:33,421] INFO Initialized last consumed offset to -1 (io.confluent.kafka.schemaregistry.storage.KafkaStoreReaderThread)
[34mschema-registry_1     |[0m [2017-10-25 05:25:33,429] INFO [kafka-store-reader-thread-_schemas]: Starting (io.confluent.kafka.schemaregistry.storage.KafkaStoreReaderThread)
[34mschema-registry_1     |[0m [2017-10-25 05:25:33,630] WARN Connection to node -2 could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
[35mkafka_1               |[0m [2017-10-25 05:25:33,738] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: _schemas-0. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[34mschema-registry_1     |[0m [2017-10-25 05:25:33,887] INFO Wait to catch up until the offset of the last message at 0 (io.confluent.kafka.schemaregistry.storage.KafkaStore)
[34mschema-registry_1     |[0m [2017-10-25 05:25:33,988] INFO Initiating client connection, connectString=zookeeper:32181 sessionTimeout=30000 watcher=org.I0Itec.zkclient.ZkClient@25e2ab5a (org.apache.zookeeper.ZooKeeper)
[34mschema-registry_1     |[0m [2017-10-25 05:25:33,992] INFO Starting ZkClient event thread. (org.I0Itec.zkclient.ZkEventThread)
[34mschema-registry_1     |[0m [2017-10-25 05:25:33,993] INFO Opening socket connection to server quickstart_zookeeper_1.quickstart_default/172.21.0.4:32181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[34mschema-registry_1     |[0m [2017-10-25 05:25:33,994] INFO Socket connection established to quickstart_zookeeper_1.quickstart_default/172.21.0.4:32181, initiating session (org.apache.zookeeper.ClientCnxn)
[33mzookeeper_1           |[0m [2017-10-25 05:25:33,994] INFO Accepted socket connection from /172.21.0.7:58370 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[33mzookeeper_1           |[0m [2017-10-25 05:25:33,995] INFO Client attempting to establish new session at /172.21.0.7:58370 (org.apache.zookeeper.server.ZooKeeperServer)
[34mschema-registry_1     |[0m [2017-10-25 05:25:33,995] INFO Waiting for keeper state SyncConnected (org.I0Itec.zkclient.ZkClient)
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:33,995][INFO ][o.e.n.Node               ] [] initializing ...
[33mzookeeper_1           |[0m [2017-10-25 05:25:34,036] INFO Established session 0x15f51fe0e370006 with negotiated timeout 30000 for client /172.21.0.7:58370 (org.apache.zookeeper.server.ZooKeeperServer)
[34mschema-registry_1     |[0m [2017-10-25 05:25:34,037] INFO Session establishment complete on server quickstart_zookeeper_1.quickstart_default/172.21.0.4:32181, sessionid = 0x15f51fe0e370006, negotiated timeout = 30000 (org.apache.zookeeper.ClientCnxn)
[34mschema-registry_1     |[0m [2017-10-25 05:25:34,037] INFO zookeeper state changed (SyncConnected) (org.I0Itec.zkclient.ZkClient)
[34mschema-registry_1     |[0m [2017-10-25 05:25:34,072] INFO Created schema registry namespace zookeeper:32181/schema_registry (io.confluent.kafka.schemaregistry.storage.KafkaSchemaRegistry)
[34mschema-registry_1     |[0m [2017-10-25 05:25:34,072] INFO Terminate ZkClient event thread. (org.I0Itec.zkclient.ZkEventThread)
[33mzookeeper_1           |[0m [2017-10-25 05:25:34,073] INFO Processed session termination for sessionid: 0x15f51fe0e370006 (org.apache.zookeeper.server.PrepRequestProcessor)
[34mschema-registry_1     |[0m [2017-10-25 05:25:34,102] INFO Session: 0x15f51fe0e370006 closed (org.apache.zookeeper.ZooKeeper)
[34mschema-registry_1     |[0m [2017-10-25 05:25:34,102] INFO EventThread shut down for session: 0x15f51fe0e370006 (org.apache.zookeeper.ClientCnxn)
[33mzookeeper_1           |[0m [2017-10-25 05:25:34,103] INFO Closed socket connection for client /172.21.0.7:58370 which had sessionid 0x15f51fe0e370006 (org.apache.zookeeper.server.NIOServerCnxn)
[34mschema-registry_1     |[0m [2017-10-25 05:25:34,103] INFO Initiating client connection, connectString=zookeeper:32181/schema_registry sessionTimeout=30000 watcher=org.I0Itec.zkclient.ZkClient@35e5d0e5 (org.apache.zookeeper.ZooKeeper)
[34mschema-registry_1     |[0m [2017-10-25 05:25:34,104] INFO Starting ZkClient event thread. (org.I0Itec.zkclient.ZkEventThread)
[34mschema-registry_1     |[0m [2017-10-25 05:25:34,106] INFO Opening socket connection to server quickstart_zookeeper_1.quickstart_default/172.21.0.4:32181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[33mzookeeper_1           |[0m [2017-10-25 05:25:34,107] INFO Accepted socket connection from /172.21.0.7:58372 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[34mschema-registry_1     |[0m [2017-10-25 05:25:34,107] INFO Socket connection established to quickstart_zookeeper_1.quickstart_default/172.21.0.4:32181, initiating session (org.apache.zookeeper.ClientCnxn)
[33mzookeeper_1           |[0m [2017-10-25 05:25:34,108] INFO Client attempting to establish new session at /172.21.0.7:58372 (org.apache.zookeeper.server.ZooKeeperServer)
[34mschema-registry_1     |[0m [2017-10-25 05:25:34,109] INFO Waiting for keeper state SyncConnected (org.I0Itec.zkclient.ZkClient)
[33mzookeeper_1           |[0m [2017-10-25 05:25:34,150] INFO Established session 0x15f51fe0e370007 with negotiated timeout 30000 for client /172.21.0.7:58372 (org.apache.zookeeper.server.ZooKeeperServer)
[34mschema-registry_1     |[0m [2017-10-25 05:25:34,150] INFO Session establishment complete on server quickstart_zookeeper_1.quickstart_default/172.21.0.4:32181, sessionid = 0x15f51fe0e370007, negotiated timeout = 30000 (org.apache.zookeeper.ClientCnxn)
[34mschema-registry_1     |[0m [2017-10-25 05:25:34,151] INFO zookeeper state changed (SyncConnected) (org.I0Itec.zkclient.ZkClient)
[34mschema-registry_1     |[0m [2017-10-25 05:25:34,197] INFO Successfully elected the new master: {"host":"schema-registry","port":8081,"master_eligibility":true,"version":1} (io.confluent.kafka.schemaregistry.zookeeper.ZookeeperMasterElector)
[33mzookeeper_1           |[0m [2017-10-25 05:25:34,201] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370007 type:create cxid:0x6 zxid:0x34 txntype:-1 reqpath:n/a Error Path:/schema_registry/schema_registry_master Error:KeeperErrorCode = NodeExists for /schema_registry/schema_registry_master (org.apache.zookeeper.server.PrepRequestProcessor)
[34mschema-registry_1     |[0m [2017-10-25 05:25:34,237] INFO /schema_registry_master exists with value {"host":"schema-registry","port":8081,"master_eligibility":true,"version":1} during connection loss; this is ok (kafka.utils.ZkUtils)
[34mschema-registry_1     |[0m [2017-10-25 05:25:34,237] INFO Successfully elected the new master: {"host":"schema-registry","port":8081,"master_eligibility":true,"version":1} (io.confluent.kafka.schemaregistry.zookeeper.ZookeeperMasterElector)
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:34,316][INFO ][o.e.e.NodeEnvironment    ] [-Ryb53P] using [1] data paths, mounts [[/ (overlay)]], net usable_space [43.5gb], net total_space [62.7gb], spins? [possibly], types [overlay]
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:34,322][INFO ][o.e.e.NodeEnvironment    ] [-Ryb53P] heap size [1.9gb], compressed ordinary object pointers [true]
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:34,323][INFO ][o.e.n.Node               ] node name [-Ryb53P] derived from node ID [-Ryb53PFTVe2Q8kpydTm0w]; set [node.name] to override
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:34,324][INFO ][o.e.n.Node               ] version[5.6.0], pid[1], build[781a835/2017-09-07T03:09:58.087Z], OS[Linux/4.9.49-moby/amd64], JVM[Oracle Corporation/OpenJDK 64-Bit Server VM/1.8.0_141/25.141-b16]
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:34,324][INFO ][o.e.n.Node               ] JVM arguments [-Xms2g, -Xmx2g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -Djdk.io.permissionsUseCanonicalPath=true, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j.skipJansi=true, -XX:+HeapDumpOnOutOfMemoryError, -Des.cgroups.hierarchy.override=/, -Des.path.home=/usr/share/elasticsearch]
[34mschema-registry_1     |[0m [2017-10-25 05:25:34,333] INFO Logging initialized @3787ms (org.eclipse.jetty.util.log)
[34mschema-registry_1     |[0m [2017-10-25 05:25:34,442] WARN DEPRECATION warning: `listeners` configuration is not configured. Falling back to the deprecated `port` configuration. (io.confluent.rest.Application)
[34mschema-registry_1     |[0m [2017-10-25 05:25:34,442] INFO Adding listener: http://0.0.0.0:8081 (io.confluent.rest.Application)
[34mschema-registry_1     |[0m [2017-10-25 05:25:34,668] INFO jetty-9.2.19.v20160908 (org.eclipse.jetty.server.Server)
[35mkafka_1               |[0m [2017-10-25 05:25:35,043] TRACE [Controller 1]: Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:35,046] DEBUG [Controller 1]: Preferred replicas by broker Map(1 -> Map([_schemas,0] -> List(1))) (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:35,051] DEBUG [Controller 1]: Topics not in preferred replica Map() (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:35,053] TRACE [Controller 1]: Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[34mschema-registry_1     |[0m [2017-10-25 05:25:36,163] INFO HV000001: Hibernate Validator 5.1.3.Final (org.hibernate.validator.internal.util.Version)
[34mschema-registry_1     |[0m [2017-10-25 05:25:36,456] INFO Started o.e.j.s.ServletContextHandler@878452d{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
[34mschema-registry_1     |[0m [2017-10-25 05:25:36,483] INFO Started NetworkTrafficServerConnector@37b70343{HTTP/1.1}{0.0.0.0:8081} (org.eclipse.jetty.server.NetworkTrafficServerConnector)
[34mschema-registry_1     |[0m [2017-10-25 05:25:36,486] INFO Started @5941ms (org.eclipse.jetty.server.Server)
[34mschema-registry_1     |[0m [2017-10-25 05:25:36,486] INFO Server started, listening for requests... (io.confluent.kafka.schemaregistry.rest.SchemaRegistryMain)
[34mschema-registry_1     |[0m [2017-10-25 05:25:36,963] INFO 172.21.0.8 - - [25/Oct/2017:05:25:36 +0000] "GET /config HTTP/1.1" 200 33  312 (io.confluent.rest-utils.requests)
[36;1mksql-datagen-users_1  |[0m Waiting a few seconds for topic creation to finish...
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:37,338][INFO ][o.e.p.PluginsService     ] [-Ryb53P] loaded module [aggs-matrix-stats]
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:37,341][INFO ][o.e.p.PluginsService     ] [-Ryb53P] loaded module [ingest-common]
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:37,342][INFO ][o.e.p.PluginsService     ] [-Ryb53P] loaded module [lang-expression]
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:37,343][INFO ][o.e.p.PluginsService     ] [-Ryb53P] loaded module [lang-groovy]
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:37,344][INFO ][o.e.p.PluginsService     ] [-Ryb53P] loaded module [lang-mustache]
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:37,345][INFO ][o.e.p.PluginsService     ] [-Ryb53P] loaded module [lang-painless]
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:37,345][INFO ][o.e.p.PluginsService     ] [-Ryb53P] loaded module [parent-join]
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:37,346][INFO ][o.e.p.PluginsService     ] [-Ryb53P] loaded module [percolator]
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:37,347][INFO ][o.e.p.PluginsService     ] [-Ryb53P] loaded module [reindex]
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:37,348][INFO ][o.e.p.PluginsService     ] [-Ryb53P] loaded module [transport-netty3]
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:37,348][INFO ][o.e.p.PluginsService     ] [-Ryb53P] loaded module [transport-netty4]
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:37,351][INFO ][o.e.p.PluginsService     ] [-Ryb53P] loaded plugin [ingest-geoip]
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:37,351][INFO ][o.e.p.PluginsService     ] [-Ryb53P] loaded plugin [ingest-user-agent]
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:37,352][INFO ][o.e.p.PluginsService     ] [-Ryb53P] loaded plugin [x-pack]
[36mpostgres_1            |[0m syncing data to disk ... ok
[36mpostgres_1            |[0m 
[36mpostgres_1            |[0m WARNING: enabling "trust" authentication for local connections
[36mpostgres_1            |[0m You can change this by editing pg_hba.conf or using the option -A, or
[36mpostgres_1            |[0m --auth-local and --auth-host, the next time you run initdb.
[36mpostgres_1            |[0m 
[36mpostgres_1            |[0m Success. You can now start the database server using:
[36mpostgres_1            |[0m 
[36mpostgres_1            |[0m     pg_ctl -D /var/lib/postgresql/data -l logfile start
[36mpostgres_1            |[0m 
[36mpostgres_1            |[0m waiting for server to start....2017-10-25 05:25:37.492 UTC [38] LOG:  listening on IPv4 address "127.0.0.1", port 5432
[36mpostgres_1            |[0m 2017-10-25 05:25:37.492 UTC [38] LOG:  could not bind IPv6 address "::1": Cannot assign requested address
[36mpostgres_1            |[0m 2017-10-25 05:25:37.492 UTC [38] HINT:  Is another postmaster already running on port 5432? If not, wait a few seconds and retry.
[36mpostgres_1            |[0m 2017-10-25 05:25:37.539 UTC [38] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
[36mpostgres_1            |[0m 2017-10-25 05:25:37.646 UTC [39] LOG:  database system was shut down at 2017-10-25 05:25:24 UTC
[36mpostgres_1            |[0m 2017-10-25 05:25:37.691 UTC [38] LOG:  database system is ready to accept connections
[36mpostgres_1            |[0m  done
[36mpostgres_1            |[0m server started
[36mpostgres_1            |[0m CREATE DATABASE
[36mpostgres_1            |[0m 
[36mpostgres_1            |[0m CREATE ROLE
[36mpostgres_1            |[0m 
[36mpostgres_1            |[0m 
[36mpostgres_1            |[0m /usr/local/bin/docker-entrypoint.sh: ignoring /docker-entrypoint-initdb.d/*
[36mpostgres_1            |[0m 
[36mpostgres_1            |[0m waiting for server to shut down...2017-10-25 05:25:40.155 UTC [38] LOG:  received fast shutdown request
[36mpostgres_1            |[0m .2017-10-25 05:25:40.193 UTC [38] LOG:  aborting any active transactions
[36mpostgres_1            |[0m 2017-10-25 05:25:40.197 UTC [38] LOG:  worker process: logical replication launcher (PID 45) exited with exit code 1
[36mpostgres_1            |[0m 2017-10-25 05:25:40.202 UTC [40] LOG:  shutting down
[32mmysql-db_1            |[0m 2017-10-25T05:25:40.256748Z 1 [Warning] 'user' entry 'root@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:40.256784Z 1 [Warning] 'user' entry 'mysql.session@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:40.256792Z 1 [Warning] 'user' entry 'mysql.sys@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:40.256806Z 1 [Warning] 'db' entry 'performance_schema mysql.session@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:40.256815Z 1 [Warning] 'db' entry 'sys mysql.sys@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:40.256835Z 1 [Warning] 'proxies_priv' entry '@ root@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:40.256875Z 1 [Warning] 'tables_priv' entry 'user mysql.session@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:40.256893Z 1 [Warning] 'tables_priv' entry 'sys_config mysql.sys@localhost' ignored in --skip-name-resolve mode.
[36;1mksql-datagen-users_1  |[0m [2017-10-25 05:25:40,340] INFO AvroDataConfig values: 
[36;1mksql-datagen-users_1  |[0m 	schemas.cache.config = 1
[36;1mksql-datagen-users_1  |[0m 	enhanced.avro.schema.support = false
[36;1mksql-datagen-users_1  |[0m 	connect.meta.data = true
[36;1mksql-datagen-users_1  |[0m  (io.confluent.connect.avro.AvroDataConfig:170)
[36mpostgres_1            |[0m 2017-10-25 05:25:40.548 UTC [38] LOG:  database system is shut down
[36mpostgres_1            |[0m  done
[36mpostgres_1            |[0m server stopped
[36mpostgres_1            |[0m 
[36mpostgres_1            |[0m PostgreSQL init process complete; ready for start up.
[36mpostgres_1            |[0m 
[36mpostgres_1            |[0m 2017-10-25 05:25:40.619 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
[36mpostgres_1            |[0m 2017-10-25 05:25:40.619 UTC [1] LOG:  listening on IPv6 address "::", port 5432
[36mpostgres_1            |[0m 2017-10-25 05:25:40.683 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
[36mpostgres_1            |[0m 2017-10-25 05:25:40.785 UTC [65] LOG:  database system was shut down at 2017-10-25 05:25:40 UTC
[36mpostgres_1            |[0m 2017-10-25 05:25:40.834 UTC [1] LOG:  database system is ready to accept connections
[36;1mksql-datagen-users_1  |[0m [2017-10-25 05:25:41,032] INFO ProducerConfig values: 
[36;1mksql-datagen-users_1  |[0m 	acks = 1
[36;1mksql-datagen-users_1  |[0m 	batch.size = 16384
[36;1mksql-datagen-users_1  |[0m 	bootstrap.servers = [kafka:29092]
[36;1mksql-datagen-users_1  |[0m 	buffer.memory = 33554432
[36;1mksql-datagen-users_1  |[0m 	client.id = KSQLDataGenProducer
[36;1mksql-datagen-users_1  |[0m 	compression.type = none
[36;1mksql-datagen-users_1  |[0m 	connections.max.idle.ms = 540000
[36;1mksql-datagen-users_1  |[0m 	enable.idempotence = false
[36;1mksql-datagen-users_1  |[0m 	interceptor.classes = null
[36;1mksql-datagen-users_1  |[0m 	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
[36;1mksql-datagen-users_1  |[0m 	linger.ms = 0
[36;1mksql-datagen-users_1  |[0m 	max.block.ms = 60000
[36;1mksql-datagen-users_1  |[0m 	max.in.flight.requests.per.connection = 5
[36;1mksql-datagen-users_1  |[0m 	max.request.size = 1048576
[36;1mksql-datagen-users_1  |[0m 	metadata.max.age.ms = 300000
[36;1mksql-datagen-users_1  |[0m 	metric.reporters = []
[36;1mksql-datagen-users_1  |[0m 	metrics.num.samples = 2
[36;1mksql-datagen-users_1  |[0m 	metrics.recording.level = INFO
[36;1mksql-datagen-users_1  |[0m 	metrics.sample.window.ms = 30000
[36;1mksql-datagen-users_1  |[0m 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
[36;1mksql-datagen-users_1  |[0m 	receive.buffer.bytes = 32768
[36;1mksql-datagen-users_1  |[0m 	reconnect.backoff.max.ms = 1000
[36;1mksql-datagen-users_1  |[0m 	reconnect.backoff.ms = 50
[36;1mksql-datagen-users_1  |[0m 	request.timeout.ms = 30000
[36;1mksql-datagen-users_1  |[0m 	retries = 0
[36;1mksql-datagen-users_1  |[0m 	retry.backoff.ms = 100
[36;1mksql-datagen-users_1  |[0m 	sasl.jaas.config = null
[36;1mksql-datagen-users_1  |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36;1mksql-datagen-users_1  |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36;1mksql-datagen-users_1  |[0m 	sasl.kerberos.service.name = null
[36;1mksql-datagen-users_1  |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36;1mksql-datagen-users_1  |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36;1mksql-datagen-users_1  |[0m 	sasl.mechanism = GSSAPI
[36;1mksql-datagen-users_1  |[0m 	security.protocol = PLAINTEXT
[36;1mksql-datagen-users_1  |[0m 	send.buffer.bytes = 131072
[36;1mksql-datagen-users_1  |[0m 	ssl.cipher.suites = null
[36;1mksql-datagen-users_1  |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[36;1mksql-datagen-users_1  |[0m 	ssl.endpoint.identification.algorithm = null
[36;1mksql-datagen-users_1  |[0m 	ssl.key.password = null
[36;1mksql-datagen-users_1  |[0m 	ssl.keymanager.algorithm = SunX509
[36;1mksql-datagen-users_1  |[0m 	ssl.keystore.location = null
[36;1mksql-datagen-users_1  |[0m 	ssl.keystore.password = null
[36;1mksql-datagen-users_1  |[0m 	ssl.keystore.type = JKS
[36;1mksql-datagen-users_1  |[0m 	ssl.protocol = TLS
[36;1mksql-datagen-users_1  |[0m 	ssl.provider = null
[36;1mksql-datagen-users_1  |[0m 	ssl.secure.random.implementation = null
[36;1mksql-datagen-users_1  |[0m 	ssl.trustmanager.algorithm = PKIX
[36;1mksql-datagen-users_1  |[0m 	ssl.truststore.location = null
[36;1mksql-datagen-users_1  |[0m 	ssl.truststore.password = null
[36;1mksql-datagen-users_1  |[0m 	ssl.truststore.type = JKS
[36;1mksql-datagen-users_1  |[0m 	transaction.timeout.ms = 60000
[36;1mksql-datagen-users_1  |[0m 	transactional.id = null
[36;1mksql-datagen-users_1  |[0m 	value.serializer = class io.confluent.ksql.serde.json.KsqlJsonSerializer
[36;1mksql-datagen-users_1  |[0m  (org.apache.kafka.clients.producer.ProducerConfig:223)
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:41,355][INFO ][o.e.x.m.j.p.l.CppLogMessageHandler] [controller/50] [Main.cc@128] controller (64 bit): Version 5.6.0 (Build 93aea61f57f7d8) Copyright (c) 2017 Elasticsearch BV
[36;1mksql-datagen-users_1  |[0m [2017-10-25 05:25:41,432] INFO Kafka version : 0.11.0.0-cp1 (org.apache.kafka.common.utils.AppInfoParser:83)
[36;1mksql-datagen-users_1  |[0m [2017-10-25 05:25:41,432] INFO Kafka commitId : 9b97a464829a1c77 (org.apache.kafka.common.utils.AppInfoParser:84)
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:41,464][INFO ][o.e.d.DiscoveryModule    ] [-Ryb53P] using discovery type [zen]
[33mzookeeper_1           |[0m [2017-10-25 05:25:41,924] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:setData cxid:0x5b zxid:0x36 txntype:-1 reqpath:n/a Error Path:/config/topics/ratings Error:KeeperErrorCode = NoNode for /config/topics/ratings (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:25:41,946] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x5c zxid:0x37 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:41,988] INFO Topic creation {"version":1,"partitions":{"0":[1]}} (kafka.admin.AdminUtils$)
[35mkafka_1               |[0m [2017-10-25 05:25:42,036] INFO [KafkaApi-1] Auto creation of topic ratings with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[36;1mksql-datagen-users_1  |[0m [2017-10-25 05:25:42,048] WARN Error while fetching metadata with correlation id 1 : {ratings=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:846)
[35mkafka_1               |[0m [2017-10-25 05:25:42,050] INFO [Controller 1]: New topics: [Set(ratings)], deleted topics: [Set()], new partition replica assignment [Map([ratings,0] -> List(1))] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:42,051] INFO [Controller 1]: New topic creation callback for [ratings,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:42,053] INFO [Controller 1]: New partition creation callback for [ratings,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:42,053] INFO [Partition state machine on Controller 1]: Invoking state change to NewPartition for partitions [ratings,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:42,054] TRACE Controller 1 epoch 1 changed partition [ratings,0] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:42,054] INFO [Replica state machine on controller 1]: Invoking state change to NewReplica for replicas [Topic=ratings,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:42,057] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [ratings,0] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:42,057] INFO [Partition state machine on Controller 1]: Invoking state change to OnlinePartition for partitions [ratings,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:42,058] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [ratings,0] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:42,058] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [ratings,0] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:42,059] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x64 zxid:0x3a txntype:-1 reqpath:n/a Error Path:/brokers/topics/ratings/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/ratings/partitions/0 (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:25:42,081] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x65 zxid:0x3b txntype:-1 reqpath:n/a Error Path:/brokers/topics/ratings/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/ratings/partitions (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:42,178] TRACE Controller 1 epoch 1 changed partition [ratings,0] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:42,180] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [ratings,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:42,183] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition ratings-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:42,183] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 3 from controller 1 epoch 1 for partition [ratings,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:42,184] INFO [Replica state machine on controller 1]: Invoking state change to OnlineReplica for replicas [Topic=ratings,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:42,185] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [ratings,0] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:42,186] TRACE Broker 1 handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition ratings-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:42,187] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions ratings-0 (kafka.server.ReplicaFetcherManager)
[36;1mksql-datagen-users_1  |[0m [2017-10-25 05:25:42,232] WARN Error while fetching metadata with correlation id 3 : {ratings=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:846)
[35mkafka_1               |[0m [2017-10-25 05:25:42,230] INFO Loading producer state from offset 0 for partition ratings-0 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:42,234] INFO Completed load of log ratings-0 with 1 log segments, log start offset 0 and log end offset 0 in 21 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:42,243] INFO Created log for partition [ratings,0] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:42,245] INFO Partition [ratings,0] on broker 1: No checkpointed highwatermark is found for partition ratings-0 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:42,248] INFO Partition [ratings,0] on broker 1: ratings-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:42,249] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 3 for partition ratings-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:42,249] TRACE Broker 1 completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition ratings-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:42,253] TRACE Controller 1 epoch 1 received response {error_code=0,partitions=[{topic=ratings,partition=0,error_code=0}]} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:42,261] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition ratings-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:42,269] TRACE Controller 1 epoch 1 received response {error_code=0} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[36;1mksql-datagen-users_1  |[0m 1 --> ([ 1 | 8 | 2 | 2740 | 1508909141462 | 'web' | 'more peanuts please' ])
[35mkafka_1               |[0m [2017-10-25 05:25:42,536] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: ratings-0. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[36;1mksql-datagen-users_1  |[0m 2 --> ([ 2 | 17 | 2 | 6836 | 1508909142571 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 3 --> ([ 3 | 3 | 3 | 6006 | 1508909142606 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 4 --> ([ 4 | 12 | 3 | 8096 | 1508909142897 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 5 --> ([ 5 | 10 | 4 | 21 | 1508909142938 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 6 --> ([ 6 | 19 | 1 | 7423 | 1508909143157 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 7 --> ([ 7 | 1 | 2 | 5444 | 1508909143286 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:43,381][INFO ][o.e.n.Node               ] initialized
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:43,381][INFO ][o.e.n.Node               ] [-Ryb53P] starting ...
[36;1mksql-datagen-users_1  |[0m 8 --> ([ 8 | 4 | 3 | 434 | 1508909143503 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 9 --> ([ 9 | -1 | 2 | 5815 | 1508909143717 | 'web' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 10 --> ([ 10 | 15 | 1 | 2700 | 1508909143750 | 'ios' | 'more peanuts please' ])
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:43,785][INFO ][o.e.t.TransportService   ] [-Ryb53P] publish_address {172.21.0.11:9300}, bound_addresses {0.0.0.0:9300}
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:43,822][INFO ][o.e.b.BootstrapChecks    ] [-Ryb53P] bound or publishing to a non-loopback or non-link-local address, enforcing bootstrap checks
[36;1mksql-datagen-users_1  |[0m 11 --> ([ 11 | 12 | 2 | 7247 | 1508909143839 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 12 --> ([ 12 | 7 | 3 | 2389 | 1508909144076 | 'iOS' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 13 --> ([ 13 | 14 | 1 | 995 | 1508909144084 | 'web' | 'more peanuts please' ])
[35mkafka_1               |[0m [2017-10-25 05:25:44,119] INFO AdminClientConfig values: 
[35mkafka_1               |[0m 	bootstrap.servers = [kafka:29092]
[35mkafka_1               |[0m 	client.id = 
[35mkafka_1               |[0m 	connections.max.idle.ms = 300000
[35mkafka_1               |[0m 	metadata.max.age.ms = 300000
[35mkafka_1               |[0m 	metric.reporters = []
[35mkafka_1               |[0m 	metrics.num.samples = 2
[35mkafka_1               |[0m 	metrics.recording.level = INFO
[35mkafka_1               |[0m 	metrics.sample.window.ms = 30000
[35mkafka_1               |[0m 	receive.buffer.bytes = 65536
[35mkafka_1               |[0m 	reconnect.backoff.max.ms = 1000
[35mkafka_1               |[0m 	reconnect.backoff.ms = 50
[35mkafka_1               |[0m 	request.timeout.ms = 120000
[35mkafka_1               |[0m 	retries = 5
[35mkafka_1               |[0m 	retry.backoff.ms = 100
[35mkafka_1               |[0m 	sasl.jaas.config = null
[35mkafka_1               |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[35mkafka_1               |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[35mkafka_1               |[0m 	sasl.kerberos.service.name = null
[35mkafka_1               |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[35mkafka_1               |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[35mkafka_1               |[0m 	sasl.mechanism = GSSAPI
[35mkafka_1               |[0m 	security.protocol = PLAINTEXT
[35mkafka_1               |[0m 	send.buffer.bytes = 131072
[35mkafka_1               |[0m 	ssl.cipher.suites = null
[35mkafka_1               |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[35mkafka_1               |[0m 	ssl.endpoint.identification.algorithm = null
[35mkafka_1               |[0m 	ssl.key.password = null
[35mkafka_1               |[0m 	ssl.keymanager.algorithm = SunX509
[35mkafka_1               |[0m 	ssl.keystore.location = null
[35mkafka_1               |[0m 	ssl.keystore.password = null
[35mkafka_1               |[0m 	ssl.keystore.type = JKS
[35mkafka_1               |[0m 	ssl.protocol = TLS
[35mkafka_1               |[0m 	ssl.provider = null
[35mkafka_1               |[0m 	ssl.secure.random.implementation = null
[35mkafka_1               |[0m 	ssl.trustmanager.algorithm = PKIX
[35mkafka_1               |[0m 	ssl.truststore.location = null
[35mkafka_1               |[0m 	ssl.truststore.password = null
[35mkafka_1               |[0m 	ssl.truststore.type = JKS
[35mkafka_1               |[0m  (org.apache.kafka.clients.admin.AdminClientConfig)
[35mkafka_1               |[0m [2017-10-25 05:25:44,129] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[35mkafka_1               |[0m [2017-10-25 05:25:44,129] WARN The configuration 'topic.replicas' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36;1mksql-datagen-users_1  |[0m 14 --> ([ 14 | 1 | 4 | 1310 | 1508909144278 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[33mzookeeper_1           |[0m [2017-10-25 05:25:44,288] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:setData cxid:0x6f zxid:0x3f txntype:-1 reqpath:n/a Error Path:/config/topics/_confluent-metrics Error:KeeperErrorCode = NoNode for /config/topics/_confluent-metrics (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:25:44,298] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x70 zxid:0x40 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:44,324] INFO Topic creation {"version":1,"partitions":{"8":[1],"4":[1],"11":[1],"9":[1],"5":[1],"10":[1],"6":[1],"1":[1],"0":[1],"2":[1],"7":[1],"3":[1]}} (kafka.admin.AdminUtils$)
[35mkafka_1               |[0m [2017-10-25 05:25:44,355] INFO [Controller 1]: New topics: [Set(_confluent-metrics)], deleted topics: [Set()], new partition replica assignment [Map([_confluent-metrics,11] -> List(1), [_confluent-metrics,5] -> List(1), [_confluent-metrics,8] -> List(1), [_confluent-metrics,3] -> List(1), [_confluent-metrics,9] -> List(1), [_confluent-metrics,2] -> List(1), [_confluent-metrics,7] -> List(1), [_confluent-metrics,0] -> List(1), [_confluent-metrics,6] -> List(1), [_confluent-metrics,1] -> List(1), [_confluent-metrics,4] -> List(1), [_confluent-metrics,10] -> List(1))] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:44,360] INFO [Controller 1]: New topic creation callback for [_confluent-metrics,11],[_confluent-metrics,5],[_confluent-metrics,8],[_confluent-metrics,3],[_confluent-metrics,9],[_confluent-metrics,2],[_confluent-metrics,7],[_confluent-metrics,0],[_confluent-metrics,6],[_confluent-metrics,1],[_confluent-metrics,4],[_confluent-metrics,10] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:44,363] INFO [Controller 1]: New partition creation callback for [_confluent-metrics,11],[_confluent-metrics,5],[_confluent-metrics,8],[_confluent-metrics,3],[_confluent-metrics,9],[_confluent-metrics,2],[_confluent-metrics,7],[_confluent-metrics,0],[_confluent-metrics,6],[_confluent-metrics,1],[_confluent-metrics,4],[_confluent-metrics,10] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:44,364] INFO [Partition state machine on Controller 1]: Invoking state change to NewPartition for partitions [_confluent-metrics,11],[_confluent-metrics,5],[_confluent-metrics,8],[_confluent-metrics,3],[_confluent-metrics,9],[_confluent-metrics,2],[_confluent-metrics,7],[_confluent-metrics,0],[_confluent-metrics,6],[_confluent-metrics,1],[_confluent-metrics,4],[_confluent-metrics,10] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:44,364] TRACE Controller 1 epoch 1 changed partition [_confluent-metrics,11] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,368] TRACE Controller 1 epoch 1 changed partition [_confluent-metrics,5] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,369] TRACE Controller 1 epoch 1 changed partition [_confluent-metrics,8] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,369] TRACE Controller 1 epoch 1 changed partition [_confluent-metrics,3] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,369] TRACE Controller 1 epoch 1 changed partition [_confluent-metrics,9] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,369] TRACE Controller 1 epoch 1 changed partition [_confluent-metrics,2] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,370] TRACE Controller 1 epoch 1 changed partition [_confluent-metrics,7] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,370] TRACE Controller 1 epoch 1 changed partition [_confluent-metrics,0] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,370] TRACE Controller 1 epoch 1 changed partition [_confluent-metrics,6] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,370] TRACE Controller 1 epoch 1 changed partition [_confluent-metrics,1] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,371] TRACE Controller 1 epoch 1 changed partition [_confluent-metrics,4] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,371] TRACE Controller 1 epoch 1 changed partition [_confluent-metrics,10] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,372] INFO [Replica state machine on controller 1]: Invoking state change to NewReplica for replicas [Topic=_confluent-metrics,Partition=10,Replica=1],[Topic=_confluent-metrics,Partition=0,Replica=1],[Topic=_confluent-metrics,Partition=7,Replica=1],[Topic=_confluent-metrics,Partition=9,Replica=1],[Topic=_confluent-metrics,Partition=3,Replica=1],[Topic=_confluent-metrics,Partition=1,Replica=1],[Topic=_confluent-metrics,Partition=8,Replica=1],[Topic=_confluent-metrics,Partition=11,Replica=1],[Topic=_confluent-metrics,Partition=5,Replica=1],[Topic=_confluent-metrics,Partition=6,Replica=1],[Topic=_confluent-metrics,Partition=2,Replica=1],[Topic=_confluent-metrics,Partition=4,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:44,373] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_confluent-metrics,10] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,375] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_confluent-metrics,0] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,376] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_confluent-metrics,7] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,377] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_confluent-metrics,9] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,379] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_confluent-metrics,3] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,380] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_confluent-metrics,1] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,383] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_confluent-metrics,8] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,389] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_confluent-metrics,11] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,390] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_confluent-metrics,5] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,396] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_confluent-metrics,6] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,398] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_confluent-metrics,2] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,399] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_confluent-metrics,4] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,400] INFO [Partition state machine on Controller 1]: Invoking state change to OnlinePartition for partitions [_confluent-metrics,11],[_confluent-metrics,5],[_confluent-metrics,8],[_confluent-metrics,3],[_confluent-metrics,9],[_confluent-metrics,2],[_confluent-metrics,7],[_confluent-metrics,0],[_confluent-metrics,6],[_confluent-metrics,1],[_confluent-metrics,4],[_confluent-metrics,10] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:44,400] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [_confluent-metrics,11] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:44,401] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [_confluent-metrics,11] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:44,402] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x83 zxid:0x43 txntype:-1 reqpath:n/a Error Path:/brokers/topics/_confluent-metrics/partitions/11 Error:KeeperErrorCode = NoNode for /brokers/topics/_confluent-metrics/partitions/11 (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:25:44,421] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x84 zxid:0x44 txntype:-1 reqpath:n/a Error Path:/brokers/topics/_confluent-metrics/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/_confluent-metrics/partitions (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 15 --> ([ 15 | 2 | 3 | 3119 | 1508909144431 | 'iOS-test' | 'more peanuts please' ])
[35mkafka_1               |[0m [2017-10-25 05:25:44,513] TRACE Controller 1 epoch 1 changed partition [_confluent-metrics,11] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,514] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [_confluent-metrics,5] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:44,514] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [_confluent-metrics,5] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:44,515] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x88 zxid:0x48 txntype:-1 reqpath:n/a Error Path:/brokers/topics/_confluent-metrics/partitions/5 Error:KeeperErrorCode = NoNode for /brokers/topics/_confluent-metrics/partitions/5 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:44,560] TRACE Controller 1 epoch 1 changed partition [_confluent-metrics,5] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,561] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [_confluent-metrics,8] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:44,561] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [_confluent-metrics,8] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:44,563] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x8b zxid:0x4b txntype:-1 reqpath:n/a Error Path:/brokers/topics/_confluent-metrics/partitions/8 Error:KeeperErrorCode = NoNode for /brokers/topics/_confluent-metrics/partitions/8 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:44,602] TRACE Controller 1 epoch 1 changed partition [_confluent-metrics,8] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,602] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [_confluent-metrics,3] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:44,603] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [_confluent-metrics,3] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:44,604] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x8e zxid:0x4e txntype:-1 reqpath:n/a Error Path:/brokers/topics/_confluent-metrics/partitions/3 Error:KeeperErrorCode = NoNode for /brokers/topics/_confluent-metrics/partitions/3 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:44,649] TRACE Controller 1 epoch 1 changed partition [_confluent-metrics,3] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,649] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [_confluent-metrics,9] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:44,650] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [_confluent-metrics,9] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:44,652] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x91 zxid:0x51 txntype:-1 reqpath:n/a Error Path:/brokers/topics/_confluent-metrics/partitions/9 Error:KeeperErrorCode = NoNode for /brokers/topics/_confluent-metrics/partitions/9 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:44,729] TRACE Controller 1 epoch 1 changed partition [_confluent-metrics,9] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,730] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [_confluent-metrics,2] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:44,730] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [_confluent-metrics,2] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:44,733] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x94 zxid:0x54 txntype:-1 reqpath:n/a Error Path:/brokers/topics/_confluent-metrics/partitions/2 Error:KeeperErrorCode = NoNode for /brokers/topics/_confluent-metrics/partitions/2 (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 16 --> ([ 16 | 10 | 3 | 4896 | 1508909144737 | 'iOS-test' | 'is this as good as it gets? really ?' ])
[35mkafka_1               |[0m [2017-10-25 05:25:44,778] TRACE Controller 1 epoch 1 changed partition [_confluent-metrics,2] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,778] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [_confluent-metrics,7] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:44,779] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [_confluent-metrics,7] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:44,780] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x97 zxid:0x57 txntype:-1 reqpath:n/a Error Path:/brokers/topics/_confluent-metrics/partitions/7 Error:KeeperErrorCode = NoNode for /brokers/topics/_confluent-metrics/partitions/7 (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 17 --> ([ 17 | 3 | 1 | 8037 | 1508909144800 | 'android' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 18 --> ([ 18 | 19 | 1 | 4127 | 1508909144803 | 'iOS-test' | 'meh' ])
[35mkafka_1               |[0m [2017-10-25 05:25:44,816] TRACE Controller 1 epoch 1 changed partition [_confluent-metrics,7] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,817] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [_confluent-metrics,0] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:44,817] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [_confluent-metrics,0] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:44,818] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x9a zxid:0x5a txntype:-1 reqpath:n/a Error Path:/brokers/topics/_confluent-metrics/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/_confluent-metrics/partitions/0 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:44,854] TRACE Controller 1 epoch 1 changed partition [_confluent-metrics,0] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,854] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [_confluent-metrics,6] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:44,855] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [_confluent-metrics,6] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:44,856] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x9d zxid:0x5d txntype:-1 reqpath:n/a Error Path:/brokers/topics/_confluent-metrics/partitions/6 Error:KeeperErrorCode = NoNode for /brokers/topics/_confluent-metrics/partitions/6 (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 19 --> ([ 19 | 18 | 1 | 5192 | 1508909144861 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[35mkafka_1               |[0m [2017-10-25 05:25:44,902] TRACE Controller 1 epoch 1 changed partition [_confluent-metrics,6] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,903] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [_confluent-metrics,1] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:44,903] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [_confluent-metrics,1] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:44,905] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0xa0 zxid:0x60 txntype:-1 reqpath:n/a Error Path:/brokers/topics/_confluent-metrics/partitions/1 Error:KeeperErrorCode = NoNode for /brokers/topics/_confluent-metrics/partitions/1 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:44,943] TRACE Controller 1 epoch 1 changed partition [_confluent-metrics,1] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:44,945] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [_confluent-metrics,4] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:44,947] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [_confluent-metrics,4] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:44,949] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0xa3 zxid:0x63 txntype:-1 reqpath:n/a Error Path:/brokers/topics/_confluent-metrics/partitions/4 Error:KeeperErrorCode = NoNode for /brokers/topics/_confluent-metrics/partitions/4 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:44,987] TRACE Controller 1 epoch 1 changed partition [_confluent-metrics,4] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[33mzookeeper_1           |[0m [2017-10-25 05:25:44,988] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0xa6 zxid:0x66 txntype:-1 reqpath:n/a Error Path:/brokers/topics/_confluent-metrics/partitions/10 Error:KeeperErrorCode = NoNode for /brokers/topics/_confluent-metrics/partitions/10 (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 20 --> ([ 20 | 11 | 2 | 4885 | 1508909144992 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[35mkafka_1               |[0m [2017-10-25 05:25:44,987] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [_confluent-metrics,10] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:44,988] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [_confluent-metrics,10] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:45,023] TRACE Controller 1 epoch 1 changed partition [_confluent-metrics,10] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,024] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [_confluent-metrics,6] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,024] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [_confluent-metrics,3] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,025] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [_confluent-metrics,8] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,025] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [_confluent-metrics,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,025] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [_confluent-metrics,11] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,026] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [_confluent-metrics,5] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,026] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [_confluent-metrics,2] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,027] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [_confluent-metrics,10] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,027] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [_confluent-metrics,4] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,028] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [_confluent-metrics,7] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,028] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [_confluent-metrics,1] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,029] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [_confluent-metrics,9] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,035] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition _confluent-metrics-6 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,036] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition _confluent-metrics-3 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,036] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition _confluent-metrics-8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,036] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition _confluent-metrics-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,037] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition _confluent-metrics-11 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,037] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition _confluent-metrics-5 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,037] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition _confluent-metrics-2 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,038] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition _confluent-metrics-10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,038] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition _confluent-metrics-4 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,038] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition _confluent-metrics-7 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,039] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition _confluent-metrics-1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,039] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition _confluent-metrics-9 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,040] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 5 from controller 1 epoch 1 for partition [_confluent-metrics,11] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,046] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 5 from controller 1 epoch 1 for partition [_confluent-metrics,9] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,046] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 5 from controller 1 epoch 1 for partition [_confluent-metrics,10] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,047] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 5 from controller 1 epoch 1 for partition [_confluent-metrics,7] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,047] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 5 from controller 1 epoch 1 for partition [_confluent-metrics,8] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,047] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 5 from controller 1 epoch 1 for partition [_confluent-metrics,5] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,048] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 5 from controller 1 epoch 1 for partition [_confluent-metrics,6] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,048] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 5 from controller 1 epoch 1 for partition [_confluent-metrics,3] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,048] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 5 from controller 1 epoch 1 for partition [_confluent-metrics,4] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,048] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 5 from controller 1 epoch 1 for partition [_confluent-metrics,1] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,048] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 5 from controller 1 epoch 1 for partition [_confluent-metrics,2] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,048] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 5 from controller 1 epoch 1 for partition [_confluent-metrics,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,049] INFO [Replica state machine on controller 1]: Invoking state change to OnlineReplica for replicas [Topic=_confluent-metrics,Partition=10,Replica=1],[Topic=_confluent-metrics,Partition=0,Replica=1],[Topic=_confluent-metrics,Partition=7,Replica=1],[Topic=_confluent-metrics,Partition=9,Replica=1],[Topic=_confluent-metrics,Partition=3,Replica=1],[Topic=_confluent-metrics,Partition=1,Replica=1],[Topic=_confluent-metrics,Partition=8,Replica=1],[Topic=_confluent-metrics,Partition=11,Replica=1],[Topic=_confluent-metrics,Partition=5,Replica=1],[Topic=_confluent-metrics,Partition=6,Replica=1],[Topic=_confluent-metrics,Partition=2,Replica=1],[Topic=_confluent-metrics,Partition=4,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:45,050] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_confluent-metrics,10] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,050] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_confluent-metrics,0] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,052] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_confluent-metrics,7] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,052] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_confluent-metrics,9] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,053] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_confluent-metrics,3] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,053] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_confluent-metrics,1] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,053] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_confluent-metrics,8] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,054] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_confluent-metrics,11] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,054] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_confluent-metrics,5] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,054] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_confluent-metrics,6] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,054] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_confluent-metrics,2] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,055] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_confluent-metrics,4] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,061] TRACE Broker 1 handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition _confluent-metrics-6 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,065] TRACE Broker 1 handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition _confluent-metrics-3 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,065] TRACE Broker 1 handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition _confluent-metrics-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,065] TRACE Broker 1 handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition _confluent-metrics-10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,066] TRACE Broker 1 handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition _confluent-metrics-7 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,066] TRACE Broker 1 handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition _confluent-metrics-4 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,066] TRACE Broker 1 handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition _confluent-metrics-1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,066] TRACE Broker 1 handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition _confluent-metrics-11 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,067] TRACE Broker 1 handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition _confluent-metrics-8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,067] TRACE Broker 1 handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition _confluent-metrics-5 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,067] TRACE Broker 1 handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition _confluent-metrics-2 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,067] TRACE Broker 1 handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition _confluent-metrics-9 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,068] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions _confluent-metrics-7,_confluent-metrics-2,_confluent-metrics-5,_confluent-metrics-11,_confluent-metrics-10,_confluent-metrics-1,_confluent-metrics-9,_confluent-metrics-0,_confluent-metrics-3,_confluent-metrics-4,_confluent-metrics-6,_confluent-metrics-8 (kafka.server.ReplicaFetcherManager)
[35mkafka_1               |[0m [2017-10-25 05:25:45,079] INFO Loading producer state from offset 0 for partition _confluent-metrics-6 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:45,080] INFO Completed load of log _confluent-metrics-6 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:45,081] INFO Created log for partition [_confluent-metrics,6] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 14400000, segment.bytes -> 1073741824, retention.ms -> 259200000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:45,082] INFO Partition [_confluent-metrics,6] on broker 1: No checkpointed highwatermark is found for partition _confluent-metrics-6 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:45,082] INFO Partition [_confluent-metrics,6] on broker 1: _confluent-metrics-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:45,089] INFO Loading producer state from offset 0 for partition _confluent-metrics-3 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:45,093] INFO Completed load of log _confluent-metrics-3 with 1 log segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:45,096] INFO Created log for partition [_confluent-metrics,3] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 14400000, segment.bytes -> 1073741824, retention.ms -> 259200000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:45,097] INFO Partition [_confluent-metrics,3] on broker 1: No checkpointed highwatermark is found for partition _confluent-metrics-3 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:45,097] INFO Partition [_confluent-metrics,3] on broker 1: _confluent-metrics-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:45,109] INFO Loading producer state from offset 0 for partition _confluent-metrics-0 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:45,110] INFO Completed load of log _confluent-metrics-0 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:45,113] INFO Created log for partition [_confluent-metrics,0] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 14400000, segment.bytes -> 1073741824, retention.ms -> 259200000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:45,114] INFO Partition [_confluent-metrics,0] on broker 1: No checkpointed highwatermark is found for partition _confluent-metrics-0 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:45,115] INFO Partition [_confluent-metrics,0] on broker 1: _confluent-metrics-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:45,138] INFO Loading producer state from offset 0 for partition _confluent-metrics-10 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:45,149] INFO Completed load of log _confluent-metrics-10 with 1 log segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:45,150] INFO Created log for partition [_confluent-metrics,10] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 14400000, segment.bytes -> 1073741824, retention.ms -> 259200000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:45,150] INFO Partition [_confluent-metrics,10] on broker 1: No checkpointed highwatermark is found for partition _confluent-metrics-10 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:45,152] INFO Partition [_confluent-metrics,10] on broker 1: _confluent-metrics-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:45,186] INFO Loading producer state from offset 0 for partition _confluent-metrics-7 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:45,190] INFO Completed load of log _confluent-metrics-7 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:45,196] INFO Created log for partition [_confluent-metrics,7] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 14400000, segment.bytes -> 1073741824, retention.ms -> 259200000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:45,196] INFO Partition [_confluent-metrics,7] on broker 1: No checkpointed highwatermark is found for partition _confluent-metrics-7 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:45,198] INFO Partition [_confluent-metrics,7] on broker 1: _confluent-metrics-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:45,203] INFO Loading producer state from offset 0 for partition _confluent-metrics-4 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:45,205] INFO Completed load of log _confluent-metrics-4 with 1 log segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:45,209] INFO Created log for partition [_confluent-metrics,4] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 14400000, segment.bytes -> 1073741824, retention.ms -> 259200000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:45,210] INFO Partition [_confluent-metrics,4] on broker 1: No checkpointed highwatermark is found for partition _confluent-metrics-4 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:45,210] INFO Partition [_confluent-metrics,4] on broker 1: _confluent-metrics-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:45,220] INFO Loading producer state from offset 0 for partition _confluent-metrics-1 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:45,220] INFO Completed load of log _confluent-metrics-1 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:45,222] INFO Created log for partition [_confluent-metrics,1] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 14400000, segment.bytes -> 1073741824, retention.ms -> 259200000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:45,222] INFO Partition [_confluent-metrics,1] on broker 1: No checkpointed highwatermark is found for partition _confluent-metrics-1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:45,222] INFO Partition [_confluent-metrics,1] on broker 1: _confluent-metrics-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:45,231] INFO Loading producer state from offset 0 for partition _confluent-metrics-11 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:45,232] INFO Completed load of log _confluent-metrics-11 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:45,238] INFO Created log for partition [_confluent-metrics,11] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 14400000, segment.bytes -> 1073741824, retention.ms -> 259200000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:45,239] INFO Partition [_confluent-metrics,11] on broker 1: No checkpointed highwatermark is found for partition _confluent-metrics-11 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:45,239] INFO Partition [_confluent-metrics,11] on broker 1: _confluent-metrics-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:45,252] INFO Loading producer state from offset 0 for partition _confluent-metrics-8 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:45,253] INFO Completed load of log _confluent-metrics-8 with 1 log segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:45,256] INFO Created log for partition [_confluent-metrics,8] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 14400000, segment.bytes -> 1073741824, retention.ms -> 259200000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:45,256] INFO Partition [_confluent-metrics,8] on broker 1: No checkpointed highwatermark is found for partition _confluent-metrics-8 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:45,256] INFO Partition [_confluent-metrics,8] on broker 1: _confluent-metrics-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:45,269] INFO Loading producer state from offset 0 for partition _confluent-metrics-5 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:45,270] INFO Completed load of log _confluent-metrics-5 with 1 log segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:45,271] INFO Created log for partition [_confluent-metrics,5] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 14400000, segment.bytes -> 1073741824, retention.ms -> 259200000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:45,271] INFO Partition [_confluent-metrics,5] on broker 1: No checkpointed highwatermark is found for partition _confluent-metrics-5 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:45,271] INFO Partition [_confluent-metrics,5] on broker 1: _confluent-metrics-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36;1mksql-datagen-users_1  |[0m 21 --> ([ 21 | 16 | 2 | 607 | 1508909145273 | 'web' | '(expletive deleted)' ])
[35mkafka_1               |[0m [2017-10-25 05:25:45,283] INFO Loading producer state from offset 0 for partition _confluent-metrics-2 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:45,284] INFO Completed load of log _confluent-metrics-2 with 1 log segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:45,289] INFO Created log for partition [_confluent-metrics,2] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 14400000, segment.bytes -> 1073741824, retention.ms -> 259200000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:45,289] INFO Partition [_confluent-metrics,2] on broker 1: No checkpointed highwatermark is found for partition _confluent-metrics-2 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:45,289] INFO Partition [_confluent-metrics,2] on broker 1: _confluent-metrics-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:45,299] INFO Loading producer state from offset 0 for partition _confluent-metrics-9 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:45,305] INFO Completed load of log _confluent-metrics-9 with 1 log segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:45,306] INFO Created log for partition [_confluent-metrics,9] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 14400000, segment.bytes -> 1073741824, retention.ms -> 259200000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:45,306] INFO Partition [_confluent-metrics,9] on broker 1: No checkpointed highwatermark is found for partition _confluent-metrics-9 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:45,306] INFO Partition [_confluent-metrics,9] on broker 1: _confluent-metrics-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:45,307] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition _confluent-metrics-2 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,307] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition _confluent-metrics-5 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,307] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition _confluent-metrics-8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,307] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition _confluent-metrics-11 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,307] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition _confluent-metrics-1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,308] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition _confluent-metrics-4 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,308] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition _confluent-metrics-7 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,308] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition _confluent-metrics-10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,310] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition _confluent-metrics-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,310] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition _confluent-metrics-3 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,310] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition _confluent-metrics-6 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,310] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition _confluent-metrics-9 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,316] TRACE Broker 1 completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition _confluent-metrics-6 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,317] TRACE Broker 1 completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition _confluent-metrics-3 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,317] TRACE Broker 1 completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition _confluent-metrics-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,317] TRACE Broker 1 completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition _confluent-metrics-10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,317] TRACE Broker 1 completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition _confluent-metrics-7 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,317] TRACE Broker 1 completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition _confluent-metrics-4 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,317] TRACE Broker 1 completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition _confluent-metrics-1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,317] TRACE Broker 1 completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition _confluent-metrics-11 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,317] TRACE Broker 1 completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition _confluent-metrics-8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,317] TRACE Broker 1 completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition _confluent-metrics-5 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,317] TRACE Broker 1 completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition _confluent-metrics-2 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,318] TRACE Broker 1 completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition _confluent-metrics-9 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,322] TRACE Controller 1 epoch 1 received response {error_code=0,partitions=[{topic=_confluent-metrics,partition=11,error_code=0},{topic=_confluent-metrics,partition=9,error_code=0},{topic=_confluent-metrics,partition=10,error_code=0},{topic=_confluent-metrics,partition=7,error_code=0},{topic=_confluent-metrics,partition=8,error_code=0},{topic=_confluent-metrics,partition=5,error_code=0},{topic=_confluent-metrics,partition=6,error_code=0},{topic=_confluent-metrics,partition=3,error_code=0},{topic=_confluent-metrics,partition=4,error_code=0},{topic=_confluent-metrics,partition=1,error_code=0},{topic=_confluent-metrics,partition=2,error_code=0},{topic=_confluent-metrics,partition=0,error_code=0}]} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,330] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition _confluent-metrics-11 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,330] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition _confluent-metrics-9 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,331] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition _confluent-metrics-10 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,331] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition _confluent-metrics-7 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,331] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition _confluent-metrics-8 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,332] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition _confluent-metrics-5 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,332] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition _confluent-metrics-6 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,332] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition _confluent-metrics-3 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,333] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition _confluent-metrics-4 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,333] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition _confluent-metrics-1 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,334] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition _confluent-metrics-2 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,334] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition _confluent-metrics-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,340] TRACE Controller 1 epoch 1 received response {error_code=0} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:45,344] INFO Created metrics reporter topic _confluent-metrics (io.confluent.metrics.reporter.ConfluentMetricsReporter)
[32mmysql-db_1            |[0m Database initialized
[32mmysql-db_1            |[0m Initializing certificates
[36;1mksql-datagen-users_1  |[0m 22 --> ([ 22 | 14 | 4 | 2776 | 1508909145429 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[32mmysql-db_1            |[0m Generating a 2048 bit RSA private key
[36;1mksql-datagen-users_1  |[0m 23 --> ([ 23 | 4 | 2 | 7582 | 1508909145462 | 'web' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 24 --> ([ 24 | 6 | 4 | 3901 | 1508909145575 | 'android' | '(expletive deleted)' ])
[35mkafka_1               |[0m [2017-10-25 05:25:45,581] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: _confluent-metrics-11. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[32mmysql-db_1            |[0m ........................................................................................................................+++
[32mmysql-db_1            |[0m .................................+++
[32mmysql-db_1            |[0m unable to write 'random state'
[32mmysql-db_1            |[0m writing new private key to 'ca-key.pem'
[32mmysql-db_1            |[0m -----
[32mmysql-db_1            |[0m Generating a 2048 bit RSA private key
[32mmysql-db_1            |[0m ..................................+++
[32mmysql-db_1            |[0m ........................+++
[32mmysql-db_1            |[0m unable to write 'random state'
[32mmysql-db_1            |[0m writing new private key to 'server-key.pem'
[32mmysql-db_1            |[0m -----
[36;1mksql-datagen-users_1  |[0m 25 --> ([ 25 | 4 | 2 | 3309 | 1508909145814 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[32mmysql-db_1            |[0m Generating a 2048 bit RSA private key
[32mmysql-db_1            |[0m ........................................................+++
[32mmysql-db_1            |[0m ................+++
[32mmysql-db_1            |[0m unable to write 'random state'
[32mmysql-db_1            |[0m writing new private key to 'client-key.pem'
[32mmysql-db_1            |[0m -----
[36;1mksql-datagen-users_1  |[0m 26 --> ([ 26 | 5 | 3 | 4603 | 1508909146027 | 'ios' | 'meh' ])
[32mmysql-db_1            |[0m Certificates initialized
[32mmysql-db_1            |[0m MySQL init process in progress...
[36;1mksql-datagen-users_1  |[0m 27 --> ([ 27 | 4 | 2 | 616 | 1508909146100 | 'android' | 'worst. flight. ever. #neveragain' ])
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.261896Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.264555Z 0 [Note] mysqld (mysqld 5.7.20) starting as process 90 ...
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.267783Z 0 [Note] InnoDB: PUNCH HOLE support available
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.267868Z 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.267907Z 0 [Note] InnoDB: Uses event mutexes
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.267914Z 0 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.267995Z 0 [Note] InnoDB: Compressed tables use zlib 1.2.3
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.268013Z 0 [Note] InnoDB: Using Linux native AIO
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.268554Z 0 [Note] InnoDB: Number of pools: 1
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.268715Z 0 [Note] InnoDB: Using CPU crc32 instructions
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.270760Z 0 [Note] InnoDB: Initializing buffer pool, total size = 128M, instances = 1, chunk size = 128M
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.281191Z 0 [Note] InnoDB: Completed initialization of buffer pool
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.283028Z 0 [Note] InnoDB: If the mysqld execution user is authorized, page cleaner thread priority can be changed. See the man page of setpriority().
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.294976Z 0 [Note] InnoDB: Highest supported file format is Barracuda.
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.323731Z 0 [Note] InnoDB: Creating shared tablespace for temporary tables
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.323774Z 0 [Note] InnoDB: Setting file './ibtmp1' size to 12 MB. Physically writing the file full; Please wait ...
[36;1mksql-datagen-users_1  |[0m 28 --> ([ 28 | 9 | 1 | 5680 | 1508909146356 | 'iOS' | 'worst. flight. ever. #neveragain' ])
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.374594Z 0 [Note] InnoDB: File './ibtmp1' size is now 12 MB.
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.377925Z 0 [Note] InnoDB: 96 redo rollback segment(s) found. 96 redo rollback segment(s) are active.
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.377961Z 0 [Note] InnoDB: 32 non-redo rollback segment(s) are active.
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.382998Z 0 [Note] InnoDB: 5.7.20 started; log sequence number 2566077
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.383192Z 0 [Note] InnoDB: Loading buffer pool(s) from /var/lib/mysql/ib_buffer_pool
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.394355Z 0 [Note] Plugin 'FEDERATED' is disabled.
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.396161Z 0 [Note] InnoDB: Buffer pool(s) load completed at 171025  5:25:46
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.401581Z 0 [Note] Found ca.pem, server-cert.pem and server-key.pem in data directory. Trying to enable SSL support using them.
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.402268Z 0 [Warning] CA certificate ca.pem is self signed.
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.427037Z 0 [Warning] 'user' entry 'root@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.427079Z 0 [Warning] 'user' entry 'mysql.session@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.427088Z 0 [Warning] 'user' entry 'mysql.sys@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.427102Z 0 [Warning] 'db' entry 'performance_schema mysql.session@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.427108Z 0 [Warning] 'db' entry 'sys mysql.sys@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.427120Z 0 [Warning] 'proxies_priv' entry '@ root@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.429835Z 0 [Warning] 'tables_priv' entry 'user mysql.session@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.429930Z 0 [Warning] 'tables_priv' entry 'sys_config mysql.sys@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.437655Z 0 [Note] Event Scheduler: Loaded 0 events
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.438136Z 0 [Note] mysqld: ready for connections.
[32mmysql-db_1            |[0m Version: '5.7.20'  socket: '/var/run/mysqld/mysqld.sock'  port: 0  MySQL Community Server (GPL)
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.438151Z 0 [Note] Executing 'SELECT * FROM INFORMATION_SCHEMA.TABLES;' to get a list of tables using the deprecated partition engine. You may use the startup option '--disable-partition-engine-check' to skip this check. 
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.438157Z 0 [Note] Beginning of list of non-natively partitioned tables
[32mmysql-db_1            |[0m 2017-10-25T05:25:46.448370Z 0 [Note] End of list of non-natively partitioned tables
[36;1mksql-datagen-users_1  |[0m 29 --> ([ 29 | 11 | 3 | 6657 | 1508909146545 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 30 --> ([ 30 | 10 | 2 | 9091 | 1508909146775 | 'android' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:47,001][INFO ][o.e.c.s.ClusterService   ] [-Ryb53P] new_master {-Ryb53P}{-Ryb53PFTVe2Q8kpydTm0w}{1_9wT-hHTX67ux1_ja8NTA}{172.21.0.11}{172.21.0.11:9300}{ml.max_open_jobs=10, ml.enabled=true}, reason: zen-disco-elected-as-master ([0] nodes joined)
[36;1mksql-datagen-users_1  |[0m 31 --> ([ 31 | 11 | 2 | 2763 | 1508909147008 | 'iOS' | 'meh' ])
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:47,084][INFO ][o.e.h.n.Netty4HttpServerTransport] [-Ryb53P] publish_address {172.21.0.11:9200}, bound_addresses {0.0.0.0:9200}
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:47,084][INFO ][o.e.n.Node               ] [-Ryb53P] started
[36;1mksql-datagen-users_1  |[0m 32 --> ([ 32 | 12 | 2 | 7653 | 1508909147154 | 'android' | 'more peanuts please' ])
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:47,265][INFO ][o.e.g.GatewayService     ] [-Ryb53P] recovered [0] indices into cluster_state
[36;1mksql-datagen-users_1  |[0m 33 --> ([ 33 | 13 | 3 | 2225 | 1508909147411 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 34 --> ([ 34 | 12 | 3 | 726 | 1508909147531 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 35 --> ([ 35 | 9 | 1 | 5272 | 1508909147590 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 36 --> ([ 36 | 18 | 2 | 1414 | 1508909147623 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:47,688][INFO ][o.e.x.m.MachineLearningTemplateRegistry] [-Ryb53P] successfully created .ml-state index template
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:47,787][INFO ][o.e.x.m.MachineLearningTemplateRegistry] [-Ryb53P] successfully created .ml-notifications index template
[36;1mksql-datagen-users_1  |[0m 37 --> ([ 37 | 4 | 2 | 2998 | 1508909147890 | 'iOS' | 'more peanuts please' ])
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:47,898][INFO ][o.e.x.m.MachineLearningTemplateRegistry] [-Ryb53P] successfully created .ml-meta index template
[36;1mksql-datagen-users_1  |[0m 38 --> ([ 38 | 9 | 2 | 3287 | 1508909148054 | 'iOS-test' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 39 --> ([ 39 | 16 | 4 | 4487 | 1508909148085 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 40 --> ([ 40 | 18 | 3 | 7109 | 1508909148231 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:48,246][INFO ][o.e.x.m.MachineLearningTemplateRegistry] [-Ryb53P] successfully created .ml-anomalies- index template
[36;1mksql-datagen-users_1  |[0m 41 --> ([ 41 | 1 | 3 | 3917 | 1508909148325 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 42 --> ([ 42 | 12 | 3 | 615 | 1508909148563 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 43 --> ([ 43 | 18 | 2 | 6869 | 1508909148579 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 44 --> ([ 44 | 10 | 1 | 6802 | 1508909148671 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 45 --> ([ 45 | 1 | 2 | 7626 | 1508909148965 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 46 --> ([ 46 | 7 | 2 | 8797 | 1508909149011 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 47 --> ([ 47 | 14 | 1 | 1709 | 1508909149019 | 'web' | 'more peanuts please' ])
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:49,026][INFO ][o.e.l.LicenseService     ] [-Ryb53P] license [06b3d03c-7e2e-4f30-a3f6-a3c356237f5e] mode [trial] - valid
[36;1mksql-datagen-users_1  |[0m 48 --> ([ 48 | 19 | 4 | 1754 | 1508909149049 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 49 --> ([ 49 | 5 | 1 | 402 | 1508909149134 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 50 --> ([ 50 | 2 | 2 | 4819 | 1508909149204 | 'web' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 51 --> ([ 51 | 19 | 2 | 6536 | 1508909149269 | 'android' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 52 --> ([ 52 | -1 | 2 | 8670 | 1508909149410 | 'web' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 53 --> ([ 53 | 17 | 3 | 3287 | 1508909149497 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 54 --> ([ 54 | 14 | 3 | 6637 | 1508909149538 | 'android' | 'your team here rocks!' ])
[32mmysql-db_1            |[0m Warning: Unable to load '/usr/share/zoneinfo/iso3166.tab' as time zone. Skipping it.
[32mmysql-db_1            |[0m Warning: Unable to load '/usr/share/zoneinfo/leap-seconds.list' as time zone. Skipping it.
[36;1mksql-datagen-users_1  |[0m 55 --> ([ 55 | 19 | 1 | 227 | 1508909149611 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 56 --> ([ 56 | 16 | 4 | 9886 | 1508909149711 | 'iOS-test' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 57 --> ([ 57 | 4 | 3 | 3421 | 1508909149728 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 58 --> ([ 58 | 10 | 1 | 9725 | 1508909149980 | 'android' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 59 --> ([ 59 | 4 | 1 | 8796 | 1508909150145 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 60 --> ([ 60 | 19 | 1 | 2492 | 1508909150222 | 'web' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 61 --> ([ 61 | 14 | 4 | 560 | 1508909150308 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 62 --> ([ 62 | 9 | 1 | 5047 | 1508909150597 | 'android' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 63 --> ([ 63 | 9 | 1 | 8904 | 1508909150660 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 64 --> ([ 64 | 8 | 2 | 1407 | 1508909150777 | 'android' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 65 --> ([ 65 | 15 | 2 | 3793 | 1508909150803 | 'iOS-test' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 66 --> ([ 66 | 2 | 4 | 3472 | 1508909151053 | 'web' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 67 --> ([ 67 | 15 | 4 | 4037 | 1508909151198 | 'web' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 68 --> ([ 68 | 15 | 4 | 1409 | 1508909151331 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 69 --> ([ 69 | 6 | 3 | 3461 | 1508909151360 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 70 --> ([ 70 | 12 | 1 | 1656 | 1508909151638 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 71 --> ([ 71 | 16 | 3 | 8394 | 1508909151739 | 'android' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 72 --> ([ 72 | 16 | 4 | 3091 | 1508909151858 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[32mmysql-db_1            |[0m Warning: Unable to load '/usr/share/zoneinfo/zone.tab' as time zone. Skipping it.
[32mmysql-db_1            |[0m 2017-10-25T05:25:52.109419Z 5 [Warning] 'user' entry 'root@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:52.109525Z 5 [Warning] 'user' entry 'mysql.sys@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:52.109574Z 5 [Warning] 'db' entry 'performance_schema mysql.session@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:52.109594Z 5 [Warning] 'db' entry 'sys mysql.sys@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:52.109626Z 5 [Warning] 'proxies_priv' entry '@ root@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:52.109745Z 5 [Warning] 'tables_priv' entry 'user mysql.session@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:52.109788Z 5 [Warning] 'tables_priv' entry 'sys_config mysql.sys@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m mysql: [Warning] Using a password on the command line interface can be insecure.
[32mmysql-db_1            |[0m 
[32mmysql-db_1            |[0m /usr/local/bin/docker-entrypoint.sh: running /docker-entrypoint-initdb.d/create.sql
[32mmysql-db_1            |[0m mysql: [Warning] Using a password on the command line interface can be insecure.
[36;1mksql-datagen-users_1  |[0m 73 --> ([ 73 | 3 | 2 | 3257 | 1508909152157 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 74 --> ([ 74 | 5 | 3 | 4344 | 1508909152324 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 75 --> ([ 75 | 14 | 3 | 3477 | 1508909152411 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[32mmysql-db_1            |[0m Done, next run sql/load-data.sql
[32mmysql-db_1            |[0m Importing airlines...
[36;1mksql-datagen-users_1  |[0m 76 --> ([ 76 | -1 | 4 | 7777 | 1508909152604 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[32mmysql-db_1            |[0m Importing airports...
[36;1mksql-datagen-users_1  |[0m 77 --> ([ 77 | 3 | 2 | 7536 | 1508909152670 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[32mmysql-db_1            |[0m Importing routes...
[36;1mksql-datagen-users_1  |[0m 78 --> ([ 78 | 10 | 2 | 8137 | 1508909152816 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 79 --> ([ 79 | 13 | 4 | 2356 | 1508909152906 | 'iOS' | 'your team here rocks!' ])
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,983] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,983] INFO Added plugin 'io.confluent.connect.storage.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,983] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,984] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,984] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,984] INFO Added plugin 'io.confluent.connect.elasticsearch.ElasticsearchSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,984] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,985] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,985] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,985] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,986] INFO Added plugin 'io.confluent.connect.hdfs.HdfsSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,986] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,986] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,986] INFO Added plugin 'io.confluent.connect.s3.S3SinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,986] INFO Added plugin 'io.confluent.connect.hdfs.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,987] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,988] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,988] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,989] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,989] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,989] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,989] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,990] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,990] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,990] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,990] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,990] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,991] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,991] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,991] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,991] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,992] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,992] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,992] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,992] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,993] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,993] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,993] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,993] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,993] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,994] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,996] INFO Added aliases 'ElasticsearchSinkConnector' and 'ElasticsearchSink' to plugin 'io.confluent.connect.elasticsearch.ElasticsearchSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:52,999] INFO Added aliases 'HdfsSinkConnector' and 'HdfsSink' to plugin 'io.confluent.connect.hdfs.HdfsSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,000] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,000] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,000] INFO Added aliases 'S3SinkConnector' and 'S3Sink' to plugin 'io.confluent.connect.s3.S3SinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,001] INFO Added aliases 'DummySourceConnector' and 'DummySource' to plugin 'io.confluent.examples.sling.json.connect.DummySourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,001] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,002] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,002] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,004] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,004] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,004] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,005] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,005] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,006] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,006] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,006] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,008] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,009] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,009] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,051] INFO DistributedConfig values: 
[33;1mconnect_1             |[0m 	access.control.allow.methods = 
[33;1mconnect_1             |[0m 	access.control.allow.origin = 
[33;1mconnect_1             |[0m 	bootstrap.servers = [kafka:29092]
[33;1mconnect_1             |[0m 	client.id = 
[33;1mconnect_1             |[0m 	config.storage.replication.factor = 1
[33;1mconnect_1             |[0m 	config.storage.topic = connect-config
[33;1mconnect_1             |[0m 	connections.max.idle.ms = 540000
[33;1mconnect_1             |[0m 	group.id = connect
[33;1mconnect_1             |[0m 	heartbeat.interval.ms = 3000
[33;1mconnect_1             |[0m 	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
[33;1mconnect_1             |[0m 	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
[33;1mconnect_1             |[0m 	key.converter = class org.apache.kafka.connect.storage.StringConverter
[33;1mconnect_1             |[0m 	metadata.max.age.ms = 300000
[33;1mconnect_1             |[0m 	metric.reporters = []
[33;1mconnect_1             |[0m 	metrics.num.samples = 2
[33;1mconnect_1             |[0m 	metrics.sample.window.ms = 30000
[33;1mconnect_1             |[0m 	offset.flush.interval.ms = 60000
[33;1mconnect_1             |[0m 	offset.flush.timeout.ms = 5000
[33;1mconnect_1             |[0m 	offset.storage.partitions = 25
[33;1mconnect_1             |[0m 	offset.storage.replication.factor = 1
[33;1mconnect_1             |[0m 	offset.storage.topic = connect-offsets
[33;1mconnect_1             |[0m 	plugin.path = [/connect-plugins]
[33;1mconnect_1             |[0m 	rebalance.timeout.ms = 60000
[33;1mconnect_1             |[0m 	receive.buffer.bytes = 32768
[33;1mconnect_1             |[0m 	reconnect.backoff.max.ms = 1000
[33;1mconnect_1             |[0m 	reconnect.backoff.ms = 50
[33;1mconnect_1             |[0m 	request.timeout.ms = 40000
[33;1mconnect_1             |[0m 	rest.advertised.host.name = connect
[33;1mconnect_1             |[0m 	rest.advertised.port = null
[33;1mconnect_1             |[0m 	rest.host.name = null
[33;1mconnect_1             |[0m 	rest.port = 8083
[33;1mconnect_1             |[0m 	retry.backoff.ms = 100
[33;1mconnect_1             |[0m 	sasl.jaas.config = null
[33;1mconnect_1             |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[33;1mconnect_1             |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[33;1mconnect_1             |[0m 	sasl.kerberos.service.name = null
[33;1mconnect_1             |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[33;1mconnect_1             |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[33;1mconnect_1             |[0m 	sasl.mechanism = GSSAPI
[33;1mconnect_1             |[0m 	security.protocol = PLAINTEXT
[33;1mconnect_1             |[0m 	send.buffer.bytes = 131072
[33;1mconnect_1             |[0m 	session.timeout.ms = 10000
[33;1mconnect_1             |[0m 	ssl.cipher.suites = null
[33;1mconnect_1             |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[33;1mconnect_1             |[0m 	ssl.endpoint.identification.algorithm = null
[33;1mconnect_1             |[0m 	ssl.key.password = null
[33;1mconnect_1             |[0m 	ssl.keymanager.algorithm = SunX509
[33;1mconnect_1             |[0m 	ssl.keystore.location = null
[33;1mconnect_1             |[0m 	ssl.keystore.password = null
[33;1mconnect_1             |[0m 	ssl.keystore.type = JKS
[33;1mconnect_1             |[0m 	ssl.protocol = TLS
[33;1mconnect_1             |[0m 	ssl.provider = null
[33;1mconnect_1             |[0m 	ssl.secure.random.implementation = null
[33;1mconnect_1             |[0m 	ssl.trustmanager.algorithm = PKIX
[33;1mconnect_1             |[0m 	ssl.truststore.location = null
[33;1mconnect_1             |[0m 	ssl.truststore.password = null
[33;1mconnect_1             |[0m 	ssl.truststore.type = JKS
[33;1mconnect_1             |[0m 	status.storage.partitions = 5
[33;1mconnect_1             |[0m 	status.storage.replication.factor = 1
[33;1mconnect_1             |[0m 	status.storage.topic = connect-status
[33;1mconnect_1             |[0m 	task.shutdown.graceful.timeout.ms = 5000
[33;1mconnect_1             |[0m 	value.converter = class org.apache.kafka.connect.json.JsonConverter
[33;1mconnect_1             |[0m 	worker.sync.timeout.ms = 3000
[33;1mconnect_1             |[0m 	worker.unsync.backoff.ms = 300000
[33;1mconnect_1             |[0m  (org.apache.kafka.connect.runtime.distributed.DistributedConfig)
[36;1mksql-datagen-users_1  |[0m 80 --> ([ 80 | 2 | 4 | 6381 | 1508909153189 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,251] INFO Logging initialized @21555ms (org.eclipse.jetty.util.log)
[32mmysql-db_1            |[0m Importing countries...
[32mmysql-db_1            |[0m INSERTING USERS NOW
[32mmysql-db_1            |[0m Done.
[32mmysql-db_1            |[0m 
[32mmysql-db_1            |[0m 
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.267621Z 0 [Note] Giving 0 client threads a chance to die gracefully
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.267716Z 0 [Note] Shutting down slave threads
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.267759Z 0 [Note] Forcefully disconnecting 0 remaining clients
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.267861Z 0 [Note] Event Scheduler: Purging the queue. 0 events
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.267943Z 0 [Note] Binlog end
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269110Z 0 [Note] Shutting down plugin 'ngram'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269168Z 0 [Note] Shutting down plugin 'BLACKHOLE'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269175Z 0 [Note] Shutting down plugin 'partition'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269179Z 0 [Note] Shutting down plugin 'ARCHIVE'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269183Z 0 [Note] Shutting down plugin 'INNODB_SYS_VIRTUAL'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269196Z 0 [Note] Shutting down plugin 'INNODB_SYS_DATAFILES'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269200Z 0 [Note] Shutting down plugin 'INNODB_SYS_TABLESPACES'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269204Z 0 [Note] Shutting down plugin 'INNODB_SYS_FOREIGN_COLS'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269206Z 0 [Note] Shutting down plugin 'INNODB_SYS_FOREIGN'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269210Z 0 [Note] Shutting down plugin 'INNODB_SYS_FIELDS'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269213Z 0 [Note] Shutting down plugin 'INNODB_SYS_COLUMNS'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269258Z 0 [Note] Shutting down plugin 'INNODB_SYS_INDEXES'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269271Z 0 [Note] Shutting down plugin 'INNODB_SYS_TABLESTATS'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269275Z 0 [Note] Shutting down plugin 'INNODB_SYS_TABLES'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269287Z 0 [Note] Shutting down plugin 'INNODB_FT_INDEX_TABLE'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269291Z 0 [Note] Shutting down plugin 'INNODB_FT_INDEX_CACHE'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269300Z 0 [Note] Shutting down plugin 'INNODB_FT_CONFIG'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269315Z 0 [Note] Shutting down plugin 'INNODB_FT_BEING_DELETED'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269318Z 0 [Note] Shutting down plugin 'INNODB_FT_DELETED'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269396Z 0 [Note] Shutting down plugin 'INNODB_FT_DEFAULT_STOPWORD'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269401Z 0 [Note] Shutting down plugin 'INNODB_METRICS'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269405Z 0 [Note] Shutting down plugin 'INNODB_TEMP_TABLE_INFO'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269417Z 0 [Note] Shutting down plugin 'INNODB_BUFFER_POOL_STATS'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269421Z 0 [Note] Shutting down plugin 'INNODB_BUFFER_PAGE_LRU'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269426Z 0 [Note] Shutting down plugin 'INNODB_BUFFER_PAGE'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269438Z 0 [Note] Shutting down plugin 'INNODB_CMP_PER_INDEX_RESET'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269444Z 0 [Note] Shutting down plugin 'INNODB_CMP_PER_INDEX'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269510Z 0 [Note] Shutting down plugin 'INNODB_CMPMEM_RESET'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269513Z 0 [Note] Shutting down plugin 'INNODB_CMPMEM'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269517Z 0 [Note] Shutting down plugin 'INNODB_CMP_RESET'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269521Z 0 [Note] Shutting down plugin 'INNODB_CMP'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269525Z 0 [Note] Shutting down plugin 'INNODB_LOCK_WAITS'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269528Z 0 [Note] Shutting down plugin 'INNODB_LOCKS'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269531Z 0 [Note] Shutting down plugin 'INNODB_TRX'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269533Z 0 [Note] Shutting down plugin 'InnoDB'
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.269690Z 0 [Note] InnoDB: FTS optimize thread exiting.
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.270023Z 0 [Note] InnoDB: Starting shutdown...
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.370450Z 0 [Note] InnoDB: Dumping buffer pool(s) to /var/lib/mysql/ib_buffer_pool
[32mmysql-db_1            |[0m 2017-10-25T05:25:53.388138Z 0 [Note] InnoDB: Buffer pool(s) dump completed at 171025  5:25:53
[36;1mksql-datagen-users_1  |[0m 81 --> ([ 81 | 7 | 3 | 8813 | 1508909153429 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,665] INFO Kafka version : 0.11.0.0-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,666] INFO Kafka commitId : 6a8cf706ddc9ab6a (org.apache.kafka.common.utils.AppInfoParser)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,681] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,686] INFO Starting REST server (org.apache.kafka.connect.runtime.rest.RestServer)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,687] INFO Herder starting (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,687] INFO Worker starting (org.apache.kafka.connect.runtime.Worker)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,687] INFO Starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,688] INFO Starting KafkaBasedLog with topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog)
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:53,703][INFO ][o.e.c.m.MetaDataCreateIndexService] [-Ryb53P] [.monitoring-es-6-2017.10.25] creating index, cause [auto(bulk api)], templates [.monitoring-es], shards [1]/[1], mappings [doc]
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,710] INFO AdminClientConfig values: 
[33;1mconnect_1             |[0m 	bootstrap.servers = [kafka:29092]
[33;1mconnect_1             |[0m 	client.id = 
[33;1mconnect_1             |[0m 	connections.max.idle.ms = 300000
[33;1mconnect_1             |[0m 	metadata.max.age.ms = 300000
[33;1mconnect_1             |[0m 	metric.reporters = []
[33;1mconnect_1             |[0m 	metrics.num.samples = 2
[33;1mconnect_1             |[0m 	metrics.recording.level = INFO
[33;1mconnect_1             |[0m 	metrics.sample.window.ms = 30000
[33;1mconnect_1             |[0m 	receive.buffer.bytes = 65536
[33;1mconnect_1             |[0m 	reconnect.backoff.max.ms = 1000
[33;1mconnect_1             |[0m 	reconnect.backoff.ms = 50
[33;1mconnect_1             |[0m 	request.timeout.ms = 120000
[33;1mconnect_1             |[0m 	retries = 5
[33;1mconnect_1             |[0m 	retry.backoff.ms = 100
[33;1mconnect_1             |[0m 	sasl.jaas.config = null
[33;1mconnect_1             |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[33;1mconnect_1             |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[33;1mconnect_1             |[0m 	sasl.kerberos.service.name = null
[33;1mconnect_1             |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[33;1mconnect_1             |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[33;1mconnect_1             |[0m 	sasl.mechanism = GSSAPI
[33;1mconnect_1             |[0m 	security.protocol = PLAINTEXT
[33;1mconnect_1             |[0m 	send.buffer.bytes = 131072
[33;1mconnect_1             |[0m 	ssl.cipher.suites = null
[33;1mconnect_1             |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[33;1mconnect_1             |[0m 	ssl.endpoint.identification.algorithm = null
[33;1mconnect_1             |[0m 	ssl.key.password = null
[33;1mconnect_1             |[0m 	ssl.keymanager.algorithm = SunX509
[33;1mconnect_1             |[0m 	ssl.keystore.location = null
[33;1mconnect_1             |[0m 	ssl.keystore.password = null
[33;1mconnect_1             |[0m 	ssl.keystore.type = JKS
[33;1mconnect_1             |[0m 	ssl.protocol = TLS
[33;1mconnect_1             |[0m 	ssl.provider = null
[33;1mconnect_1             |[0m 	ssl.secure.random.implementation = null
[33;1mconnect_1             |[0m 	ssl.trustmanager.algorithm = PKIX
[33;1mconnect_1             |[0m 	ssl.truststore.location = null
[33;1mconnect_1             |[0m 	ssl.truststore.password = null
[33;1mconnect_1             |[0m 	ssl.truststore.type = JKS
[33;1mconnect_1             |[0m  (org.apache.kafka.clients.admin.AdminClientConfig)
[36;1mksql-datagen-users_1  |[0m 82 --> ([ 82 | 10 | 3 | 1694 | 1508909153727 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 83 --> ([ 83 | 18 | 1 | 6644 | 1508909153763 | 'android' | 'is this as good as it gets? really ?' ])
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,769] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,769] WARN The configuration 'replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,771] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,771] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,771] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,771] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,771] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,771] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,776] WARN The configuration 'internal.key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,777] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,777] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,800] WARN The configuration 'internal.key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,800] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,800] WARN The configuration 'internal.value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,800] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,801] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,801] WARN The configuration 'internal.value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,801] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,801] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,802] WARN The configuration 'log4j.root.loglevel' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,802] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,802] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36;1mksql-datagen-users_1  |[0m 84 --> ([ 84 | 9 | 4 | 7199 | 1508909153973 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[33;1mconnect_1             |[0m [2017-10-25 05:25:53,993] INFO jetty-9.2.15.v20160210 (org.eclipse.jetty.server.Server)
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:54,067][INFO ][o.e.c.m.MetaDataCreateIndexService] [-Ryb53P] [.watches] creating index, cause [auto(bulk api)], templates [watches], shards [1]/[1], mappings [watch]
[36;1mksql-datagen-users_1  |[0m 85 --> ([ 85 | 5 | 3 | 460 | 1508909154078 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 86 --> ([ 86 | 11 | 4 | 7374 | 1508909154196 | 'ios' | 'is this as good as it gets? really ?' ])
[33mzookeeper_1           |[0m [2017-10-25 05:25:54,203] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:setData cxid:0xb6 zxid:0x69 txntype:-1 reqpath:n/a Error Path:/config/topics/connect-offsets Error:KeeperErrorCode = NoNode for /config/topics/connect-offsets (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:25:54,215] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0xb7 zxid:0x6a txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:54,260] INFO Topic creation {"version":1,"partitions":{"12":[1],"8":[1],"19":[1],"23":[1],"4":[1],"15":[1],"11":[1],"9":[1],"22":[1],"13":[1],"24":[1],"16":[1],"5":[1],"10":[1],"21":[1],"6":[1],"1":[1],"17":[1],"14":[1],"0":[1],"20":[1],"2":[1],"18":[1],"7":[1],"3":[1]}} (kafka.admin.AdminUtils$)
[35mkafka_1               |[0m [2017-10-25 05:25:54,303] INFO [Controller 1]: New topics: [Set(connect-offsets)], deleted topics: [Set()], new partition replica assignment [Map([connect-offsets,17] -> List(1), [connect-offsets,2] -> List(1), [connect-offsets,15] -> List(1), [connect-offsets,12] -> List(1), [connect-offsets,9] -> List(1), [connect-offsets,4] -> List(1), [connect-offsets,5] -> List(1), [connect-offsets,22] -> List(1), [connect-offsets,0] -> List(1), [connect-offsets,10] -> List(1), [connect-offsets,7] -> List(1), [connect-offsets,11] -> List(1), [connect-offsets,6] -> List(1), [connect-offsets,24] -> List(1), [connect-offsets,14] -> List(1), [connect-offsets,23] -> List(1), [connect-offsets,1] -> List(1), [connect-offsets,8] -> List(1), [connect-offsets,18] -> List(1), [connect-offsets,3] -> List(1), [connect-offsets,21] -> List(1), [connect-offsets,16] -> List(1), [connect-offsets,20] -> List(1), [connect-offsets,19] -> List(1), [connect-offsets,13] -> List(1))] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:54,303] INFO [Controller 1]: New topic creation callback for [connect-offsets,17],[connect-offsets,2],[connect-offsets,15],[connect-offsets,12],[connect-offsets,9],[connect-offsets,4],[connect-offsets,5],[connect-offsets,22],[connect-offsets,0],[connect-offsets,10],[connect-offsets,7],[connect-offsets,11],[connect-offsets,6],[connect-offsets,24],[connect-offsets,14],[connect-offsets,23],[connect-offsets,1],[connect-offsets,8],[connect-offsets,18],[connect-offsets,3],[connect-offsets,21],[connect-offsets,16],[connect-offsets,20],[connect-offsets,19],[connect-offsets,13] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:54,311] INFO [Controller 1]: New partition creation callback for [connect-offsets,17],[connect-offsets,2],[connect-offsets,15],[connect-offsets,12],[connect-offsets,9],[connect-offsets,4],[connect-offsets,5],[connect-offsets,22],[connect-offsets,0],[connect-offsets,10],[connect-offsets,7],[connect-offsets,11],[connect-offsets,6],[connect-offsets,24],[connect-offsets,14],[connect-offsets,23],[connect-offsets,1],[connect-offsets,8],[connect-offsets,18],[connect-offsets,3],[connect-offsets,21],[connect-offsets,16],[connect-offsets,20],[connect-offsets,19],[connect-offsets,13] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:54,313] INFO [Partition state machine on Controller 1]: Invoking state change to NewPartition for partitions [connect-offsets,17],[connect-offsets,2],[connect-offsets,15],[connect-offsets,12],[connect-offsets,9],[connect-offsets,4],[connect-offsets,5],[connect-offsets,22],[connect-offsets,0],[connect-offsets,10],[connect-offsets,7],[connect-offsets,11],[connect-offsets,6],[connect-offsets,24],[connect-offsets,14],[connect-offsets,23],[connect-offsets,1],[connect-offsets,8],[connect-offsets,18],[connect-offsets,3],[connect-offsets,21],[connect-offsets,16],[connect-offsets,20],[connect-offsets,19],[connect-offsets,13] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:54,314] TRACE Controller 1 epoch 1 changed partition [connect-offsets,17] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,314] TRACE Controller 1 epoch 1 changed partition [connect-offsets,2] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,316] TRACE Controller 1 epoch 1 changed partition [connect-offsets,15] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,319] TRACE Controller 1 epoch 1 changed partition [connect-offsets,12] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,319] TRACE Controller 1 epoch 1 changed partition [connect-offsets,9] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,319] TRACE Controller 1 epoch 1 changed partition [connect-offsets,4] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,320] TRACE Controller 1 epoch 1 changed partition [connect-offsets,5] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,320] TRACE Controller 1 epoch 1 changed partition [connect-offsets,22] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,320] TRACE Controller 1 epoch 1 changed partition [connect-offsets,0] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,321] TRACE Controller 1 epoch 1 changed partition [connect-offsets,10] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,321] TRACE Controller 1 epoch 1 changed partition [connect-offsets,7] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,322] TRACE Controller 1 epoch 1 changed partition [connect-offsets,11] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,322] TRACE Controller 1 epoch 1 changed partition [connect-offsets,6] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,323] TRACE Controller 1 epoch 1 changed partition [connect-offsets,24] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,323] TRACE Controller 1 epoch 1 changed partition [connect-offsets,14] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,323] TRACE Controller 1 epoch 1 changed partition [connect-offsets,23] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,326] TRACE Controller 1 epoch 1 changed partition [connect-offsets,1] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,332] TRACE Controller 1 epoch 1 changed partition [connect-offsets,8] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,332] TRACE Controller 1 epoch 1 changed partition [connect-offsets,18] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,333] TRACE Controller 1 epoch 1 changed partition [connect-offsets,3] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,333] TRACE Controller 1 epoch 1 changed partition [connect-offsets,21] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,333] TRACE Controller 1 epoch 1 changed partition [connect-offsets,16] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,333] TRACE Controller 1 epoch 1 changed partition [connect-offsets,20] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,334] TRACE Controller 1 epoch 1 changed partition [connect-offsets,19] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,334] TRACE Controller 1 epoch 1 changed partition [connect-offsets,13] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,335] INFO [Replica state machine on controller 1]: Invoking state change to NewReplica for replicas [Topic=connect-offsets,Partition=4,Replica=1],[Topic=connect-offsets,Partition=21,Replica=1],[Topic=connect-offsets,Partition=15,Replica=1],[Topic=connect-offsets,Partition=24,Replica=1],[Topic=connect-offsets,Partition=11,Replica=1],[Topic=connect-offsets,Partition=23,Replica=1],[Topic=connect-offsets,Partition=19,Replica=1],[Topic=connect-offsets,Partition=6,Replica=1],[Topic=connect-offsets,Partition=10,Replica=1],[Topic=connect-offsets,Partition=9,Replica=1],[Topic=connect-offsets,Partition=14,Replica=1],[Topic=connect-offsets,Partition=5,Replica=1],[Topic=connect-offsets,Partition=13,Replica=1],[Topic=connect-offsets,Partition=0,Replica=1],[Topic=connect-offsets,Partition=1,Replica=1],[Topic=connect-offsets,Partition=8,Replica=1],[Topic=connect-offsets,Partition=12,Replica=1],[Topic=connect-offsets,Partition=3,Replica=1],[Topic=connect-offsets,Partition=17,Replica=1],[Topic=connect-offsets,Partition=2,Replica=1],[Topic=connect-offsets,Partition=22,Replica=1],[Topic=connect-offsets,Partition=18,Replica=1],[Topic=connect-offsets,Partition=7,Replica=1],[Topic=connect-offsets,Partition=20,Replica=1],[Topic=connect-offsets,Partition=16,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:54,338] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,4] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,350] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,21] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,351] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,15] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,352] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,24] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,354] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,11] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,358] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,23] from NonExistentReplica to NewReplica (state.change.logger)
[36;1mksql-datagen-users_1  |[0m 87 --> ([ 87 | 0 | 2 | 1173 | 1508909154363 | 'iOS-test' | 'meh' ])
[35mkafka_1               |[0m [2017-10-25 05:25:54,371] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,19] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,374] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,6] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,377] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,10] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,381] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,9] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,384] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,14] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,390] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,5] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,391] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,13] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,393] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,0] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,395] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,1] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,397] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,8] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,402] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,12] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,404] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,3] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,405] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,17] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,406] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,2] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,410] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,22] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,411] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,18] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,415] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,7] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,416] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,20] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,417] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,16] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,417] INFO [Partition state machine on Controller 1]: Invoking state change to OnlinePartition for partitions [connect-offsets,17],[connect-offsets,2],[connect-offsets,15],[connect-offsets,12],[connect-offsets,9],[connect-offsets,4],[connect-offsets,5],[connect-offsets,22],[connect-offsets,0],[connect-offsets,10],[connect-offsets,7],[connect-offsets,11],[connect-offsets,6],[connect-offsets,24],[connect-offsets,14],[connect-offsets,23],[connect-offsets,1],[connect-offsets,8],[connect-offsets,18],[connect-offsets,3],[connect-offsets,21],[connect-offsets,16],[connect-offsets,20],[connect-offsets,19],[connect-offsets,13] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:54,418] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-offsets,17] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:54,418] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-offsets,17] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:54,419] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0xd7 zxid:0x6d txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-offsets/partitions/17 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-offsets/partitions/17 (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:25:54,440] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0xd8 zxid:0x6e txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/connect-offsets/partitions (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 88 --> ([ 88 | 16 | 2 | 7667 | 1508909154509 | 'iOS-test' | 'is this as good as it gets? really ?' ])
[35mkafka_1               |[0m [2017-10-25 05:25:54,573] TRACE Controller 1 epoch 1 changed partition [connect-offsets,17] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[33mzookeeper_1           |[0m [2017-10-25 05:25:54,576] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0xdc zxid:0x72 txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-offsets/partitions/2 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-offsets/partitions/2 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:54,574] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-offsets,2] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:54,574] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-offsets,2] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:54,632] TRACE Controller 1 epoch 1 changed partition [connect-offsets,2] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,632] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-offsets,15] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:54,633] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-offsets,15] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:54,634] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0xdf zxid:0x75 txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-offsets/partitions/15 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-offsets/partitions/15 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:54,690] TRACE Controller 1 epoch 1 changed partition [connect-offsets,15] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,691] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-offsets,12] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:54,692] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-offsets,12] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:54,694] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0xe2 zxid:0x78 txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-offsets/partitions/12 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-offsets/partitions/12 (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 89 --> ([ 89 | 18 | 2 | 9580 | 1508909154733 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[35mkafka_1               |[0m [2017-10-25 05:25:54,745] TRACE Controller 1 epoch 1 changed partition [connect-offsets,12] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,746] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-offsets,9] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:54,746] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-offsets,9] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:54,747] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0xe5 zxid:0x7b txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-offsets/partitions/9 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-offsets/partitions/9 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:54,786] TRACE Controller 1 epoch 1 changed partition [connect-offsets,9] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,787] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-offsets,4] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:54,788] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-offsets,4] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:54,789] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0xe8 zxid:0x7e txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-offsets/partitions/4 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-offsets/partitions/4 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:54,821] TRACE Controller 1 epoch 1 changed partition [connect-offsets,4] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,821] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-offsets,5] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:54,822] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-offsets,5] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:54,823] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0xeb zxid:0x81 txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-offsets/partitions/5 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-offsets/partitions/5 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:54,864] TRACE Controller 1 epoch 1 changed partition [connect-offsets,5] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,864] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-offsets,22] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:54,865] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-offsets,22] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:54,866] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0xee zxid:0x84 txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-offsets/partitions/22 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-offsets/partitions/22 (org.apache.zookeeper.server.PrepRequestProcessor)
[35;1melasticsearch_1       |[0m [2017-10-25T05:25:54,881][INFO ][o.e.c.m.MetaDataMappingService] [-Ryb53P] [.watches/TK9ww4TGQEieKPV0J5JaaQ] update_mapping [watch]
[35mkafka_1               |[0m [2017-10-25 05:25:54,900] TRACE Controller 1 epoch 1 changed partition [connect-offsets,22] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,900] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-offsets,0] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:54,901] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-offsets,0] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:54,902] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0xf1 zxid:0x87 txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-offsets/partitions/0 (org.apache.zookeeper.server.PrepRequestProcessor)
[32mmysql-db_1            |[0m 2017-10-25T05:25:54.912739Z 0 [Note] InnoDB: Shutdown completed; log sequence number 12170379
[32mmysql-db_1            |[0m 2017-10-25T05:25:54.920830Z 0 [Note] InnoDB: Removed temporary tablespace data file: "ibtmp1"
[32mmysql-db_1            |[0m 2017-10-25T05:25:54.920867Z 0 [Note] Shutting down plugin 'MRG_MYISAM'
[32mmysql-db_1            |[0m 2017-10-25T05:25:54.920874Z 0 [Note] Shutting down plugin 'MyISAM'
[32mmysql-db_1            |[0m 2017-10-25T05:25:54.920885Z 0 [Note] Shutting down plugin 'CSV'
[32mmysql-db_1            |[0m 2017-10-25T05:25:54.920890Z 0 [Note] Shutting down plugin 'MEMORY'
[32mmysql-db_1            |[0m 2017-10-25T05:25:54.920895Z 0 [Note] Shutting down plugin 'PERFORMANCE_SCHEMA'
[32mmysql-db_1            |[0m 2017-10-25T05:25:54.920919Z 0 [Note] Shutting down plugin 'sha256_password'
[32mmysql-db_1            |[0m 2017-10-25T05:25:54.920933Z 0 [Note] Shutting down plugin 'mysql_native_password'
[32mmysql-db_1            |[0m 2017-10-25T05:25:54.921406Z 0 [Note] Shutting down plugin 'binlog'
[32mmysql-db_1            |[0m 2017-10-25T05:25:54.931985Z 0 [Note] mysqld: Shutdown complete
[32mmysql-db_1            |[0m 
[36;1mksql-datagen-users_1  |[0m 90 --> ([ 90 | 15 | 1 | 9343 | 1508909154940 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[35mkafka_1               |[0m [2017-10-25 05:25:54,973] TRACE Controller 1 epoch 1 changed partition [connect-offsets,0] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:54,974] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-offsets,10] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:54,974] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-offsets,10] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:54,976] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0xf4 zxid:0x8a txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-offsets/partitions/10 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-offsets/partitions/10 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:55,018] TRACE Controller 1 epoch 1 changed partition [connect-offsets,10] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,018] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-offsets,7] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:55,019] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-offsets,7] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:55,020] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0xf7 zxid:0x8d txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-offsets/partitions/7 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-offsets/partitions/7 (org.apache.zookeeper.server.PrepRequestProcessor)
[32mmysql-db_1            |[0m 
[32mmysql-db_1            |[0m MySQL init process done. Ready for start up.
[32mmysql-db_1            |[0m 
[35mkafka_1               |[0m [2017-10-25 05:25:55,057] TRACE Controller 1 epoch 1 changed partition [connect-offsets,7] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,058] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-offsets,11] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:55,058] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-offsets,11] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:55,059] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0xfa zxid:0x90 txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-offsets/partitions/11 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-offsets/partitions/11 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:55,130] TRACE Controller 1 epoch 1 changed partition [connect-offsets,11] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,131] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-offsets,6] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:55,132] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-offsets,6] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:55,133] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0xfd zxid:0x93 txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-offsets/partitions/6 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-offsets/partitions/6 (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 91 --> ([ 91 | 5 | 3 | 1342 | 1508909155152 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[35mkafka_1               |[0m [2017-10-25 05:25:55,172] TRACE Controller 1 epoch 1 changed partition [connect-offsets,6] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,173] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-offsets,24] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:55,173] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-offsets,24] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:55,174] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x100 zxid:0x96 txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-offsets/partitions/24 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-offsets/partitions/24 (org.apache.zookeeper.server.PrepRequestProcessor)
[33;1mconnect_1             |[0m Oct 25, 2017 5:25:55 AM org.glassfish.jersey.internal.Errors logErrors
[33;1mconnect_1             |[0m WARNING: The following warnings have been detected: WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
[33;1mconnect_1             |[0m WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
[33;1mconnect_1             |[0m WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
[33;1mconnect_1             |[0m WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.
[33;1mconnect_1             |[0m 
[35mkafka_1               |[0m [2017-10-25 05:25:55,217] TRACE Controller 1 epoch 1 changed partition [connect-offsets,24] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,217] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-offsets,14] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:55,218] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-offsets,14] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:55,219] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x103 zxid:0x99 txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-offsets/partitions/14 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-offsets/partitions/14 (org.apache.zookeeper.server.PrepRequestProcessor)
[33;1mconnect_1             |[0m [2017-10-25 05:25:55,229] INFO Started o.e.j.s.ServletContextHandler@3d1624a8{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.249292Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.250293Z 0 [Note] mysqld (mysqld 5.7.20) starting as process 1 ...
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.254557Z 0 [Note] InnoDB: PUNCH HOLE support available
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.254593Z 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.254599Z 0 [Note] InnoDB: Uses event mutexes
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.254602Z 0 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.254605Z 0 [Note] InnoDB: Compressed tables use zlib 1.2.3
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.254608Z 0 [Note] InnoDB: Using Linux native AIO
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.255029Z 0 [Note] InnoDB: Number of pools: 1
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.255470Z 0 [Note] InnoDB: Using CPU crc32 instructions
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.257374Z 0 [Note] InnoDB: Initializing buffer pool, total size = 128M, instances = 1, chunk size = 128M
[35mkafka_1               |[0m [2017-10-25 05:25:55,257] TRACE Controller 1 epoch 1 changed partition [connect-offsets,14] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,258] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-offsets,23] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:55,258] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-offsets,23] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:55,259] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x106 zxid:0x9c txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-offsets/partitions/23 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-offsets/partitions/23 (org.apache.zookeeper.server.PrepRequestProcessor)
[33;1mconnect_1             |[0m [2017-10-25 05:25:55,263] INFO Started ServerConnector@36c81e64{HTTP/1.1}{0.0.0.0:8083} (org.eclipse.jetty.server.ServerConnector)
[33;1mconnect_1             |[0m [2017-10-25 05:25:55,263] INFO Started @23567ms (org.eclipse.jetty.server.Server)
[33;1mconnect_1             |[0m [2017-10-25 05:25:55,270] INFO REST server listening at http://172.21.0.9:8083/, advertising URL http://connect:8083/ (org.apache.kafka.connect.runtime.rest.RestServer)
[33;1mconnect_1             |[0m [2017-10-25 05:25:55,270] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect)
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.271243Z 0 [Note] InnoDB: Completed initialization of buffer pool
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.274261Z 0 [Note] InnoDB: If the mysqld execution user is authorized, page cleaner thread priority can be changed. See the man page of setpriority().
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.287970Z 0 [Note] InnoDB: Highest supported file format is Barracuda.
[35mkafka_1               |[0m [2017-10-25 05:25:55,297] TRACE Controller 1 epoch 1 changed partition [connect-offsets,23] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,298] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-offsets,1] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:55,300] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-offsets,1] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:55,301] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x109 zxid:0x9f txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-offsets/partitions/1 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-offsets/partitions/1 (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 92 --> ([ 92 | 6 | 4 | 5049 | 1508909155307 | 'iOS-test' | '(expletive deleted)' ])
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.327251Z 0 [Note] InnoDB: Creating shared tablespace for temporary tables
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.327703Z 0 [Note] InnoDB: Setting file './ibtmp1' size to 12 MB. Physically writing the file full; Please wait ...
[35mkafka_1               |[0m [2017-10-25 05:25:55,345] TRACE Controller 1 epoch 1 changed partition [connect-offsets,1] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,345] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-offsets,8] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:55,346] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-offsets,8] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:55,348] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x10c zxid:0xa2 txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-offsets/partitions/8 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-offsets/partitions/8 (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 93 --> ([ 93 | 18 | 4 | 5122 | 1508909155371 | 'android' | 'your team here rocks!' ])
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.397199Z 0 [Note] InnoDB: File './ibtmp1' size is now 12 MB.
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.399460Z 0 [Note] InnoDB: 96 redo rollback segment(s) found. 96 redo rollback segment(s) are active.
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.399492Z 0 [Note] InnoDB: 32 non-redo rollback segment(s) are active.
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.401024Z 0 [Note] InnoDB: Waiting for purge to start
[35mkafka_1               |[0m [2017-10-25 05:25:55,412] TRACE Controller 1 epoch 1 changed partition [connect-offsets,8] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,412] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-offsets,18] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:55,412] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-offsets,18] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:55,415] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x10f zxid:0xa5 txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-offsets/partitions/18 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-offsets/partitions/18 (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 94 --> ([ 94 | 8 | 3 | 3108 | 1508909155433 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.451273Z 0 [Note] InnoDB: 5.7.20 started; log sequence number 12170379
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.451732Z 0 [Note] Plugin 'FEDERATED' is disabled.
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.451943Z 0 [Note] InnoDB: Loading buffer pool(s) from /var/lib/mysql/ib_buffer_pool
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.455878Z 0 [Note] Found ca.pem, server-cert.pem and server-key.pem in data directory. Trying to enable SSL support using them.
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.457132Z 0 [Warning] CA certificate ca.pem is self signed.
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.459996Z 0 [Note] Server hostname (bind-address): '*'; port: 3306
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.460062Z 0 [Note] IPv6 is available.
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.460079Z 0 [Note]   - '::' resolves to '::';
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.460123Z 0 [Note] Server socket created on IP: '::'.
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.462149Z 0 [Note] InnoDB: Buffer pool(s) load completed at 171025  5:25:55
[33mzookeeper_1           |[0m [2017-10-25 05:25:55,469] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x112 zxid:0xa8 txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-offsets/partitions/3 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-offsets/partitions/3 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:55,467] TRACE Controller 1 epoch 1 changed partition [connect-offsets,18] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,467] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-offsets,3] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:55,468] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-offsets,3] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.488184Z 0 [Warning] 'user' entry 'root@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.488224Z 0 [Warning] 'user' entry 'mysql.sys@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.488280Z 0 [Warning] 'db' entry 'performance_schema mysql.session@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.488298Z 0 [Warning] 'db' entry 'sys mysql.sys@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.488309Z 0 [Warning] 'proxies_priv' entry '@ root@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.494432Z 0 [Warning] 'tables_priv' entry 'user mysql.session@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.494499Z 0 [Warning] 'tables_priv' entry 'sys_config mysql.sys@localhost' ignored in --skip-name-resolve mode.
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.500954Z 0 [Note] Event Scheduler: Loaded 0 events
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.501337Z 0 [Note] mysqld: ready for connections.
[32mmysql-db_1            |[0m Version: '5.7.20'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Community Server (GPL)
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.501362Z 0 [Note] Executing 'SELECT * FROM INFORMATION_SCHEMA.TABLES;' to get a list of tables using the deprecated partition engine. You may use the startup option '--disable-partition-engine-check' to skip this check. 
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.501428Z 0 [Note] Beginning of list of non-natively partitioned tables
[32mmysql-db_1            |[0m 2017-10-25T05:25:55.510698Z 0 [Note] End of list of non-natively partitioned tables
[35mkafka_1               |[0m [2017-10-25 05:25:55,538] TRACE Controller 1 epoch 1 changed partition [connect-offsets,3] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,538] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-offsets,21] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:55,539] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-offsets,21] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:55,540] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x115 zxid:0xab txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-offsets/partitions/21 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-offsets/partitions/21 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:55,572] TRACE Controller 1 epoch 1 changed partition [connect-offsets,21] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,572] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-offsets,16] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:55,573] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-offsets,16] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:55,574] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x118 zxid:0xae txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-offsets/partitions/16 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-offsets/partitions/16 (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 95 --> ([ 95 | 4 | 2 | 8182 | 1508909155591 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[35mkafka_1               |[0m [2017-10-25 05:25:55,609] TRACE Controller 1 epoch 1 changed partition [connect-offsets,16] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,609] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-offsets,20] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:55,609] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-offsets,20] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:55,610] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x11b zxid:0xb1 txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-offsets/partitions/20 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-offsets/partitions/20 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:55,654] TRACE Controller 1 epoch 1 changed partition [connect-offsets,20] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,654] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-offsets,19] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:55,654] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-offsets,19] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[36;1mksql-datagen-users_1  |[0m 96 --> ([ 96 | 12 | 1 | 1071 | 1508909155655 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[33mzookeeper_1           |[0m [2017-10-25 05:25:55,659] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x11e zxid:0xb4 txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-offsets/partitions/19 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-offsets/partitions/19 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:55,700] TRACE Controller 1 epoch 1 changed partition [connect-offsets,19] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,700] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-offsets,13] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:55,701] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-offsets,13] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:55,701] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x121 zxid:0xb7 txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-offsets/partitions/13 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-offsets/partitions/13 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:55,736] TRACE Controller 1 epoch 1 changed partition [connect-offsets,13] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,736] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-offsets,10] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,736] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-offsets,4] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,736] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-offsets,15] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,736] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-offsets,12] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,736] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-offsets,1] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,736] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-offsets,23] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,736] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-offsets,9] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,736] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-offsets,20] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,736] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-offsets,14] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,736] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-offsets,6] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,736] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-offsets,17] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,737] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-offsets,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,737] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-offsets,22] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,737] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-offsets,11] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,737] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-offsets,3] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,737] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-offsets,8] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,737] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-offsets,19] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,737] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-offsets,16] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,737] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-offsets,5] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,737] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-offsets,21] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,737] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-offsets,2] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,737] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-offsets,24] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,737] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-offsets,13] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,737] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-offsets,7] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,737] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-offsets,18] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,738] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-offsets-10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,738] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-offsets-4 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,738] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-offsets-15 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,738] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-offsets-12 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,738] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-offsets-1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,738] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-offsets-23 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,738] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-offsets-9 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,739] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-offsets-20 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,739] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-offsets-14 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,739] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-offsets-6 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,739] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-offsets-17 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,739] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-offsets-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,739] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-offsets-22 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,739] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-offsets-11 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,739] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-offsets-3 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,739] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-offsets-8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,739] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-offsets-19 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,739] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-offsets-16 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,740] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-offsets-5 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,740] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-offsets-21 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,740] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-offsets-2 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,740] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-offsets-24 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,740] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-offsets-13 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,740] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-offsets-7 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,740] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-offsets-18 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,741] INFO [Replica state machine on controller 1]: Invoking state change to OnlineReplica for replicas [Topic=connect-offsets,Partition=4,Replica=1],[Topic=connect-offsets,Partition=21,Replica=1],[Topic=connect-offsets,Partition=15,Replica=1],[Topic=connect-offsets,Partition=24,Replica=1],[Topic=connect-offsets,Partition=11,Replica=1],[Topic=connect-offsets,Partition=23,Replica=1],[Topic=connect-offsets,Partition=19,Replica=1],[Topic=connect-offsets,Partition=6,Replica=1],[Topic=connect-offsets,Partition=10,Replica=1],[Topic=connect-offsets,Partition=9,Replica=1],[Topic=connect-offsets,Partition=14,Replica=1],[Topic=connect-offsets,Partition=5,Replica=1],[Topic=connect-offsets,Partition=13,Replica=1],[Topic=connect-offsets,Partition=0,Replica=1],[Topic=connect-offsets,Partition=1,Replica=1],[Topic=connect-offsets,Partition=8,Replica=1],[Topic=connect-offsets,Partition=12,Replica=1],[Topic=connect-offsets,Partition=3,Replica=1],[Topic=connect-offsets,Partition=17,Replica=1],[Topic=connect-offsets,Partition=2,Replica=1],[Topic=connect-offsets,Partition=22,Replica=1],[Topic=connect-offsets,Partition=18,Replica=1],[Topic=connect-offsets,Partition=7,Replica=1],[Topic=connect-offsets,Partition=20,Replica=1],[Topic=connect-offsets,Partition=16,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:55,741] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,4] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,741] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,21] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,741] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,15] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,741] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,24] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,741] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,11] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,742] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,23] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,742] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,19] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,742] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,6] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,742] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,10] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,742] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,9] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,742] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,14] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,742] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,5] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,742] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,13] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,743] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,0] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,743] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,1] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,743] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,8] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,743] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,12] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,743] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,3] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,743] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,17] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,743] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,2] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,743] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,22] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,743] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,18] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,743] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,7] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,743] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,20] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,743] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-offsets,16] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,744] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 7 from controller 1 epoch 1 for partition [connect-offsets,10] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,745] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 7 from controller 1 epoch 1 for partition [connect-offsets,8] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,745] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 7 from controller 1 epoch 1 for partition [connect-offsets,14] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,746] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 7 from controller 1 epoch 1 for partition [connect-offsets,12] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,746] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 7 from controller 1 epoch 1 for partition [connect-offsets,2] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,746] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 7 from controller 1 epoch 1 for partition [connect-offsets,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,746] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 7 from controller 1 epoch 1 for partition [connect-offsets,6] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,746] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 7 from controller 1 epoch 1 for partition [connect-offsets,4] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,746] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 7 from controller 1 epoch 1 for partition [connect-offsets,24] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,746] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 7 from controller 1 epoch 1 for partition [connect-offsets,18] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,746] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 7 from controller 1 epoch 1 for partition [connect-offsets,16] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,746] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 7 from controller 1 epoch 1 for partition [connect-offsets,22] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,746] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 7 from controller 1 epoch 1 for partition [connect-offsets,20] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,746] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 7 from controller 1 epoch 1 for partition [connect-offsets,9] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,746] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 7 from controller 1 epoch 1 for partition [connect-offsets,7] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,747] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 7 from controller 1 epoch 1 for partition [connect-offsets,13] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,747] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 7 from controller 1 epoch 1 for partition [connect-offsets,11] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,747] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 7 from controller 1 epoch 1 for partition [connect-offsets,1] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,747] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 7 from controller 1 epoch 1 for partition [connect-offsets,5] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,748] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 7 from controller 1 epoch 1 for partition [connect-offsets,3] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,748] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 7 from controller 1 epoch 1 for partition [connect-offsets,23] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,748] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 7 from controller 1 epoch 1 for partition [connect-offsets,17] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,748] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 7 from controller 1 epoch 1 for partition [connect-offsets,15] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,748] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 7 from controller 1 epoch 1 for partition [connect-offsets,21] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,748] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 7 from controller 1 epoch 1 for partition [connect-offsets,19] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,764] TRACE Broker 1 handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,765] TRACE Broker 1 handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-5 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,765] TRACE Broker 1 handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-24 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,765] TRACE Broker 1 handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-21 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,765] TRACE Broker 1 handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-2 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,765] TRACE Broker 1 handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-18 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,765] TRACE Broker 1 handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-15 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,765] TRACE Broker 1 handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-12 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,765] TRACE Broker 1 handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-9 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,765] TRACE Broker 1 handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-6 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,765] TRACE Broker 1 handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-3 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,765] TRACE Broker 1 handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-22 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,765] TRACE Broker 1 handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,766] TRACE Broker 1 handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-19 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,766] TRACE Broker 1 handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-16 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,766] TRACE Broker 1 handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-13 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,766] TRACE Broker 1 handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,766] TRACE Broker 1 handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-7 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,766] TRACE Broker 1 handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-17 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,766] TRACE Broker 1 handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-4 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,766] TRACE Broker 1 handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-14 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,766] TRACE Broker 1 handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-23 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,766] TRACE Broker 1 handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-11 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,766] TRACE Broker 1 handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-20 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,766] TRACE Broker 1 handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:55,767] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions connect-offsets-20,connect-offsets-7,connect-offsets-24,connect-offsets-16,connect-offsets-3,connect-offsets-21,connect-offsets-11,connect-offsets-6,connect-offsets-17,connect-offsets-10,connect-offsets-2,connect-offsets-18,connect-offsets-23,connect-offsets-15,connect-offsets-4,connect-offsets-12,connect-offsets-5,connect-offsets-13,connect-offsets-14,connect-offsets-0,connect-offsets-8,connect-offsets-9,connect-offsets-1,connect-offsets-19,connect-offsets-22 (kafka.server.ReplicaFetcherManager)
[35mkafka_1               |[0m [2017-10-25 05:25:55,772] INFO Loading producer state from offset 0 for partition connect-offsets-8 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,773] INFO Completed load of log connect-offsets-8 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,774] INFO Created log for partition [connect-offsets,8] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:55,775] INFO Partition [connect-offsets,8] on broker 1: No checkpointed highwatermark is found for partition connect-offsets-8 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,776] INFO Partition [connect-offsets,8] on broker 1: connect-offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,781] INFO Loading producer state from offset 0 for partition connect-offsets-5 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,781] INFO Completed load of log connect-offsets-5 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,782] INFO Created log for partition [connect-offsets,5] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:55,783] INFO Partition [connect-offsets,5] on broker 1: No checkpointed highwatermark is found for partition connect-offsets-5 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,784] INFO Partition [connect-offsets,5] on broker 1: connect-offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,788] INFO Loading producer state from offset 0 for partition connect-offsets-24 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,788] INFO Completed load of log connect-offsets-24 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,789] INFO Created log for partition [connect-offsets,24] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:55,790] INFO Partition [connect-offsets,24] on broker 1: No checkpointed highwatermark is found for partition connect-offsets-24 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,790] INFO Partition [connect-offsets,24] on broker 1: connect-offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,794] INFO Loading producer state from offset 0 for partition connect-offsets-21 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,795] INFO Completed load of log connect-offsets-21 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,796] INFO Created log for partition [connect-offsets,21] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:55,796] INFO Partition [connect-offsets,21] on broker 1: No checkpointed highwatermark is found for partition connect-offsets-21 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,796] INFO Partition [connect-offsets,21] on broker 1: connect-offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,800] INFO Loading producer state from offset 0 for partition connect-offsets-2 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,801] INFO Completed load of log connect-offsets-2 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,802] INFO Created log for partition [connect-offsets,2] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:55,803] INFO Partition [connect-offsets,2] on broker 1: No checkpointed highwatermark is found for partition connect-offsets-2 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,803] INFO Partition [connect-offsets,2] on broker 1: connect-offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,806] INFO Loading producer state from offset 0 for partition connect-offsets-18 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,806] INFO Completed load of log connect-offsets-18 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,807] INFO Created log for partition [connect-offsets,18] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:55,808] INFO Partition [connect-offsets,18] on broker 1: No checkpointed highwatermark is found for partition connect-offsets-18 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,808] INFO Partition [connect-offsets,18] on broker 1: connect-offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,813] INFO Loading producer state from offset 0 for partition connect-offsets-15 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,813] INFO Completed load of log connect-offsets-15 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,815] INFO Created log for partition [connect-offsets,15] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:55,815] INFO Partition [connect-offsets,15] on broker 1: No checkpointed highwatermark is found for partition connect-offsets-15 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,815] INFO Partition [connect-offsets,15] on broker 1: connect-offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,818] INFO Loading producer state from offset 0 for partition connect-offsets-12 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,819] INFO Completed load of log connect-offsets-12 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,820] INFO Created log for partition [connect-offsets,12] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:55,820] INFO Partition [connect-offsets,12] on broker 1: No checkpointed highwatermark is found for partition connect-offsets-12 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,820] INFO Partition [connect-offsets,12] on broker 1: connect-offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,823] INFO Loading producer state from offset 0 for partition connect-offsets-9 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,824] INFO Completed load of log connect-offsets-9 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,825] INFO Created log for partition [connect-offsets,9] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:55,825] INFO Partition [connect-offsets,9] on broker 1: No checkpointed highwatermark is found for partition connect-offsets-9 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,826] INFO Partition [connect-offsets,9] on broker 1: connect-offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36;1mksql-datagen-users_1  |[0m 97 --> ([ 97 | 11 | 1 | 6130 | 1508909155827 | 'iOS-test' | 'meh' ])
[35mkafka_1               |[0m [2017-10-25 05:25:55,832] INFO Loading producer state from offset 0 for partition connect-offsets-6 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,832] INFO Completed load of log connect-offsets-6 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,833] INFO Created log for partition [connect-offsets,6] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:55,834] INFO Partition [connect-offsets,6] on broker 1: No checkpointed highwatermark is found for partition connect-offsets-6 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,834] INFO Partition [connect-offsets,6] on broker 1: connect-offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,838] INFO Loading producer state from offset 0 for partition connect-offsets-3 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,839] INFO Completed load of log connect-offsets-3 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,839] INFO Created log for partition [connect-offsets,3] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:55,840] INFO Partition [connect-offsets,3] on broker 1: No checkpointed highwatermark is found for partition connect-offsets-3 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,840] INFO Partition [connect-offsets,3] on broker 1: connect-offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,843] INFO Loading producer state from offset 0 for partition connect-offsets-22 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,844] INFO Completed load of log connect-offsets-22 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,845] INFO Created log for partition [connect-offsets,22] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:55,845] INFO Partition [connect-offsets,22] on broker 1: No checkpointed highwatermark is found for partition connect-offsets-22 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,846] INFO Partition [connect-offsets,22] on broker 1: connect-offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,850] INFO Loading producer state from offset 0 for partition connect-offsets-0 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,851] INFO Completed load of log connect-offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,852] INFO Created log for partition [connect-offsets,0] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:55,853] INFO Partition [connect-offsets,0] on broker 1: No checkpointed highwatermark is found for partition connect-offsets-0 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,854] INFO Partition [connect-offsets,0] on broker 1: connect-offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,857] INFO Loading producer state from offset 0 for partition connect-offsets-19 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,858] INFO Completed load of log connect-offsets-19 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,859] INFO Created log for partition [connect-offsets,19] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:55,859] INFO Partition [connect-offsets,19] on broker 1: No checkpointed highwatermark is found for partition connect-offsets-19 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,860] INFO Partition [connect-offsets,19] on broker 1: connect-offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,862] INFO Loading producer state from offset 0 for partition connect-offsets-16 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,863] INFO Completed load of log connect-offsets-16 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,864] INFO Created log for partition [connect-offsets,16] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:55,864] INFO Partition [connect-offsets,16] on broker 1: No checkpointed highwatermark is found for partition connect-offsets-16 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,864] INFO Partition [connect-offsets,16] on broker 1: connect-offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,867] INFO Loading producer state from offset 0 for partition connect-offsets-13 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,868] INFO Completed load of log connect-offsets-13 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,870] INFO Created log for partition [connect-offsets,13] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:55,870] INFO Partition [connect-offsets,13] on broker 1: No checkpointed highwatermark is found for partition connect-offsets-13 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,871] INFO Partition [connect-offsets,13] on broker 1: connect-offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,887] INFO Loading producer state from offset 0 for partition connect-offsets-10 with message format version 2 (kafka.log.Log)
[36;1mksql-datagen-users_1  |[0m 98 --> ([ 98 | 3 | 2 | 1498 | 1508909155887 | 'android' | '(expletive deleted)' ])
[35mkafka_1               |[0m [2017-10-25 05:25:55,894] INFO Completed load of log connect-offsets-10 with 1 log segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,896] INFO Created log for partition [connect-offsets,10] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:55,905] INFO Partition [connect-offsets,10] on broker 1: No checkpointed highwatermark is found for partition connect-offsets-10 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,906] INFO Partition [connect-offsets,10] on broker 1: connect-offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,919] INFO Loading producer state from offset 0 for partition connect-offsets-7 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,919] INFO Completed load of log connect-offsets-7 with 1 log segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,921] INFO Created log for partition [connect-offsets,7] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:55,921] INFO Partition [connect-offsets,7] on broker 1: No checkpointed highwatermark is found for partition connect-offsets-7 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,921] INFO Partition [connect-offsets,7] on broker 1: connect-offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,936] INFO Loading producer state from offset 0 for partition connect-offsets-17 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,936] INFO Completed load of log connect-offsets-17 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,938] INFO Created log for partition [connect-offsets,17] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:55,938] INFO Partition [connect-offsets,17] on broker 1: No checkpointed highwatermark is found for partition connect-offsets-17 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,939] INFO Partition [connect-offsets,17] on broker 1: connect-offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,946] INFO Loading producer state from offset 0 for partition connect-offsets-4 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,947] INFO Completed load of log connect-offsets-4 with 1 log segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,950] INFO Created log for partition [connect-offsets,4] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:55,950] INFO Partition [connect-offsets,4] on broker 1: No checkpointed highwatermark is found for partition connect-offsets-4 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,950] INFO Partition [connect-offsets,4] on broker 1: connect-offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,975] INFO Loading producer state from offset 0 for partition connect-offsets-14 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,975] INFO Completed load of log connect-offsets-14 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,976] INFO Created log for partition [connect-offsets,14] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:55,977] INFO Partition [connect-offsets,14] on broker 1: No checkpointed highwatermark is found for partition connect-offsets-14 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,977] INFO Partition [connect-offsets,14] on broker 1: connect-offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,984] INFO Loading producer state from offset 0 for partition connect-offsets-23 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,984] INFO Completed load of log connect-offsets-23 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,985] INFO Created log for partition [connect-offsets,23] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:55,985] INFO Partition [connect-offsets,23] on broker 1: No checkpointed highwatermark is found for partition connect-offsets-23 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,986] INFO Partition [connect-offsets,23] on broker 1: connect-offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:55,996] INFO Loading producer state from offset 0 for partition connect-offsets-11 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,998] INFO Completed load of log connect-offsets-11 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:55,999] INFO Created log for partition [connect-offsets,11] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:56,000] INFO Partition [connect-offsets,11] on broker 1: No checkpointed highwatermark is found for partition connect-offsets-11 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:56,000] INFO Partition [connect-offsets,11] on broker 1: connect-offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:56,007] INFO Loading producer state from offset 0 for partition connect-offsets-20 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:56,007] INFO Completed load of log connect-offsets-20 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:56,008] INFO Created log for partition [connect-offsets,20] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:56,009] INFO Partition [connect-offsets,20] on broker 1: No checkpointed highwatermark is found for partition connect-offsets-20 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:56,010] INFO Partition [connect-offsets,20] on broker 1: connect-offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:56,016] INFO Loading producer state from offset 0 for partition connect-offsets-1 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:56,016] INFO Completed load of log connect-offsets-1 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:56,017] INFO Created log for partition [connect-offsets,1] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[36;1mksql-datagen-users_1  |[0m 99 --> ([ 99 | 12 | 2 | 6116 | 1508909156018 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[35mkafka_1               |[0m [2017-10-25 05:25:56,019] INFO Partition [connect-offsets,1] on broker 1: No checkpointed highwatermark is found for partition connect-offsets-1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:56,019] INFO Partition [connect-offsets,1] on broker 1: connect-offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:56,019] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition connect-offsets-1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,019] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition connect-offsets-4 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,020] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition connect-offsets-7 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,020] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition connect-offsets-10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,021] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition connect-offsets-13 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,021] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition connect-offsets-16 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,021] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition connect-offsets-19 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,021] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition connect-offsets-22 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,021] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition connect-offsets-2 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,021] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition connect-offsets-5 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,021] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition connect-offsets-8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,021] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition connect-offsets-11 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,021] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition connect-offsets-14 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,021] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition connect-offsets-17 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,021] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition connect-offsets-20 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,021] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition connect-offsets-23 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,021] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition connect-offsets-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,021] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition connect-offsets-3 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,022] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition connect-offsets-6 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,022] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition connect-offsets-9 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,022] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition connect-offsets-12 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,022] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition connect-offsets-15 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,022] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition connect-offsets-18 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,022] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition connect-offsets-21 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,022] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition connect-offsets-24 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,022] TRACE Broker 1 completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,023] TRACE Broker 1 completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-5 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,023] TRACE Broker 1 completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-24 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,023] TRACE Broker 1 completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-21 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,023] TRACE Broker 1 completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-2 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,023] TRACE Broker 1 completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-18 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,024] TRACE Broker 1 completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-15 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,024] TRACE Broker 1 completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-12 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,024] TRACE Broker 1 completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-9 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,024] TRACE Broker 1 completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-6 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,024] TRACE Broker 1 completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-3 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,024] TRACE Broker 1 completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-22 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,024] TRACE Broker 1 completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,024] TRACE Broker 1 completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-19 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,024] TRACE Broker 1 completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-16 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,024] TRACE Broker 1 completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-13 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,024] TRACE Broker 1 completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,024] TRACE Broker 1 completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-7 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,025] TRACE Broker 1 completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-17 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,025] TRACE Broker 1 completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-4 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,025] TRACE Broker 1 completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-14 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,025] TRACE Broker 1 completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-23 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,025] TRACE Broker 1 completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-11 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,025] TRACE Broker 1 completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-20 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,025] TRACE Broker 1 completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,026] TRACE Controller 1 epoch 1 received response {error_code=0,partitions=[{topic=connect-offsets,partition=10,error_code=0},{topic=connect-offsets,partition=8,error_code=0},{topic=connect-offsets,partition=14,error_code=0},{topic=connect-offsets,partition=12,error_code=0},{topic=connect-offsets,partition=2,error_code=0},{topic=connect-offsets,partition=0,error_code=0},{topic=connect-offsets,partition=6,error_code=0},{topic=connect-offsets,partition=4,error_code=0},{topic=connect-offsets,partition=24,error_code=0},{topic=connect-offsets,partition=18,error_code=0},{topic=connect-offsets,partition=16,error_code=0},{topic=connect-offsets,partition=22,error_code=0},{topic=connect-offsets,partition=20,error_code=0},{topic=connect-offsets,partition=9,error_code=0},{topic=connect-offsets,partition=7,error_code=0},{topic=connect-offsets,partition=13,error_code=0},{topic=connect-offsets,partition=11,error_code=0},{topic=connect-offsets,partition=1,error_code=0},{topic=connect-offsets,partition=5,error_code=0},{topic=connect-offsets,partition=3,error_code=0},{topic=connect-offsets,partition=23,error_code=0},{topic=connect-offsets,partition=17,error_code=0},{topic=connect-offsets,partition=15,error_code=0},{topic=connect-offsets,partition=21,error_code=0},{topic=connect-offsets,partition=19,error_code=0}]} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,028] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-offsets-10 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,029] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-offsets-8 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,029] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-offsets-14 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,029] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-offsets-12 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,029] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-offsets-2 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,030] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-offsets-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,030] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-offsets-6 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,030] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-offsets-4 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,031] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-offsets-24 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,031] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-offsets-18 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,031] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-offsets-16 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,031] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-offsets-22 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,032] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-offsets-20 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,032] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-offsets-9 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,032] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-offsets-7 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,032] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-offsets-13 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,033] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-offsets-11 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,033] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-offsets-1 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,033] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-offsets-5 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,034] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-offsets-3 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,034] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-offsets-23 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,034] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-offsets-17 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,035] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-offsets-15 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,036] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-offsets-21 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,036] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-offsets-19 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,038] TRACE Controller 1 epoch 1 received response {error_code=0} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,043] INFO Created topic (name=connect-offsets, numPartitions=25, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at kafka:29092 (org.apache.kafka.connect.util.TopicAdmin)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,057] INFO ProducerConfig values: 
[33;1mconnect_1             |[0m 	acks = all
[33;1mconnect_1             |[0m 	batch.size = 16384
[33;1mconnect_1             |[0m 	bootstrap.servers = [kafka:29092]
[33;1mconnect_1             |[0m 	buffer.memory = 33554432
[33;1mconnect_1             |[0m 	client.id = 
[33;1mconnect_1             |[0m 	compression.type = none
[33;1mconnect_1             |[0m 	connections.max.idle.ms = 540000
[33;1mconnect_1             |[0m 	enable.idempotence = false
[33;1mconnect_1             |[0m 	interceptor.classes = null
[33;1mconnect_1             |[0m 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[33;1mconnect_1             |[0m 	linger.ms = 0
[33;1mconnect_1             |[0m 	max.block.ms = 60000
[33;1mconnect_1             |[0m 	max.in.flight.requests.per.connection = 1
[33;1mconnect_1             |[0m 	max.request.size = 1048576
[33;1mconnect_1             |[0m 	metadata.max.age.ms = 300000
[33;1mconnect_1             |[0m 	metric.reporters = []
[33;1mconnect_1             |[0m 	metrics.num.samples = 2
[33;1mconnect_1             |[0m 	metrics.recording.level = INFO
[33;1mconnect_1             |[0m 	metrics.sample.window.ms = 30000
[33;1mconnect_1             |[0m 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
[33;1mconnect_1             |[0m 	receive.buffer.bytes = 32768
[33;1mconnect_1             |[0m 	reconnect.backoff.max.ms = 1000
[33;1mconnect_1             |[0m 	reconnect.backoff.ms = 50
[33;1mconnect_1             |[0m 	request.timeout.ms = 30000
[33;1mconnect_1             |[0m 	retries = 2147483647
[33;1mconnect_1             |[0m 	retry.backoff.ms = 100
[33;1mconnect_1             |[0m 	sasl.jaas.config = null
[33;1mconnect_1             |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[33;1mconnect_1             |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[33;1mconnect_1             |[0m 	sasl.kerberos.service.name = null
[33;1mconnect_1             |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[33;1mconnect_1             |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[33;1mconnect_1             |[0m 	sasl.mechanism = GSSAPI
[33;1mconnect_1             |[0m 	security.protocol = PLAINTEXT
[33;1mconnect_1             |[0m 	send.buffer.bytes = 131072
[33;1mconnect_1             |[0m 	ssl.cipher.suites = null
[33;1mconnect_1             |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[33;1mconnect_1             |[0m 	ssl.endpoint.identification.algorithm = null
[33;1mconnect_1             |[0m 	ssl.key.password = null
[33;1mconnect_1             |[0m 	ssl.keymanager.algorithm = SunX509
[33;1mconnect_1             |[0m 	ssl.keystore.location = null
[33;1mconnect_1             |[0m 	ssl.keystore.password = null
[33;1mconnect_1             |[0m 	ssl.keystore.type = JKS
[33;1mconnect_1             |[0m 	ssl.protocol = TLS
[33;1mconnect_1             |[0m 	ssl.provider = null
[33;1mconnect_1             |[0m 	ssl.secure.random.implementation = null
[33;1mconnect_1             |[0m 	ssl.trustmanager.algorithm = PKIX
[33;1mconnect_1             |[0m 	ssl.truststore.location = null
[33;1mconnect_1             |[0m 	ssl.truststore.password = null
[33;1mconnect_1             |[0m 	ssl.truststore.type = JKS
[33;1mconnect_1             |[0m 	transaction.timeout.ms = 60000
[33;1mconnect_1             |[0m 	transactional.id = null
[33;1mconnect_1             |[0m 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[33;1mconnect_1             |[0m  (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,079] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,079] WARN The configuration 'replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,079] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,079] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,079] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,080] WARN The configuration 'internal.key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,080] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,080] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,080] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,080] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,080] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,080] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,080] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,081] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,081] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,081] WARN The configuration 'internal.key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,081] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,082] WARN The configuration 'internal.value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,082] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,082] WARN The configuration 'internal.value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,082] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,082] WARN The configuration 'log4j.root.loglevel' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,082] INFO Kafka version : 0.11.0.0-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,089] INFO Kafka commitId : 6a8cf706ddc9ab6a (org.apache.kafka.common.utils.AppInfoParser)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,099] INFO ConsumerConfig values: 
[33;1mconnect_1             |[0m 	auto.commit.interval.ms = 5000
[33;1mconnect_1             |[0m 	auto.offset.reset = earliest
[33;1mconnect_1             |[0m 	bootstrap.servers = [kafka:29092]
[33;1mconnect_1             |[0m 	check.crcs = true
[33;1mconnect_1             |[0m 	client.id = 
[33;1mconnect_1             |[0m 	connections.max.idle.ms = 540000
[33;1mconnect_1             |[0m 	enable.auto.commit = false
[33;1mconnect_1             |[0m 	exclude.internal.topics = true
[33;1mconnect_1             |[0m 	fetch.max.bytes = 52428800
[33;1mconnect_1             |[0m 	fetch.max.wait.ms = 500
[33;1mconnect_1             |[0m 	fetch.min.bytes = 1
[33;1mconnect_1             |[0m 	group.id = connect
[33;1mconnect_1             |[0m 	heartbeat.interval.ms = 3000
[33;1mconnect_1             |[0m 	interceptor.classes = null
[33;1mconnect_1             |[0m 	internal.leave.group.on.close = true
[33;1mconnect_1             |[0m 	isolation.level = read_uncommitted
[33;1mconnect_1             |[0m 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[33;1mconnect_1             |[0m 	max.partition.fetch.bytes = 1048576
[33;1mconnect_1             |[0m 	max.poll.interval.ms = 300000
[33;1mconnect_1             |[0m 	max.poll.records = 500
[33;1mconnect_1             |[0m 	metadata.max.age.ms = 300000
[33;1mconnect_1             |[0m 	metric.reporters = []
[33;1mconnect_1             |[0m 	metrics.num.samples = 2
[33;1mconnect_1             |[0m 	metrics.recording.level = INFO
[33;1mconnect_1             |[0m 	metrics.sample.window.ms = 30000
[33;1mconnect_1             |[0m 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
[33;1mconnect_1             |[0m 	receive.buffer.bytes = 65536
[33;1mconnect_1             |[0m 	reconnect.backoff.max.ms = 1000
[33;1mconnect_1             |[0m 	reconnect.backoff.ms = 50
[33;1mconnect_1             |[0m 	request.timeout.ms = 305000
[33;1mconnect_1             |[0m 	retry.backoff.ms = 100
[33;1mconnect_1             |[0m 	sasl.jaas.config = null
[33;1mconnect_1             |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[33;1mconnect_1             |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[33;1mconnect_1             |[0m 	sasl.kerberos.service.name = null
[33;1mconnect_1             |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[33;1mconnect_1             |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[33;1mconnect_1             |[0m 	sasl.mechanism = GSSAPI
[33;1mconnect_1             |[0m 	security.protocol = PLAINTEXT
[33;1mconnect_1             |[0m 	send.buffer.bytes = 131072
[33;1mconnect_1             |[0m 	session.timeout.ms = 10000
[33;1mconnect_1             |[0m 	ssl.cipher.suites = null
[33;1mconnect_1             |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[33;1mconnect_1             |[0m 	ssl.endpoint.identification.algorithm = null
[33;1mconnect_1             |[0m 	ssl.key.password = null
[33;1mconnect_1             |[0m 	ssl.keymanager.algorithm = SunX509
[33;1mconnect_1             |[0m 	ssl.keystore.location = null
[33;1mconnect_1             |[0m 	ssl.keystore.password = null
[33;1mconnect_1             |[0m 	ssl.keystore.type = JKS
[33;1mconnect_1             |[0m 	ssl.protocol = TLS
[33;1mconnect_1             |[0m 	ssl.provider = null
[33;1mconnect_1             |[0m 	ssl.secure.random.implementation = null
[33;1mconnect_1             |[0m 	ssl.trustmanager.algorithm = PKIX
[33;1mconnect_1             |[0m 	ssl.truststore.location = null
[33;1mconnect_1             |[0m 	ssl.truststore.password = null
[33;1mconnect_1             |[0m 	ssl.truststore.type = JKS
[33;1mconnect_1             |[0m 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[33;1mconnect_1             |[0m  (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,126] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,126] WARN The configuration 'replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,126] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,126] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,127] WARN The configuration 'internal.key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,127] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,127] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,127] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,127] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,128] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,128] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,128] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,128] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,129] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,129] WARN The configuration 'internal.key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,130] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,130] WARN The configuration 'internal.value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,130] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,130] WARN The configuration 'internal.value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,130] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,131] WARN The configuration 'log4j.root.loglevel' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,131] INFO Kafka version : 0.11.0.0-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[33;1mconnect_1             |[0m [2017-10-25 05:25:56,131] INFO Kafka commitId : 6a8cf706ddc9ab6a (org.apache.kafka.common.utils.AppInfoParser)
[36;1mksql-datagen-users_1  |[0m 100 --> ([ 100 | 16 | 3 | 5510 | 1508909156174 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[33mzookeeper_1           |[0m [2017-10-25 05:25:56,203] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:setData cxid:0x141 zxid:0xba txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:25:56,216] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x142 zxid:0xbb txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:56,238] INFO Topic creation {"version":1,"partitions":{"45":[1],"34":[1],"12":[1],"8":[1],"19":[1],"23":[1],"4":[1],"40":[1],"15":[1],"11":[1],"9":[1],"44":[1],"33":[1],"22":[1],"26":[1],"37":[1],"13":[1],"46":[1],"24":[1],"35":[1],"16":[1],"5":[1],"10":[1],"48":[1],"21":[1],"43":[1],"32":[1],"49":[1],"6":[1],"36":[1],"1":[1],"39":[1],"17":[1],"25":[1],"14":[1],"47":[1],"31":[1],"42":[1],"0":[1],"20":[1],"27":[1],"2":[1],"38":[1],"18":[1],"30":[1],"7":[1],"29":[1],"41":[1],"3":[1],"28":[1]}} (kafka.admin.AdminUtils$)
[35mkafka_1               |[0m [2017-10-25 05:25:56,251] INFO [KafkaApi-1] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[35mkafka_1               |[0m [2017-10-25 05:25:56,261] INFO [Controller 1]: New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map([__consumer_offsets,19] -> List(1), [__consumer_offsets,30] -> List(1), [__consumer_offsets,47] -> List(1), [__consumer_offsets,29] -> List(1), [__consumer_offsets,41] -> List(1), [__consumer_offsets,39] -> List(1), [__consumer_offsets,10] -> List(1), [__consumer_offsets,17] -> List(1), [__consumer_offsets,14] -> List(1), [__consumer_offsets,40] -> List(1), [__consumer_offsets,18] -> List(1), [__consumer_offsets,26] -> List(1), [__consumer_offsets,0] -> List(1), [__consumer_offsets,24] -> List(1), [__consumer_offsets,33] -> List(1), [__consumer_offsets,20] -> List(1), [__consumer_offsets,21] -> List(1), [__consumer_offsets,3] -> List(1), [__consumer_offsets,5] -> List(1), [__consumer_offsets,22] -> List(1), [__consumer_offsets,12] -> List(1), [__consumer_offsets,8] -> List(1), [__consumer_offsets,23] -> List(1), [__consumer_offsets,15] -> List(1), [__consumer_offsets,48] -> List(1), [__consumer_offsets,11] -> List(1), [__consumer_offsets,13] -> List(1), [__consumer_offsets,49] -> List(1), [__consumer_offsets,6] -> List(1), [__consumer_offsets,28] -> List(1), [__consumer_offsets,4] -> List(1), [__consumer_offsets,37] -> List(1), [__consumer_offsets,31] -> List(1), [__consumer_offsets,44] -> List(1), [__consumer_offsets,42] -> List(1), [__consumer_offsets,34] -> List(1), [__consumer_offsets,46] -> List(1), [__consumer_offsets,25] -> List(1), [__consumer_offsets,45] -> List(1), [__consumer_offsets,27] -> List(1), [__consumer_offsets,32] -> List(1), [__consumer_offsets,43] -> List(1), [__consumer_offsets,36] -> List(1), [__consumer_offsets,35] -> List(1), [__consumer_offsets,7] -> List(1), [__consumer_offsets,9] -> List(1), [__consumer_offsets,38] -> List(1), [__consumer_offsets,1] -> List(1), [__consumer_offsets,16] -> List(1), [__consumer_offsets,2] -> List(1))] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:56,269] INFO [Controller 1]: New topic creation callback for [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:56,271] INFO [Controller 1]: New partition creation callback for [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:56,273] INFO [Partition state machine on Controller 1]: Invoking state change to NewPartition for partitions [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:56,274] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,19] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,274] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,30] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,275] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,47] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,276] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,29] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,277] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,41] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,278] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,39] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,278] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,10] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,279] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,17] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,280] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,14] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,280] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,40] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,280] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,18] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,281] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,26] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,281] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,0] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,281] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,24] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,282] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,33] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,282] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,20] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,282] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,21] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,282] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,3] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,282] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,5] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,283] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,22] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,283] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,12] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,283] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,8] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,283] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,23] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,283] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,15] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,284] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,48] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,284] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,11] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,285] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,13] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,285] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,49] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,285] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,6] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,285] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,28] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,285] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,4] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,285] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,37] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,285] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,31] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,287] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,44] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,288] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,42] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,288] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,34] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,288] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,46] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,288] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,25] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,289] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,45] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,289] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,27] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,289] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,32] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,289] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,43] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,289] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,36] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,289] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,35] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,291] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,7] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,291] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,9] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,291] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,38] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,291] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,1] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,291] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,16] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,291] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,2] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,308] INFO [Replica state machine on controller 1]: Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=28,Replica=1],[Topic=__consumer_offsets,Partition=48,Replica=1],[Topic=__consumer_offsets,Partition=5,Replica=1],[Topic=__consumer_offsets,Partition=21,Replica=1],[Topic=__consumer_offsets,Partition=2,Replica=1],[Topic=__consumer_offsets,Partition=18,Replica=1],[Topic=__consumer_offsets,Partition=23,Replica=1],[Topic=__consumer_offsets,Partition=9,Replica=1],[Topic=__consumer_offsets,Partition=39,Replica=1],[Topic=__consumer_offsets,Partition=31,Replica=1],[Topic=__consumer_offsets,Partition=19,Replica=1],[Topic=__consumer_offsets,Partition=10,Replica=1],[Topic=__consumer_offsets,Partition=22,Replica=1],[Topic=__consumer_offsets,Partition=43,Replica=1],[Topic=__consumer_offsets,Partition=40,Replica=1],[Topic=__consumer_offsets,Partition=27,Replica=1],[Topic=__consumer_offsets,Partition=6,Replica=1],[Topic=__consumer_offsets,Partition=1,Replica=1],[Topic=__consumer_offsets,Partition=47,Replica=1],[Topic=__consumer_offsets,Partition=30,Replica=1],[Topic=__consumer_offsets,Partition=42,Replica=1],[Topic=__consumer_offsets,Partition=41,Replica=1],[Topic=__consumer_offsets,Partition=3,Replica=1],[Topic=__consumer_offsets,Partition=13,Replica=1],[Topic=__consumer_offsets,Partition=4,Replica=1],[Topic=__consumer_offsets,Partition=16,Replica=1],[Topic=__consumer_offsets,Partition=46,Replica=1],[Topic=__consumer_offsets,Partition=49,Replica=1],[Topic=__consumer_offsets,Partition=14,Replica=1],[Topic=__consumer_offsets,Partition=45,Replica=1],[Topic=__consumer_offsets,Partition=37,Replica=1],[Topic=__consumer_offsets,Partition=29,Replica=1],[Topic=__consumer_offsets,Partition=20,Replica=1],[Topic=__consumer_offsets,Partition=8,Replica=1],[Topic=__consumer_offsets,Partition=38,Replica=1],[Topic=__consumer_offsets,Partition=7,Replica=1],[Topic=__consumer_offsets,Partition=0,Replica=1],[Topic=__consumer_offsets,Partition=34,Replica=1],[Topic=__consumer_offsets,Partition=33,Replica=1],[Topic=__consumer_offsets,Partition=26,Replica=1],[Topic=__consumer_offsets,Partition=44,Replica=1],[Topic=__consumer_offsets,Partition=32,Replica=1],[Topic=__consumer_offsets,Partition=25,Replica=1],[Topic=__consumer_offsets,Partition=11,Replica=1],[Topic=__consumer_offsets,Partition=36,Replica=1],[Topic=__consumer_offsets,Partition=12,Replica=1],[Topic=__consumer_offsets,Partition=35,Replica=1],[Topic=__consumer_offsets,Partition=15,Replica=1],[Topic=__consumer_offsets,Partition=17,Replica=1],[Topic=__consumer_offsets,Partition=24,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:56,309] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,28] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,310] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,48] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,311] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,5] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,312] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,21] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,313] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,2] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,317] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,18] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,318] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,23] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,319] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,9] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,320] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,39] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,320] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,31] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,321] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,19] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,322] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,10] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,323] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,22] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,323] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,43] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,324] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,40] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,324] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,27] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,325] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,6] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,326] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,1] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,326] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,47] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,327] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,30] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,328] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,42] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,329] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,41] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,330] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,3] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,331] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,13] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,331] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,4] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,332] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,16] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,333] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,46] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,334] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,49] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,334] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,14] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,336] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,45] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,337] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,37] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,337] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,29] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,338] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,20] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,338] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,8] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,339] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,38] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,340] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,7] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,341] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,0] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,342] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,34] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,343] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,33] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,345] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,26] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,346] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,44] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,348] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,32] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,350] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,25] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,350] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,11] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,351] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,36] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,352] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,12] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,353] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,35] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,353] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,15] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,354] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,17] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,355] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,24] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,355] INFO [Partition state machine on Controller 1]: Invoking state change to OnlinePartition for partitions [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:56,355] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,19] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:56,355] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,19] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:56,356] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x17e zxid:0xbe txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/19 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/19 (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:25:56,372] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x17f zxid:0xbf txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:56,433] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,19] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,433] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,30] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:56,434] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,30] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:56,435] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x186 zxid:0xc3 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/30 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/30 (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 101 --> ([ 101 | 15 | 2 | 5238 | 1508909156448 | 'iOS-test' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 102 --> ([ 102 | 5 | 4 | 2102 | 1508909156471 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[35mkafka_1               |[0m [2017-10-25 05:25:56,476] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,30] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,476] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,47] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:56,476] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,47] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:56,477] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x189 zxid:0xc6 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/47 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/47 (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 103 --> ([ 103 | 6 | 4 | 8390 | 1508909156488 | 'iOS' | 'worst. flight. ever. #neveragain' ])
[35mkafka_1               |[0m [2017-10-25 05:25:56,513] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,47] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,514] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,29] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:56,514] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,29] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:56,516] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x18e zxid:0xc9 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/29 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/29 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:56,551] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,29] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,551] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,41] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:56,551] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,41] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:56,559] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x192 zxid:0xcc txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/41 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/41 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:56,606] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,41] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,606] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,39] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:56,606] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,39] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:56,607] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x197 zxid:0xcf txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/39 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/39 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:56,651] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,39] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,652] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,10] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:56,652] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,10] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:56,653] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x19b zxid:0xd2 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/10 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/10 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:56,691] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,10] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,691] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,17] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:56,691] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,17] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:56,692] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x19e zxid:0xd5 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/17 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/17 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:56,732] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,17] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,733] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,14] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:56,733] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,14] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:56,734] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x1a4 zxid:0xd8 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/14 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/14 (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 104 --> ([ 104 | 9 | 2 | 3722 | 1508909156748 | 'android' | 'more peanuts please' ])
[35mkafka_1               |[0m [2017-10-25 05:25:56,765] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,14] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,765] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,40] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:56,766] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,40] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:56,766] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x1a7 zxid:0xdb txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/40 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/40 (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 105 --> ([ 105 | 0 | 4 | 2335 | 1508909156767 | 'android' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 106 --> ([ 106 | 18 | 1 | 1143 | 1508909156797 | 'web' | 'your team here rocks!' ])
[35mkafka_1               |[0m [2017-10-25 05:25:56,800] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,40] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,801] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,18] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:56,801] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,18] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:56,801] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x1aa zxid:0xde txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/18 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/18 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:56,832] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,18] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,833] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,26] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:56,833] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,26] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:56,834] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x1b0 zxid:0xe1 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/26 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/26 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:56,868] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,26] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,868] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,0] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:56,869] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,0] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:56,870] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x1b3 zxid:0xe4 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0 (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 107 --> ([ 107 | 14 | 2 | 5235 | 1508909156896 | 'ios' | 'meh' ])
[35mkafka_1               |[0m [2017-10-25 05:25:56,902] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,0] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,902] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,24] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:56,903] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,24] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:56,904] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x1b6 zxid:0xe7 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/24 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/24 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:56,944] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,24] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,944] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,33] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:56,944] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,33] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:56,945] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x1bc zxid:0xea txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/33 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/33 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:56,983] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,33] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:56,983] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,20] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:56,983] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,20] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:56,984] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x1bf zxid:0xed txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/20 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/20 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:57,036] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,20] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:57,037] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,21] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:57,038] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,21] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:57,041] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x1c4 zxid:0xf0 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/21 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/21 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:57,078] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,21] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:57,079] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,3] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:57,079] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,3] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:57,080] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x1c8 zxid:0xf3 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/3 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/3 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:57,112] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,3] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:57,112] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,5] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:57,113] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,5] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:57,116] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x1cb zxid:0xf6 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/5 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/5 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:57,160] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,5] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:57,161] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,22] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:57,161] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,22] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:57,162] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x1d1 zxid:0xf9 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/22 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/22 (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 108 --> ([ 108 | 9 | 4 | 3110 | 1508909157176 | 'android' | 'is this as good as it gets? really ?' ])
[35mkafka_1               |[0m [2017-10-25 05:25:57,200] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,22] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:57,200] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,12] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:57,201] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,12] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:57,202] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x1d4 zxid:0xfc txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/12 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/12 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:57,241] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,12] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:57,242] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,8] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:57,242] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,8] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:57,246] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x1d9 zxid:0xff txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/8 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/8 (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 109 --> ([ 109 | 6 | 4 | 3431 | 1508909157270 | 'android' | 'more peanuts please' ])
[35mkafka_1               |[0m [2017-10-25 05:25:57,302] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,8] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:57,304] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,23] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:57,304] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,23] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:57,305] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x1dd zxid:0x102 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/23 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/23 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:57,363] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,23] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:57,363] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,15] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:57,364] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,15] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:57,365] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x1e3 zxid:0x105 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/15 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/15 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:57,400] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,15] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:57,400] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,48] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:57,400] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,48] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:57,401] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x1e6 zxid:0x108 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/48 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/48 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:57,437] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,48] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:57,437] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,11] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:57,437] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,11] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:57,438] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x1ea zxid:0x10b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/11 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/11 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:57,510] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,11] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:57,511] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,13] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:57,511] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,13] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:57,513] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x1ef zxid:0x10e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/13 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/13 (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 110 --> ([ 110 | -1 | 1 | 8796 | 1508909157523 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 111 --> ([ 111 | 17 | 4 | 9691 | 1508909157534 | 'web' | '(expletive deleted)' ])
[35mkafka_1               |[0m [2017-10-25 05:25:57,555] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,13] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:57,556] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,49] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:57,556] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,49] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:57,558] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x1f5 zxid:0x111 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/49 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/49 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:57,598] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,49] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:57,598] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,6] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:57,598] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,6] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:57,599] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x1f8 zxid:0x114 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/6 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/6 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:57,634] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,6] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:57,634] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,28] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:57,634] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,28] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:57,634] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x1fc zxid:0x117 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/28 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/28 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:57,677] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,28] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:57,677] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,4] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:57,677] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,4] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:57,678] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x201 zxid:0x11a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/4 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/4 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:57,711] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,4] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:57,711] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,37] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:57,711] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,37] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:57,712] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x204 zxid:0x11d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/37 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/37 (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 112 --> ([ 112 | 3 | 1 | 3551 | 1508909157714 | 'iOS-test' | 'meh' ])
[35mkafka_1               |[0m [2017-10-25 05:25:57,746] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,37] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:57,747] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,31] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:57,747] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,31] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:57,748] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x209 zxid:0x120 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/31 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/31 (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 113 --> ([ 113 | -1 | 2 | 9718 | 1508909157758 | 'ios' | 'is this as good as it gets? really ?' ])
[35mkafka_1               |[0m [2017-10-25 05:25:57,781] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,31] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:57,781] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,44] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:57,781] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,44] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:57,782] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x20d zxid:0x123 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/44 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/44 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:57,816] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,44] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:57,816] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,42] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:57,816] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,42] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:57,817] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x210 zxid:0x126 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/42 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/42 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:57,861] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,42] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:57,862] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,34] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:57,862] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,34] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:57,863] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x215 zxid:0x129 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/34 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/34 (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 114 --> ([ 114 | 6 | 4 | 964 | 1508909157867 | 'web' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[35mkafka_1               |[0m [2017-10-25 05:25:57,906] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,34] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:57,907] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,46] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:57,908] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,46] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:57,909] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x219 zxid:0x12c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/46 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/46 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:57,963] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,46] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:57,964] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,25] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:57,964] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,25] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:57,965] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x21e zxid:0x12f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/25 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/25 (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 115 --> ([ 115 | 9 | 2 | 6153 | 1508909157988 | 'ios' | 'meh' ])
[35mkafka_1               |[0m [2017-10-25 05:25:58,004] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,25] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,005] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,45] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:58,005] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,45] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:58,006] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x222 zxid:0x132 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/45 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/45 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:58,044] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,45] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,044] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,27] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:58,044] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,27] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:58,045] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x225 zxid:0x135 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/27 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/27 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:58,083] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,27] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,083] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,32] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:58,083] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,32] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:58,086] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x22b zxid:0x138 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/32 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/32 (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 116 --> ([ 116 | 11 | 1 | 6752 | 1508909158115 | 'ios' | 'your team here rocks!' ])
[35mkafka_1               |[0m [2017-10-25 05:25:58,119] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,32] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,120] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,43] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:58,120] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,43] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:58,121] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x22e zxid:0x13b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/43 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/43 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:58,165] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,43] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,165] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,36] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:58,165] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,36] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[36;1mksql-datagen-users_1  |[0m 117 --> ([ 117 | 11 | 1 | 1835 | 1508909158167 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[33mzookeeper_1           |[0m [2017-10-25 05:25:58,173] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x234 zxid:0x13e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/36 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/36 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:58,218] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,36] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,219] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,35] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:58,219] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,35] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:58,220] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x237 zxid:0x141 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/35 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/35 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:58,255] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,35] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,255] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,7] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:58,255] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,7] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:58,256] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x23c zxid:0x144 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/7 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/7 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:58,292] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,7] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,293] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,9] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:58,293] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,9] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:58,294] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x240 zxid:0x147 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/9 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/9 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:58,330] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,9] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,330] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,38] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:58,330] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,38] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:58,331] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x243 zxid:0x14a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/38 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/38 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:58,382] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,38] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,383] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,1] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:58,383] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,1] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:58,384] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x248 zxid:0x14d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/1 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/1 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:58,419] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,1] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,420] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,16] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:58,420] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,16] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:58,421] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x24c zxid:0x150 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/16 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/16 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:58,459] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,16] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,460] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [__consumer_offsets,2] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:58,460] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [__consumer_offsets,2] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[36;1mksql-datagen-users_1  |[0m 118 --> ([ 118 | 9 | 2 | 2474 | 1508909158461 | 'iOS-test' | 'your team here rocks!' ])
[33mzookeeper_1           |[0m [2017-10-25 05:25:58,462] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x250 zxid:0x153 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/2 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/2 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:58,498] TRACE Controller 1 epoch 1 changed partition [__consumer_offsets,2] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,498] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,49] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,499] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,38] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,499] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,27] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,499] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,16] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,500] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,8] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,500] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,19] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,502] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,2] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,503] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,13] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,503] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,24] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,503] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,46] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,503] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,35] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,504] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,5] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,504] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,43] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,504] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,21] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,505] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,32] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,505] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,10] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,505] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,37] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,505] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,48] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,506] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,29] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,506] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,40] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,506] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,18] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,506] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,7] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,507] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,23] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,507] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,34] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,507] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,45] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,507] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,26] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,508] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,15] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,508] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,4] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,508] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,42] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,508] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,20] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,509] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,31] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,509] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,9] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,509] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,12] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,509] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,1] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,509] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,17] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,510] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,28] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,510] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,6] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,510] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,39] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,510] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,44] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,510] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,47] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,511] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,36] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,511] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,3] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,511] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,14] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,511] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,25] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,511] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,30] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,512] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,41] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,512] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,22] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,512] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,33] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,513] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,11] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,513] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [__consumer_offsets,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,513] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-49 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,513] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-38 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,514] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-27 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,514] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-16 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,514] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-19 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,514] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,515] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-2 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,515] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-13 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,515] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-24 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,515] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-46 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,515] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-35 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,515] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-5 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,515] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-43 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,515] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,13] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,520] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-21 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,520] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,46] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,520] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,9] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,520] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,42] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,520] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-32 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,520] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,21] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,520] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,17] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,520] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,521] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,30] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,521] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,26] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,521] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,5] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,521] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,38] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,521] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,1] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,521] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-37 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,521] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,34] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,521] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,16] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,522] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-48 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,522] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,45] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,522] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,12] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,522] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,41] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,522] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,24] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,522] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-29 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,522] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,20] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,523] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-40 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,523] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,49] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,523] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-18 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,523] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,524] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,29] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,524] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,25] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,524] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,8] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,524] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,37] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,524] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,4] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,524] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,33] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,524] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,15] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,525] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,48] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,525] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,11] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,525] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,44] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,525] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,23] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,525] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,19] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,525] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,32] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,525] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,28] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,525] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,7] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,526] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,40] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,526] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,3] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,526] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,36] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,526] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,47] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,526] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,14] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,526] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,43] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,526] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,10] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,526] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,22] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,526] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,18] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,526] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,31] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,526] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,27] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,526] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,39] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,527] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,6] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,527] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,35] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,527] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 9 from controller 1 epoch 1 for partition [__consumer_offsets,2] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,529] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-7 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,529] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-23 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,529] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-34 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,529] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-45 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,529] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-26 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,529] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-15 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,529] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-4 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,529] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-42 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,531] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-20 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,531] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-31 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,531] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-9 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,531] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-12 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,531] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,531] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-17 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,531] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-28 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,531] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-6 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,531] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-39 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,531] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-44 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,531] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-47 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,531] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-36 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,531] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-14 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,532] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-3 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,533] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-25 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,533] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-30 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,533] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-41 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,533] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-33 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,534] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-22 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,534] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-11 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,534] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition __consumer_offsets-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,536] INFO [Replica state machine on controller 1]: Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=28,Replica=1],[Topic=__consumer_offsets,Partition=48,Replica=1],[Topic=__consumer_offsets,Partition=5,Replica=1],[Topic=__consumer_offsets,Partition=21,Replica=1],[Topic=__consumer_offsets,Partition=2,Replica=1],[Topic=__consumer_offsets,Partition=18,Replica=1],[Topic=__consumer_offsets,Partition=23,Replica=1],[Topic=__consumer_offsets,Partition=9,Replica=1],[Topic=__consumer_offsets,Partition=39,Replica=1],[Topic=__consumer_offsets,Partition=31,Replica=1],[Topic=__consumer_offsets,Partition=19,Replica=1],[Topic=__consumer_offsets,Partition=10,Replica=1],[Topic=__consumer_offsets,Partition=22,Replica=1],[Topic=__consumer_offsets,Partition=43,Replica=1],[Topic=__consumer_offsets,Partition=40,Replica=1],[Topic=__consumer_offsets,Partition=27,Replica=1],[Topic=__consumer_offsets,Partition=6,Replica=1],[Topic=__consumer_offsets,Partition=1,Replica=1],[Topic=__consumer_offsets,Partition=47,Replica=1],[Topic=__consumer_offsets,Partition=30,Replica=1],[Topic=__consumer_offsets,Partition=42,Replica=1],[Topic=__consumer_offsets,Partition=41,Replica=1],[Topic=__consumer_offsets,Partition=3,Replica=1],[Topic=__consumer_offsets,Partition=13,Replica=1],[Topic=__consumer_offsets,Partition=4,Replica=1],[Topic=__consumer_offsets,Partition=16,Replica=1],[Topic=__consumer_offsets,Partition=46,Replica=1],[Topic=__consumer_offsets,Partition=49,Replica=1],[Topic=__consumer_offsets,Partition=14,Replica=1],[Topic=__consumer_offsets,Partition=45,Replica=1],[Topic=__consumer_offsets,Partition=37,Replica=1],[Topic=__consumer_offsets,Partition=29,Replica=1],[Topic=__consumer_offsets,Partition=20,Replica=1],[Topic=__consumer_offsets,Partition=8,Replica=1],[Topic=__consumer_offsets,Partition=38,Replica=1],[Topic=__consumer_offsets,Partition=7,Replica=1],[Topic=__consumer_offsets,Partition=0,Replica=1],[Topic=__consumer_offsets,Partition=34,Replica=1],[Topic=__consumer_offsets,Partition=33,Replica=1],[Topic=__consumer_offsets,Partition=26,Replica=1],[Topic=__consumer_offsets,Partition=44,Replica=1],[Topic=__consumer_offsets,Partition=32,Replica=1],[Topic=__consumer_offsets,Partition=25,Replica=1],[Topic=__consumer_offsets,Partition=11,Replica=1],[Topic=__consumer_offsets,Partition=36,Replica=1],[Topic=__consumer_offsets,Partition=12,Replica=1],[Topic=__consumer_offsets,Partition=35,Replica=1],[Topic=__consumer_offsets,Partition=15,Replica=1],[Topic=__consumer_offsets,Partition=17,Replica=1],[Topic=__consumer_offsets,Partition=24,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:58,536] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,28] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,536] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,48] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,539] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,5] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,539] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,21] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,539] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,2] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,540] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,18] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,540] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,23] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,540] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,9] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,540] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,39] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,541] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,31] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,541] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,19] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,541] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,10] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,542] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,22] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,543] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,43] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,543] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,40] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,543] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,27] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,543] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,6] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,543] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,1] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,543] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,47] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,543] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,30] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,544] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,42] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,544] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,41] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,544] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,3] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,544] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,13] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,545] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,4] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,545] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,16] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,545] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,46] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,545] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,49] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,545] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,14] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,546] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,45] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,546] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,37] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,546] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,29] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,547] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,20] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,547] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,8] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,547] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,38] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,548] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,7] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,548] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,0] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,548] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,34] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,548] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,33] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,548] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,26] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,548] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,44] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,548] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,32] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,549] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,25] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,549] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,11] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,549] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,36] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,549] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,12] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,549] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,35] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,549] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,15] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,550] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,17] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,550] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [__consumer_offsets,24] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,564] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,565] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-29 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,565] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-48 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,565] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,565] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-45 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,565] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-26 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,565] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-7 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,565] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-42 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,566] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-4 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,566] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-23 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,566] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,566] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-20 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,566] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-39 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,566] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-17 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,566] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-36 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,566] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-14 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,566] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-33 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,566] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-49 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,566] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-11 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,566] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-30 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,567] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-46 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,567] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-27 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,567] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,567] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-24 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,567] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-43 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,567] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-5 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,567] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-21 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,567] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-2 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,567] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-40 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,567] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-37 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,568] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-18 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,568] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-34 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,568] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-15 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,568] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-12 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,568] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-31 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,568] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-9 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,568] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-47 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,568] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-19 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,568] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-28 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,568] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-38 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,568] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-35 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,568] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-44 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,569] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-6 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,569] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-25 (state.change.logger)
[36;1mksql-datagen-users_1  |[0m 119 --> ([ 119 | 13 | 1 | 6591 | 1508909158568 | 'iOS' | 'is this as good as it gets? really ?' ])
[35mkafka_1               |[0m [2017-10-25 05:25:58,569] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-16 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,569] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-22 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,569] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-41 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,569] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-32 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,570] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-3 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,570] TRACE Broker 1 handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-13 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,571] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-37,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-38,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-13,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.server.ReplicaFetcherManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,574] INFO Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,575] INFO Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,575] INFO Created log for partition [__consumer_offsets,0] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,576] INFO Partition [__consumer_offsets,0] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,576] INFO Partition [__consumer_offsets,0] on broker 1: __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,579] INFO Loading producer state from offset 0 for partition __consumer_offsets-29 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,579] INFO Completed load of log __consumer_offsets-29 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,579] INFO Created log for partition [__consumer_offsets,29] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,580] INFO Partition [__consumer_offsets,29] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,580] INFO Partition [__consumer_offsets,29] on broker 1: __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,583] INFO Loading producer state from offset 0 for partition __consumer_offsets-48 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,583] INFO Completed load of log __consumer_offsets-48 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,584] INFO Created log for partition [__consumer_offsets,48] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,584] INFO Partition [__consumer_offsets,48] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,584] INFO Partition [__consumer_offsets,48] on broker 1: __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,589] INFO Loading producer state from offset 0 for partition __consumer_offsets-10 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,590] INFO Completed load of log __consumer_offsets-10 with 1 log segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,591] INFO Created log for partition [__consumer_offsets,10] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,591] INFO Partition [__consumer_offsets,10] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,591] INFO Partition [__consumer_offsets,10] on broker 1: __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,596] INFO Loading producer state from offset 0 for partition __consumer_offsets-45 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,597] INFO Completed load of log __consumer_offsets-45 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,598] INFO Created log for partition [__consumer_offsets,45] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,598] INFO Partition [__consumer_offsets,45] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,598] INFO Partition [__consumer_offsets,45] on broker 1: __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,602] INFO Loading producer state from offset 0 for partition __consumer_offsets-26 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,603] INFO Completed load of log __consumer_offsets-26 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,603] INFO Created log for partition [__consumer_offsets,26] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,604] INFO Partition [__consumer_offsets,26] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,604] INFO Partition [__consumer_offsets,26] on broker 1: __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,606] INFO Loading producer state from offset 0 for partition __consumer_offsets-7 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,607] INFO Completed load of log __consumer_offsets-7 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,607] INFO Created log for partition [__consumer_offsets,7] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,608] INFO Partition [__consumer_offsets,7] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,608] INFO Partition [__consumer_offsets,7] on broker 1: __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,610] INFO Loading producer state from offset 0 for partition __consumer_offsets-42 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,610] INFO Completed load of log __consumer_offsets-42 with 1 log segments, log start offset 0 and log end offset 0 in 0 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,611] INFO Created log for partition [__consumer_offsets,42] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,611] INFO Partition [__consumer_offsets,42] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,611] INFO Partition [__consumer_offsets,42] on broker 1: __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,614] INFO Loading producer state from offset 0 for partition __consumer_offsets-4 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,614] INFO Completed load of log __consumer_offsets-4 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,614] INFO Created log for partition [__consumer_offsets,4] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,615] INFO Partition [__consumer_offsets,4] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,615] INFO Partition [__consumer_offsets,4] on broker 1: __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,617] INFO Loading producer state from offset 0 for partition __consumer_offsets-23 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,618] INFO Completed load of log __consumer_offsets-23 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,618] INFO Created log for partition [__consumer_offsets,23] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,618] INFO Partition [__consumer_offsets,23] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,619] INFO Partition [__consumer_offsets,23] on broker 1: __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,621] INFO Loading producer state from offset 0 for partition __consumer_offsets-1 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,621] INFO Completed load of log __consumer_offsets-1 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,622] INFO Created log for partition [__consumer_offsets,1] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,622] INFO Partition [__consumer_offsets,1] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,622] INFO Partition [__consumer_offsets,1] on broker 1: __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,624] INFO Loading producer state from offset 0 for partition __consumer_offsets-20 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,625] INFO Completed load of log __consumer_offsets-20 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,625] INFO Created log for partition [__consumer_offsets,20] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,626] INFO Partition [__consumer_offsets,20] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,626] INFO Partition [__consumer_offsets,20] on broker 1: __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,629] INFO Loading producer state from offset 0 for partition __consumer_offsets-39 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,630] INFO Completed load of log __consumer_offsets-39 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,630] INFO Created log for partition [__consumer_offsets,39] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,630] INFO Partition [__consumer_offsets,39] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,631] INFO Partition [__consumer_offsets,39] on broker 1: __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,633] INFO Loading producer state from offset 0 for partition __consumer_offsets-17 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,633] INFO Completed load of log __consumer_offsets-17 with 1 log segments, log start offset 0 and log end offset 0 in 0 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,634] INFO Created log for partition [__consumer_offsets,17] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,634] INFO Partition [__consumer_offsets,17] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,634] INFO Partition [__consumer_offsets,17] on broker 1: __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,637] INFO Loading producer state from offset 0 for partition __consumer_offsets-36 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,637] INFO Completed load of log __consumer_offsets-36 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,637] INFO Created log for partition [__consumer_offsets,36] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,638] INFO Partition [__consumer_offsets,36] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,638] INFO Partition [__consumer_offsets,36] on broker 1: __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,640] INFO Loading producer state from offset 0 for partition __consumer_offsets-14 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,641] INFO Completed load of log __consumer_offsets-14 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,642] INFO Created log for partition [__consumer_offsets,14] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,643] INFO Partition [__consumer_offsets,14] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,643] INFO Partition [__consumer_offsets,14] on broker 1: __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,647] INFO Loading producer state from offset 0 for partition __consumer_offsets-33 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,647] INFO Completed load of log __consumer_offsets-33 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,647] INFO Created log for partition [__consumer_offsets,33] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,648] INFO Partition [__consumer_offsets,33] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,648] INFO Partition [__consumer_offsets,33] on broker 1: __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,650] INFO Loading producer state from offset 0 for partition __consumer_offsets-49 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,651] INFO Completed load of log __consumer_offsets-49 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,651] INFO Created log for partition [__consumer_offsets,49] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,651] INFO Partition [__consumer_offsets,49] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,651] INFO Partition [__consumer_offsets,49] on broker 1: __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,654] INFO Loading producer state from offset 0 for partition __consumer_offsets-11 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,654] INFO Completed load of log __consumer_offsets-11 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,655] INFO Created log for partition [__consumer_offsets,11] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,655] INFO Partition [__consumer_offsets,11] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,655] INFO Partition [__consumer_offsets,11] on broker 1: __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,658] INFO Loading producer state from offset 0 for partition __consumer_offsets-30 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,658] INFO Completed load of log __consumer_offsets-30 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,659] INFO Created log for partition [__consumer_offsets,30] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,660] INFO Partition [__consumer_offsets,30] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,660] INFO Partition [__consumer_offsets,30] on broker 1: __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,662] INFO Loading producer state from offset 0 for partition __consumer_offsets-46 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,663] INFO Completed load of log __consumer_offsets-46 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,663] INFO Created log for partition [__consumer_offsets,46] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,664] INFO Partition [__consumer_offsets,46] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,664] INFO Partition [__consumer_offsets,46] on broker 1: __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,667] INFO Loading producer state from offset 0 for partition __consumer_offsets-27 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,667] INFO Completed load of log __consumer_offsets-27 with 1 log segments, log start offset 0 and log end offset 0 in 0 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,668] INFO Created log for partition [__consumer_offsets,27] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,668] INFO Partition [__consumer_offsets,27] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,668] INFO Partition [__consumer_offsets,27] on broker 1: __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,672] INFO Loading producer state from offset 0 for partition __consumer_offsets-8 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,672] INFO Completed load of log __consumer_offsets-8 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,673] INFO Created log for partition [__consumer_offsets,8] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,673] INFO Partition [__consumer_offsets,8] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,673] INFO Partition [__consumer_offsets,8] on broker 1: __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,684] INFO Loading producer state from offset 0 for partition __consumer_offsets-24 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,684] INFO Completed load of log __consumer_offsets-24 with 1 log segments, log start offset 0 and log end offset 0 in 0 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,685] INFO Created log for partition [__consumer_offsets,24] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,685] INFO Partition [__consumer_offsets,24] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,685] INFO Partition [__consumer_offsets,24] on broker 1: __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,688] INFO Loading producer state from offset 0 for partition __consumer_offsets-43 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,688] INFO Completed load of log __consumer_offsets-43 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,689] INFO Created log for partition [__consumer_offsets,43] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,689] INFO Partition [__consumer_offsets,43] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,689] INFO Partition [__consumer_offsets,43] on broker 1: __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,692] INFO Loading producer state from offset 0 for partition __consumer_offsets-5 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,692] INFO Completed load of log __consumer_offsets-5 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,692] INFO Created log for partition [__consumer_offsets,5] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,693] INFO Partition [__consumer_offsets,5] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,693] INFO Partition [__consumer_offsets,5] on broker 1: __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,695] INFO Loading producer state from offset 0 for partition __consumer_offsets-21 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,696] INFO Completed load of log __consumer_offsets-21 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,696] INFO Created log for partition [__consumer_offsets,21] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,696] INFO Partition [__consumer_offsets,21] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,696] INFO Partition [__consumer_offsets,21] on broker 1: __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,699] INFO Loading producer state from offset 0 for partition __consumer_offsets-2 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,699] INFO Completed load of log __consumer_offsets-2 with 1 log segments, log start offset 0 and log end offset 0 in 0 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,700] INFO Created log for partition [__consumer_offsets,2] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,700] INFO Partition [__consumer_offsets,2] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,700] INFO Partition [__consumer_offsets,2] on broker 1: __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,703] INFO Loading producer state from offset 0 for partition __consumer_offsets-40 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,703] INFO Completed load of log __consumer_offsets-40 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,703] INFO Created log for partition [__consumer_offsets,40] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,704] INFO Partition [__consumer_offsets,40] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,704] INFO Partition [__consumer_offsets,40] on broker 1: __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,706] INFO Loading producer state from offset 0 for partition __consumer_offsets-37 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,706] INFO Completed load of log __consumer_offsets-37 with 1 log segments, log start offset 0 and log end offset 0 in 0 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,707] INFO Created log for partition [__consumer_offsets,37] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,707] INFO Partition [__consumer_offsets,37] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,707] INFO Partition [__consumer_offsets,37] on broker 1: __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,710] INFO Loading producer state from offset 0 for partition __consumer_offsets-18 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,710] INFO Completed load of log __consumer_offsets-18 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,711] INFO Created log for partition [__consumer_offsets,18] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,711] INFO Partition [__consumer_offsets,18] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,711] INFO Partition [__consumer_offsets,18] on broker 1: __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,713] INFO Loading producer state from offset 0 for partition __consumer_offsets-34 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,714] INFO Completed load of log __consumer_offsets-34 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,714] INFO Created log for partition [__consumer_offsets,34] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,714] INFO Partition [__consumer_offsets,34] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,714] INFO Partition [__consumer_offsets,34] on broker 1: __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,718] INFO Loading producer state from offset 0 for partition __consumer_offsets-15 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,719] INFO Completed load of log __consumer_offsets-15 with 1 log segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,719] INFO Created log for partition [__consumer_offsets,15] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,719] INFO Partition [__consumer_offsets,15] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,719] INFO Partition [__consumer_offsets,15] on broker 1: __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,722] INFO Loading producer state from offset 0 for partition __consumer_offsets-12 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,722] INFO Completed load of log __consumer_offsets-12 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,722] INFO Created log for partition [__consumer_offsets,12] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,723] INFO Partition [__consumer_offsets,12] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,723] INFO Partition [__consumer_offsets,12] on broker 1: __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,725] INFO Loading producer state from offset 0 for partition __consumer_offsets-31 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,725] INFO Completed load of log __consumer_offsets-31 with 1 log segments, log start offset 0 and log end offset 0 in 0 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,726] INFO Created log for partition [__consumer_offsets,31] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,726] INFO Partition [__consumer_offsets,31] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,726] INFO Partition [__consumer_offsets,31] on broker 1: __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,730] INFO Loading producer state from offset 0 for partition __consumer_offsets-9 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,730] INFO Completed load of log __consumer_offsets-9 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,731] INFO Created log for partition [__consumer_offsets,9] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,731] INFO Partition [__consumer_offsets,9] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,731] INFO Partition [__consumer_offsets,9] on broker 1: __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,733] INFO Loading producer state from offset 0 for partition __consumer_offsets-47 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,734] INFO Completed load of log __consumer_offsets-47 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,734] INFO Created log for partition [__consumer_offsets,47] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,734] INFO Partition [__consumer_offsets,47] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,734] INFO Partition [__consumer_offsets,47] on broker 1: __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,737] INFO Loading producer state from offset 0 for partition __consumer_offsets-19 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,737] INFO Completed load of log __consumer_offsets-19 with 1 log segments, log start offset 0 and log end offset 0 in 0 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,738] INFO Created log for partition [__consumer_offsets,19] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,738] INFO Partition [__consumer_offsets,19] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,738] INFO Partition [__consumer_offsets,19] on broker 1: __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,741] INFO Loading producer state from offset 0 for partition __consumer_offsets-28 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,741] INFO Completed load of log __consumer_offsets-28 with 1 log segments, log start offset 0 and log end offset 0 in 0 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,742] INFO Created log for partition [__consumer_offsets,28] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,743] INFO Partition [__consumer_offsets,28] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,744] INFO Partition [__consumer_offsets,28] on broker 1: __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,746] INFO Loading producer state from offset 0 for partition __consumer_offsets-38 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,746] INFO Completed load of log __consumer_offsets-38 with 1 log segments, log start offset 0 and log end offset 0 in 0 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,747] INFO Created log for partition [__consumer_offsets,38] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,747] INFO Partition [__consumer_offsets,38] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,747] INFO Partition [__consumer_offsets,38] on broker 1: __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,750] INFO Loading producer state from offset 0 for partition __consumer_offsets-35 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,750] INFO Completed load of log __consumer_offsets-35 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,751] INFO Created log for partition [__consumer_offsets,35] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,751] INFO Partition [__consumer_offsets,35] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,751] INFO Partition [__consumer_offsets,35] on broker 1: __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,753] INFO Loading producer state from offset 0 for partition __consumer_offsets-44 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,754] INFO Completed load of log __consumer_offsets-44 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,754] INFO Created log for partition [__consumer_offsets,44] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,754] INFO Partition [__consumer_offsets,44] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,755] INFO Partition [__consumer_offsets,44] on broker 1: __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,757] INFO Loading producer state from offset 0 for partition __consumer_offsets-6 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,757] INFO Completed load of log __consumer_offsets-6 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,758] INFO Created log for partition [__consumer_offsets,6] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,758] INFO Partition [__consumer_offsets,6] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,758] INFO Partition [__consumer_offsets,6] on broker 1: __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,761] INFO Loading producer state from offset 0 for partition __consumer_offsets-25 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,761] INFO Completed load of log __consumer_offsets-25 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,761] INFO Created log for partition [__consumer_offsets,25] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,762] INFO Partition [__consumer_offsets,25] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,762] INFO Partition [__consumer_offsets,25] on broker 1: __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,765] INFO Loading producer state from offset 0 for partition __consumer_offsets-16 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,765] INFO Completed load of log __consumer_offsets-16 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,766] INFO Created log for partition [__consumer_offsets,16] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,766] INFO Partition [__consumer_offsets,16] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,766] INFO Partition [__consumer_offsets,16] on broker 1: __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,770] INFO Loading producer state from offset 0 for partition __consumer_offsets-22 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,771] INFO Completed load of log __consumer_offsets-22 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,771] INFO Created log for partition [__consumer_offsets,22] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,772] INFO Partition [__consumer_offsets,22] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,772] INFO Partition [__consumer_offsets,22] on broker 1: __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,774] INFO Loading producer state from offset 0 for partition __consumer_offsets-41 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,774] INFO Completed load of log __consumer_offsets-41 with 1 log segments, log start offset 0 and log end offset 0 in 0 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,775] INFO Created log for partition [__consumer_offsets,41] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,776] INFO Partition [__consumer_offsets,41] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,776] INFO Partition [__consumer_offsets,41] on broker 1: __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,778] INFO Loading producer state from offset 0 for partition __consumer_offsets-32 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,778] INFO Completed load of log __consumer_offsets-32 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,779] INFO Created log for partition [__consumer_offsets,32] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,779] INFO Partition [__consumer_offsets,32] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,779] INFO Partition [__consumer_offsets,32] on broker 1: __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,781] INFO Loading producer state from offset 0 for partition __consumer_offsets-3 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,782] INFO Completed load of log __consumer_offsets-3 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,782] INFO Created log for partition [__consumer_offsets,3] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,782] INFO Partition [__consumer_offsets,3] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,782] INFO Partition [__consumer_offsets,3] on broker 1: __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,785] INFO Loading producer state from offset 0 for partition __consumer_offsets-13 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,785] INFO Completed load of log __consumer_offsets-13 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:58,786] INFO Created log for partition [__consumer_offsets,13] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,786] INFO Partition [__consumer_offsets,13] on broker 1: No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,786] INFO Partition [__consumer_offsets,13] on broker 1: __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:58,786] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-22 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,786] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-25 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,786] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-28 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,786] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-31 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,786] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-34 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,786] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-37 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,786] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-40 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,786] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-43 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,786] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-46 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,786] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-49 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,786] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-41 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,786] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-44 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,786] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-47 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,786] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,786] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-4 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,786] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-7 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,786] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,786] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-13 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,786] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-16 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,786] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-19 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,786] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-2 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,786] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-5 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-11 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-14 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-17 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-20 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-23 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-26 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-29 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-32 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-35 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-38 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-3 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-6 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-9 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-12 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-15 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-18 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-21 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-24 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-27 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-30 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-33 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-36 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-39 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-42 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-45 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition __consumer_offsets-48 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-29 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-48 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-45 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-26 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-7 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-42 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-4 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-23 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-20 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-39 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-17 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,787] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-36 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-14 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-33 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-49 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-11 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-30 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-46 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-27 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-8 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-24 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-43 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-5 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-21 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-2 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-40 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-37 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-18 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-34 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-15 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-12 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-31 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-9 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-47 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-19 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-28 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-38 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-35 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-44 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-6 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-25 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-16 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-22 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-41 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-32 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-3 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,788] TRACE Broker 1 completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-13 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,789] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,795] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-22 in 5 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,795] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,795] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,795] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,795] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,795] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,795] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,795] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,795] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,796] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,796] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-25 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,796] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-28 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,796] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,796] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-34 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,796] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,796] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,796] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,796] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,796] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,796] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,796] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,797] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,796] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,797] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,797] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,797] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,797] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,797] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,797] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,798] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,797] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-44 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,798] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,798] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,798] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,798] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,798] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,798] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,798] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,798] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,798] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,798] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,798] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,798] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,798] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,798] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,798] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,798] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,798] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,798] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,798] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,798] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,798] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,798] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,798] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,799] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,799] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,799] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,799] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,800] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,800] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,800] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,800] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,800] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,800] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,800] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,800] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,801] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,801] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,801] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,801] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,801] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,801] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,801] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,801] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,802] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,802] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,802] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,802] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,802] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-6 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,802] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,802] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,803] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-12 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,804] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-15 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,804] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,804] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,804] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,804] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,805] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,805] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,805] INFO [Group Metadata Manager on Broker 1]: Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,804] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,805] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-27 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,806] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-30 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,806] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,806] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,806] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,806] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,806] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,806] INFO [Group Metadata Manager on Broker 1]: Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mkafka_1               |[0m [2017-10-25 05:25:58,807] TRACE Controller 1 epoch 1 received response {error_code=0,partitions=[{topic=__consumer_offsets,partition=13,error_code=0},{topic=__consumer_offsets,partition=46,error_code=0},{topic=__consumer_offsets,partition=9,error_code=0},{topic=__consumer_offsets,partition=42,error_code=0},{topic=__consumer_offsets,partition=21,error_code=0},{topic=__consumer_offsets,partition=17,error_code=0},{topic=__consumer_offsets,partition=30,error_code=0},{topic=__consumer_offsets,partition=26,error_code=0},{topic=__consumer_offsets,partition=5,error_code=0},{topic=__consumer_offsets,partition=38,error_code=0},{topic=__consumer_offsets,partition=1,error_code=0},{topic=__consumer_offsets,partition=34,error_code=0},{topic=__consumer_offsets,partition=16,error_code=0},{topic=__consumer_offsets,partition=45,error_code=0},{topic=__consumer_offsets,partition=12,error_code=0},{topic=__consumer_offsets,partition=41,error_code=0},{topic=__consumer_offsets,partition=24,error_code=0},{topic=__consumer_offsets,partition=20,error_code=0},{topic=__consumer_offsets,partition=49,error_code=0},{topic=__consumer_offsets,partition=0,error_code=0},{topic=__consumer_offsets,partition=29,error_code=0},{topic=__consumer_offsets,partition=25,error_code=0},{topic=__consumer_offsets,partition=8,error_code=0},{topic=__consumer_offsets,partition=37,error_code=0},{topic=__consumer_offsets,partition=4,error_code=0},{topic=__consumer_offsets,partition=33,error_code=0},{topic=__consumer_offsets,partition=15,error_code=0},{topic=__consumer_offsets,partition=48,error_code=0},{topic=__consumer_offsets,partition=11,error_code=0},{topic=__consumer_offsets,partition=44,error_code=0},{topic=__consumer_offsets,partition=23,error_code=0},{topic=__consumer_offsets,partition=19,error_code=0},{topic=__consumer_offsets,partition=32,error_code=0},{topic=__consumer_offsets,partition=28,error_code=0},{topic=__consumer_offsets,partition=7,error_code=0},{topic=__consumer_offsets,partition=40,error_code=0},{topic=__consumer_offsets,partition=3,error_code=0},{topic=__consumer_offsets,partition=36,error_code=0},{topic=__consumer_offsets,partition=47,error_code=0},{topic=__consumer_offsets,partition=14,error_code=0},{topic=__consumer_offsets,partition=43,error_code=0},{topic=__consumer_offsets,partition=10,error_code=0},{topic=__consumer_offsets,partition=22,error_code=0},{topic=__consumer_offsets,partition=18,error_code=0},{topic=__consumer_offsets,partition=31,error_code=0},{topic=__consumer_offsets,partition=27,error_code=0},{topic=__consumer_offsets,partition=39,error_code=0},{topic=__consumer_offsets,partition=6,error_code=0},{topic=__consumer_offsets,partition=35,error_code=0},{topic=__consumer_offsets,partition=2,error_code=0}]} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,808] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-13 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,809] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-46 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,809] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-9 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,809] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-42 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,809] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-21 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,809] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-17 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,810] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-30 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,810] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-26 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,810] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-5 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,810] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-38 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,811] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-1 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,811] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-34 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,811] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-16 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,811] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-45 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,811] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-12 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,812] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-41 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,812] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-24 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,812] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-20 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,812] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-49 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,812] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,813] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-29 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,813] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-25 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,813] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-8 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,813] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-37 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,813] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-4 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,814] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-33 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,814] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-15 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,814] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-48 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,814] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-11 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,814] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-44 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,815] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-23 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,815] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-19 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,815] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-32 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,815] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-28 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,815] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-7 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,816] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-40 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,816] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-3 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,816] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-36 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,816] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-47 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,816] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-14 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,817] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-43 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,817] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-10 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,817] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-22 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,817] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-18 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,817] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-31 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,818] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-27 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,818] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-39 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,818] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-6 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,818] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-35 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,819] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition __consumer_offsets-2 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:58,819] TRACE Controller 1 epoch 1 received response {error_code=0} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[36;1mksql-datagen-users_1  |[0m 120 --> ([ 120 | 11 | 3 | 9567 | 1508909158848 | 'ios' | '(expletive deleted)' ])
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,871] INFO Discovered coordinator kafka:29092 (id: 2147483646 rack: null) for group connect. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,887] INFO Finished reading KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,888] INFO Started KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,888] INFO Finished reading offsets topic and starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,889] INFO Worker started (org.apache.kafka.connect.runtime.Worker)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,890] INFO Starting KafkaBasedLog with topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,890] INFO AdminClientConfig values: 
[33;1mconnect_1             |[0m 	bootstrap.servers = [kafka:29092]
[33;1mconnect_1             |[0m 	client.id = 
[33;1mconnect_1             |[0m 	connections.max.idle.ms = 300000
[33;1mconnect_1             |[0m 	metadata.max.age.ms = 300000
[33;1mconnect_1             |[0m 	metric.reporters = []
[33;1mconnect_1             |[0m 	metrics.num.samples = 2
[33;1mconnect_1             |[0m 	metrics.recording.level = INFO
[33;1mconnect_1             |[0m 	metrics.sample.window.ms = 30000
[33;1mconnect_1             |[0m 	receive.buffer.bytes = 65536
[33;1mconnect_1             |[0m 	reconnect.backoff.max.ms = 1000
[33;1mconnect_1             |[0m 	reconnect.backoff.ms = 50
[33;1mconnect_1             |[0m 	request.timeout.ms = 120000
[33;1mconnect_1             |[0m 	retries = 5
[33;1mconnect_1             |[0m 	retry.backoff.ms = 100
[33;1mconnect_1             |[0m 	sasl.jaas.config = null
[33;1mconnect_1             |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[33;1mconnect_1             |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[33;1mconnect_1             |[0m 	sasl.kerberos.service.name = null
[33;1mconnect_1             |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[33;1mconnect_1             |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[33;1mconnect_1             |[0m 	sasl.mechanism = GSSAPI
[33;1mconnect_1             |[0m 	security.protocol = PLAINTEXT
[33;1mconnect_1             |[0m 	send.buffer.bytes = 131072
[33;1mconnect_1             |[0m 	ssl.cipher.suites = null
[33;1mconnect_1             |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[33;1mconnect_1             |[0m 	ssl.endpoint.identification.algorithm = null
[33;1mconnect_1             |[0m 	ssl.key.password = null
[33;1mconnect_1             |[0m 	ssl.keymanager.algorithm = SunX509
[33;1mconnect_1             |[0m 	ssl.keystore.location = null
[33;1mconnect_1             |[0m 	ssl.keystore.password = null
[33;1mconnect_1             |[0m 	ssl.keystore.type = JKS
[33;1mconnect_1             |[0m 	ssl.protocol = TLS
[33;1mconnect_1             |[0m 	ssl.provider = null
[33;1mconnect_1             |[0m 	ssl.secure.random.implementation = null
[33;1mconnect_1             |[0m 	ssl.trustmanager.algorithm = PKIX
[33;1mconnect_1             |[0m 	ssl.truststore.location = null
[33;1mconnect_1             |[0m 	ssl.truststore.password = null
[33;1mconnect_1             |[0m 	ssl.truststore.type = JKS
[33;1mconnect_1             |[0m  (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,892] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,892] WARN The configuration 'replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,892] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,892] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,896] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,897] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,897] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,897] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,897] WARN The configuration 'internal.key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,897] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,898] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,898] WARN The configuration 'internal.key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,901] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,902] WARN The configuration 'internal.value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,902] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,902] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,902] WARN The configuration 'internal.value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,903] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,903] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,903] WARN The configuration 'log4j.root.loglevel' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,903] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:58,903] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33mzookeeper_1           |[0m [2017-10-25 05:25:59,003] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:setData cxid:0x291 zxid:0x156 txntype:-1 reqpath:n/a Error Path:/config/topics/connect-status Error:KeeperErrorCode = NoNode for /config/topics/connect-status (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:25:59,023] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x292 zxid:0x157 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:59,045] INFO Topic creation {"version":1,"partitions":{"4":[1],"1":[1],"0":[1],"2":[1],"3":[1]}} (kafka.admin.AdminUtils$)
[35mkafka_1               |[0m [2017-10-25 05:25:59,062] INFO [Controller 1]: New topics: [Set(connect-status)], deleted topics: [Set()], new partition replica assignment [Map([connect-status,0] -> List(1), [connect-status,2] -> List(1), [connect-status,4] -> List(1), [connect-status,3] -> List(1), [connect-status,1] -> List(1))] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:59,062] INFO [Controller 1]: New topic creation callback for [connect-status,0],[connect-status,2],[connect-status,4],[connect-status,3],[connect-status,1] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:59,063] INFO [Controller 1]: New partition creation callback for [connect-status,0],[connect-status,2],[connect-status,4],[connect-status,3],[connect-status,1] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:59,063] INFO [Partition state machine on Controller 1]: Invoking state change to NewPartition for partitions [connect-status,0],[connect-status,2],[connect-status,4],[connect-status,3],[connect-status,1] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:59,063] TRACE Controller 1 epoch 1 changed partition [connect-status,0] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,063] TRACE Controller 1 epoch 1 changed partition [connect-status,2] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,063] TRACE Controller 1 epoch 1 changed partition [connect-status,4] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,063] TRACE Controller 1 epoch 1 changed partition [connect-status,3] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,063] TRACE Controller 1 epoch 1 changed partition [connect-status,1] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,063] INFO [Replica state machine on controller 1]: Invoking state change to NewReplica for replicas [Topic=connect-status,Partition=0,Replica=1],[Topic=connect-status,Partition=2,Replica=1],[Topic=connect-status,Partition=4,Replica=1],[Topic=connect-status,Partition=3,Replica=1],[Topic=connect-status,Partition=1,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:59,065] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-status,0] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,066] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-status,2] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,066] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-status,4] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,067] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-status,3] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,068] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-status,1] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,068] INFO [Partition state machine on Controller 1]: Invoking state change to OnlinePartition for partitions [connect-status,0],[connect-status,2],[connect-status,4],[connect-status,3],[connect-status,1] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:59,069] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-status,0] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:59,069] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-status,0] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:59,069] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x29e zxid:0x15a txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-status/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-status/partitions/0 (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:25:59,079] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x29f zxid:0x15b txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-status/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/connect-status/partitions (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:59,113] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: _confluent-metrics-5. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[35mkafka_1               |[0m [2017-10-25 05:25:59,122] TRACE Controller 1 epoch 1 changed partition [connect-status,0] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,122] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-status,2] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:59,122] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-status,2] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:59,122] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x2a3 zxid:0x15f txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-status/partitions/2 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-status/partitions/2 (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 121 --> ([ 121 | 6 | 1 | 4301 | 1508909159136 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[35mkafka_1               |[0m [2017-10-25 05:25:59,166] TRACE Controller 1 epoch 1 changed partition [connect-status,2] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,166] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-status,4] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:59,166] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-status,4] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:59,167] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x2a6 zxid:0x162 txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-status/partitions/4 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-status/partitions/4 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:59,200] TRACE Controller 1 epoch 1 changed partition [connect-status,4] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,200] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-status,3] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:59,200] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-status,3] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:59,201] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x2a9 zxid:0x165 txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-status/partitions/3 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-status/partitions/3 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:59,239] TRACE Controller 1 epoch 1 changed partition [connect-status,3] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[33mzookeeper_1           |[0m [2017-10-25 05:25:59,241] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x2ac zxid:0x168 txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-status/partitions/1 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-status/partitions/1 (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:59,240] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-status,1] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:59,240] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-status,1] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:59,287] TRACE Controller 1 epoch 1 changed partition [connect-status,1] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,287] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-status,2] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,288] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-status,4] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,288] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-status,1] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,288] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-status,3] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,288] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-status,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,289] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-status-2 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,289] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-status-4 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,289] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-status-1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,289] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-status-3 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,290] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-status-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,291] INFO [Replica state machine on controller 1]: Invoking state change to OnlineReplica for replicas [Topic=connect-status,Partition=0,Replica=1],[Topic=connect-status,Partition=2,Replica=1],[Topic=connect-status,Partition=4,Replica=1],[Topic=connect-status,Partition=3,Replica=1],[Topic=connect-status,Partition=1,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:59,291] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-status,0] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,291] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-status,2] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,291] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-status,4] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,292] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-status,3] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,292] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-status,1] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,293] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 11 from controller 1 epoch 1 for partition [connect-status,1] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,293] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 11 from controller 1 epoch 1 for partition [connect-status,2] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,293] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 11 from controller 1 epoch 1 for partition [connect-status,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,294] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 11 from controller 1 epoch 1 for partition [connect-status,3] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,294] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 11 from controller 1 epoch 1 for partition [connect-status,4] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,297] TRACE Broker 1 handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition connect-status-1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,297] TRACE Broker 1 handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition connect-status-2 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,297] TRACE Broker 1 handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition connect-status-3 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,297] TRACE Broker 1 handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition connect-status-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,298] TRACE Broker 1 handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition connect-status-4 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,298] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions connect-status-4,connect-status-3,connect-status-2,connect-status-0,connect-status-1 (kafka.server.ReplicaFetcherManager)
[35mkafka_1               |[0m [2017-10-25 05:25:59,301] INFO Loading producer state from offset 0 for partition connect-status-1 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:59,302] INFO Completed load of log connect-status-1 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:59,303] INFO Created log for partition [connect-status,1] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:59,303] INFO Partition [connect-status,1] on broker 1: No checkpointed highwatermark is found for partition connect-status-1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:59,303] INFO Partition [connect-status,1] on broker 1: connect-status-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:59,306] INFO Loading producer state from offset 0 for partition connect-status-2 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:59,307] INFO Completed load of log connect-status-2 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:59,308] INFO Created log for partition [connect-status,2] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:59,308] INFO Partition [connect-status,2] on broker 1: No checkpointed highwatermark is found for partition connect-status-2 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:59,308] INFO Partition [connect-status,2] on broker 1: connect-status-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:59,311] INFO Loading producer state from offset 0 for partition connect-status-3 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:59,312] INFO Completed load of log connect-status-3 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:59,312] INFO Created log for partition [connect-status,3] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:59,313] INFO Partition [connect-status,3] on broker 1: No checkpointed highwatermark is found for partition connect-status-3 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:59,313] INFO Partition [connect-status,3] on broker 1: connect-status-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:59,315] INFO Loading producer state from offset 0 for partition connect-status-0 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:59,316] INFO Completed load of log connect-status-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:59,316] INFO Created log for partition [connect-status,0] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:59,317] INFO Partition [connect-status,0] on broker 1: No checkpointed highwatermark is found for partition connect-status-0 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:59,317] INFO Partition [connect-status,0] on broker 1: connect-status-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:59,319] INFO Loading producer state from offset 0 for partition connect-status-4 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:59,320] INFO Completed load of log connect-status-4 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:59,320] INFO Created log for partition [connect-status,4] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:59,321] INFO Partition [connect-status,4] on broker 1: No checkpointed highwatermark is found for partition connect-status-4 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:59,321] INFO Partition [connect-status,4] on broker 1: connect-status-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:59,321] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition connect-status-4 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,321] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition connect-status-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,322] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition connect-status-3 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,322] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition connect-status-2 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,322] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition connect-status-1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,322] TRACE Broker 1 completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition connect-status-1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,322] TRACE Broker 1 completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition connect-status-2 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,323] TRACE Broker 1 completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition connect-status-3 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,323] TRACE Broker 1 completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition connect-status-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,323] TRACE Broker 1 completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition connect-status-4 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,324] TRACE Controller 1 epoch 1 received response {error_code=0,partitions=[{topic=connect-status,partition=1,error_code=0},{topic=connect-status,partition=2,error_code=0},{topic=connect-status,partition=0,error_code=0},{topic=connect-status,partition=3,error_code=0},{topic=connect-status,partition=4,error_code=0}]} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,325] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-status-1 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,325] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-status-2 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,325] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-status-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,326] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-status-3 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,326] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-status-4 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,327] TRACE Controller 1 epoch 1 received response {error_code=0} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,327] INFO Created topic (name=connect-status, numPartitions=5, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at kafka:29092 (org.apache.kafka.connect.util.TopicAdmin)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,330] INFO ProducerConfig values: 
[33;1mconnect_1             |[0m 	acks = all
[33;1mconnect_1             |[0m 	batch.size = 16384
[33;1mconnect_1             |[0m 	bootstrap.servers = [kafka:29092]
[33;1mconnect_1             |[0m 	buffer.memory = 33554432
[33;1mconnect_1             |[0m 	client.id = 
[33;1mconnect_1             |[0m 	compression.type = none
[33;1mconnect_1             |[0m 	connections.max.idle.ms = 540000
[33;1mconnect_1             |[0m 	enable.idempotence = false
[33;1mconnect_1             |[0m 	interceptor.classes = null
[33;1mconnect_1             |[0m 	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
[33;1mconnect_1             |[0m 	linger.ms = 0
[33;1mconnect_1             |[0m 	max.block.ms = 60000
[33;1mconnect_1             |[0m 	max.in.flight.requests.per.connection = 1
[33;1mconnect_1             |[0m 	max.request.size = 1048576
[33;1mconnect_1             |[0m 	metadata.max.age.ms = 300000
[33;1mconnect_1             |[0m 	metric.reporters = []
[33;1mconnect_1             |[0m 	metrics.num.samples = 2
[33;1mconnect_1             |[0m 	metrics.recording.level = INFO
[33;1mconnect_1             |[0m 	metrics.sample.window.ms = 30000
[33;1mconnect_1             |[0m 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
[33;1mconnect_1             |[0m 	receive.buffer.bytes = 32768
[33;1mconnect_1             |[0m 	reconnect.backoff.max.ms = 1000
[33;1mconnect_1             |[0m 	reconnect.backoff.ms = 50
[33;1mconnect_1             |[0m 	request.timeout.ms = 30000
[33;1mconnect_1             |[0m 	retries = 0
[33;1mconnect_1             |[0m 	retry.backoff.ms = 100
[33;1mconnect_1             |[0m 	sasl.jaas.config = null
[33;1mconnect_1             |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[33;1mconnect_1             |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[33;1mconnect_1             |[0m 	sasl.kerberos.service.name = null
[33;1mconnect_1             |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[33;1mconnect_1             |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[33;1mconnect_1             |[0m 	sasl.mechanism = GSSAPI
[33;1mconnect_1             |[0m 	security.protocol = PLAINTEXT
[33;1mconnect_1             |[0m 	send.buffer.bytes = 131072
[33;1mconnect_1             |[0m 	ssl.cipher.suites = null
[33;1mconnect_1             |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[33;1mconnect_1             |[0m 	ssl.endpoint.identification.algorithm = null
[33;1mconnect_1             |[0m 	ssl.key.password = null
[33;1mconnect_1             |[0m 	ssl.keymanager.algorithm = SunX509
[33;1mconnect_1             |[0m 	ssl.keystore.location = null
[33;1mconnect_1             |[0m 	ssl.keystore.password = null
[33;1mconnect_1             |[0m 	ssl.keystore.type = JKS
[33;1mconnect_1             |[0m 	ssl.protocol = TLS
[33;1mconnect_1             |[0m 	ssl.provider = null
[33;1mconnect_1             |[0m 	ssl.secure.random.implementation = null
[33;1mconnect_1             |[0m 	ssl.trustmanager.algorithm = PKIX
[33;1mconnect_1             |[0m 	ssl.truststore.location = null
[33;1mconnect_1             |[0m 	ssl.truststore.password = null
[33;1mconnect_1             |[0m 	ssl.truststore.type = JKS
[33;1mconnect_1             |[0m 	transaction.timeout.ms = 60000
[33;1mconnect_1             |[0m 	transactional.id = null
[33;1mconnect_1             |[0m 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[33;1mconnect_1             |[0m  (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,343] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,343] WARN The configuration 'replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,343] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,343] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,344] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,344] WARN The configuration 'internal.key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,344] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,344] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,344] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,345] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,345] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,345] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,345] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,346] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,346] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,346] WARN The configuration 'internal.key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,346] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,346] WARN The configuration 'internal.value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,347] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,347] WARN The configuration 'internal.value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,347] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,347] WARN The configuration 'log4j.root.loglevel' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,347] INFO Kafka version : 0.11.0.0-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,348] INFO Kafka commitId : 6a8cf706ddc9ab6a (org.apache.kafka.common.utils.AppInfoParser)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,349] INFO ConsumerConfig values: 
[33;1mconnect_1             |[0m 	auto.commit.interval.ms = 5000
[33;1mconnect_1             |[0m 	auto.offset.reset = earliest
[33;1mconnect_1             |[0m 	bootstrap.servers = [kafka:29092]
[33;1mconnect_1             |[0m 	check.crcs = true
[33;1mconnect_1             |[0m 	client.id = 
[33;1mconnect_1             |[0m 	connections.max.idle.ms = 540000
[33;1mconnect_1             |[0m 	enable.auto.commit = false
[33;1mconnect_1             |[0m 	exclude.internal.topics = true
[33;1mconnect_1             |[0m 	fetch.max.bytes = 52428800
[33;1mconnect_1             |[0m 	fetch.max.wait.ms = 500
[33;1mconnect_1             |[0m 	fetch.min.bytes = 1
[33;1mconnect_1             |[0m 	group.id = connect
[33;1mconnect_1             |[0m 	heartbeat.interval.ms = 3000
[33;1mconnect_1             |[0m 	interceptor.classes = null
[33;1mconnect_1             |[0m 	internal.leave.group.on.close = true
[33;1mconnect_1             |[0m 	isolation.level = read_uncommitted
[33;1mconnect_1             |[0m 	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
[33;1mconnect_1             |[0m 	max.partition.fetch.bytes = 1048576
[33;1mconnect_1             |[0m 	max.poll.interval.ms = 300000
[33;1mconnect_1             |[0m 	max.poll.records = 500
[33;1mconnect_1             |[0m 	metadata.max.age.ms = 300000
[33;1mconnect_1             |[0m 	metric.reporters = []
[33;1mconnect_1             |[0m 	metrics.num.samples = 2
[33;1mconnect_1             |[0m 	metrics.recording.level = INFO
[33;1mconnect_1             |[0m 	metrics.sample.window.ms = 30000
[33;1mconnect_1             |[0m 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
[33;1mconnect_1             |[0m 	receive.buffer.bytes = 65536
[33;1mconnect_1             |[0m 	reconnect.backoff.max.ms = 1000
[33;1mconnect_1             |[0m 	reconnect.backoff.ms = 50
[33;1mconnect_1             |[0m 	request.timeout.ms = 305000
[33;1mconnect_1             |[0m 	retry.backoff.ms = 100
[33;1mconnect_1             |[0m 	sasl.jaas.config = null
[33;1mconnect_1             |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[33;1mconnect_1             |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[33;1mconnect_1             |[0m 	sasl.kerberos.service.name = null
[33;1mconnect_1             |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[33;1mconnect_1             |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[33;1mconnect_1             |[0m 	sasl.mechanism = GSSAPI
[33;1mconnect_1             |[0m 	security.protocol = PLAINTEXT
[33;1mconnect_1             |[0m 	send.buffer.bytes = 131072
[33;1mconnect_1             |[0m 	session.timeout.ms = 10000
[33;1mconnect_1             |[0m 	ssl.cipher.suites = null
[33;1mconnect_1             |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[33;1mconnect_1             |[0m 	ssl.endpoint.identification.algorithm = null
[33;1mconnect_1             |[0m 	ssl.key.password = null
[33;1mconnect_1             |[0m 	ssl.keymanager.algorithm = SunX509
[33;1mconnect_1             |[0m 	ssl.keystore.location = null
[33;1mconnect_1             |[0m 	ssl.keystore.password = null
[33;1mconnect_1             |[0m 	ssl.keystore.type = JKS
[33;1mconnect_1             |[0m 	ssl.protocol = TLS
[33;1mconnect_1             |[0m 	ssl.provider = null
[33;1mconnect_1             |[0m 	ssl.secure.random.implementation = null
[33;1mconnect_1             |[0m 	ssl.trustmanager.algorithm = PKIX
[33;1mconnect_1             |[0m 	ssl.truststore.location = null
[33;1mconnect_1             |[0m 	ssl.truststore.password = null
[33;1mconnect_1             |[0m 	ssl.truststore.type = JKS
[33;1mconnect_1             |[0m 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[33;1mconnect_1             |[0m  (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,353] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,353] WARN The configuration 'replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,354] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,354] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,354] WARN The configuration 'internal.key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,354] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,355] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,355] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,355] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,355] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,355] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,356] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,356] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,356] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,356] WARN The configuration 'internal.key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,356] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,356] WARN The configuration 'internal.value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,357] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,357] WARN The configuration 'internal.value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,357] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,357] WARN The configuration 'log4j.root.loglevel' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,357] INFO Kafka version : 0.11.0.0-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,358] INFO Kafka commitId : 6a8cf706ddc9ab6a (org.apache.kafka.common.utils.AppInfoParser)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,378] INFO Discovered coordinator kafka:29092 (id: 2147483646 rack: null) for group connect. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,391] INFO Finished reading KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,391] INFO Started KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,394] INFO Starting KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,394] INFO Starting KafkaBasedLog with topic connect-config (org.apache.kafka.connect.util.KafkaBasedLog)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,394] INFO AdminClientConfig values: 
[33;1mconnect_1             |[0m 	bootstrap.servers = [kafka:29092]
[33;1mconnect_1             |[0m 	client.id = 
[33;1mconnect_1             |[0m 	connections.max.idle.ms = 300000
[33;1mconnect_1             |[0m 	metadata.max.age.ms = 300000
[33;1mconnect_1             |[0m 	metric.reporters = []
[33;1mconnect_1             |[0m 	metrics.num.samples = 2
[33;1mconnect_1             |[0m 	metrics.recording.level = INFO
[33;1mconnect_1             |[0m 	metrics.sample.window.ms = 30000
[33;1mconnect_1             |[0m 	receive.buffer.bytes = 65536
[33;1mconnect_1             |[0m 	reconnect.backoff.max.ms = 1000
[33;1mconnect_1             |[0m 	reconnect.backoff.ms = 50
[33;1mconnect_1             |[0m 	request.timeout.ms = 120000
[33;1mconnect_1             |[0m 	retries = 5
[33;1mconnect_1             |[0m 	retry.backoff.ms = 100
[33;1mconnect_1             |[0m 	sasl.jaas.config = null
[33;1mconnect_1             |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[33;1mconnect_1             |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[33;1mconnect_1             |[0m 	sasl.kerberos.service.name = null
[33;1mconnect_1             |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[33;1mconnect_1             |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[33;1mconnect_1             |[0m 	sasl.mechanism = GSSAPI
[33;1mconnect_1             |[0m 	security.protocol = PLAINTEXT
[33;1mconnect_1             |[0m 	send.buffer.bytes = 131072
[33;1mconnect_1             |[0m 	ssl.cipher.suites = null
[33;1mconnect_1             |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[33;1mconnect_1             |[0m 	ssl.endpoint.identification.algorithm = null
[33;1mconnect_1             |[0m 	ssl.key.password = null
[33;1mconnect_1             |[0m 	ssl.keymanager.algorithm = SunX509
[33;1mconnect_1             |[0m 	ssl.keystore.location = null
[33;1mconnect_1             |[0m 	ssl.keystore.password = null
[33;1mconnect_1             |[0m 	ssl.keystore.type = JKS
[33;1mconnect_1             |[0m 	ssl.protocol = TLS
[33;1mconnect_1             |[0m 	ssl.provider = null
[33;1mconnect_1             |[0m 	ssl.secure.random.implementation = null
[33;1mconnect_1             |[0m 	ssl.trustmanager.algorithm = PKIX
[33;1mconnect_1             |[0m 	ssl.truststore.location = null
[33;1mconnect_1             |[0m 	ssl.truststore.password = null
[33;1mconnect_1             |[0m 	ssl.truststore.type = JKS
[33;1mconnect_1             |[0m  (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,396] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,396] WARN The configuration 'replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,396] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,396] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,396] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,396] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,397] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,397] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,398] WARN The configuration 'internal.key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,398] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,398] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,398] WARN The configuration 'internal.key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,398] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,398] WARN The configuration 'internal.value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,398] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,398] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,399] WARN The configuration 'internal.value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,399] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,402] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,402] WARN The configuration 'log4j.root.loglevel' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,402] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,402] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36;1mksql-datagen-users_1  |[0m 122 --> ([ 122 | 5 | 2 | 6750 | 1508909159435 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 123 --> ([ 123 | 16 | 3 | 9482 | 1508909159456 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 124 --> ([ 124 | 19 | 1 | 6249 | 1508909159459 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[33mzookeeper_1           |[0m [2017-10-25 05:25:59,506] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:setData cxid:0x2b5 zxid:0x16b txntype:-1 reqpath:n/a Error Path:/config/topics/connect-config Error:KeeperErrorCode = NoNode for /config/topics/connect-config (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:25:59,521] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x2b6 zxid:0x16c txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:59,558] INFO Topic creation {"version":1,"partitions":{"0":[1]}} (kafka.admin.AdminUtils$)
[36;1mksql-datagen-users_1  |[0m 125 --> ([ 125 | 9 | 4 | 5186 | 1508909159564 | 'android' | 'is this as good as it gets? really ?' ])
[35mkafka_1               |[0m [2017-10-25 05:25:59,573] INFO [Controller 1]: New topics: [Set(connect-config)], deleted topics: [Set()], new partition replica assignment [Map([connect-config,0] -> List(1))] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:59,574] INFO [Controller 1]: New topic creation callback for [connect-config,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:59,576] INFO [Controller 1]: New partition creation callback for [connect-config,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:25:59,576] INFO [Partition state machine on Controller 1]: Invoking state change to NewPartition for partitions [connect-config,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:59,577] TRACE Controller 1 epoch 1 changed partition [connect-config,0] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,577] INFO [Replica state machine on controller 1]: Invoking state change to NewReplica for replicas [Topic=connect-config,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:59,580] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-config,0] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,580] INFO [Partition state machine on Controller 1]: Invoking state change to OnlinePartition for partitions [connect-config,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:59,580] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [connect-config,0] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:59,580] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [connect-config,0] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:25:59,581] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x2be zxid:0x16f txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-config/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/connect-config/partitions/0 (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:25:59,596] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x2bf zxid:0x170 txntype:-1 reqpath:n/a Error Path:/brokers/topics/connect-config/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/connect-config/partitions (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:25:59,643] TRACE Controller 1 epoch 1 changed partition [connect-config,0] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,643] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [connect-config,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,645] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition connect-config-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,645] INFO [Replica state machine on controller 1]: Invoking state change to OnlineReplica for replicas [Topic=connect-config,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:25:59,645] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 13 from controller 1 epoch 1 for partition [connect-config,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,645] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [connect-config,0] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,646] TRACE Broker 1 handling LeaderAndIsr request correlationId 13 from controller 1 epoch 1 starting the become-leader transition for partition connect-config-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,646] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions connect-config-0 (kafka.server.ReplicaFetcherManager)
[35mkafka_1               |[0m [2017-10-25 05:25:59,723] INFO Loading producer state from offset 0 for partition connect-config-0 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:59,723] INFO Completed load of log connect-config-0 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:25:59,724] INFO Created log for partition [connect-config,0] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:25:59,725] INFO Partition [connect-config,0] on broker 1: No checkpointed highwatermark is found for partition connect-config-0 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:59,725] INFO Partition [connect-config,0] on broker 1: connect-config-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:25:59,725] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 13 for partition connect-config-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,725] TRACE Broker 1 completed LeaderAndIsr request correlationId 13 from controller 1 epoch 1 for the become-leader transition for partition connect-config-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,727] TRACE Controller 1 epoch 1 received response {error_code=0,partitions=[{topic=connect-config,partition=0,error_code=0}]} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,729] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition connect-config-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 14 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:25:59,730] TRACE Controller 1 epoch 1 received response {error_code=0} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,731] INFO Created topic (name=connect-config, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at kafka:29092 (org.apache.kafka.connect.util.TopicAdmin)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,733] INFO ProducerConfig values: 
[33;1mconnect_1             |[0m 	acks = all
[33;1mconnect_1             |[0m 	batch.size = 16384
[33;1mconnect_1             |[0m 	bootstrap.servers = [kafka:29092]
[33;1mconnect_1             |[0m 	buffer.memory = 33554432
[33;1mconnect_1             |[0m 	client.id = 
[33;1mconnect_1             |[0m 	compression.type = none
[33;1mconnect_1             |[0m 	connections.max.idle.ms = 540000
[33;1mconnect_1             |[0m 	enable.idempotence = false
[33;1mconnect_1             |[0m 	interceptor.classes = null
[33;1mconnect_1             |[0m 	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
[33;1mconnect_1             |[0m 	linger.ms = 0
[33;1mconnect_1             |[0m 	max.block.ms = 60000
[33;1mconnect_1             |[0m 	max.in.flight.requests.per.connection = 1
[33;1mconnect_1             |[0m 	max.request.size = 1048576
[33;1mconnect_1             |[0m 	metadata.max.age.ms = 300000
[33;1mconnect_1             |[0m 	metric.reporters = []
[33;1mconnect_1             |[0m 	metrics.num.samples = 2
[33;1mconnect_1             |[0m 	metrics.recording.level = INFO
[33;1mconnect_1             |[0m 	metrics.sample.window.ms = 30000
[33;1mconnect_1             |[0m 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
[33;1mconnect_1             |[0m 	receive.buffer.bytes = 32768
[33;1mconnect_1             |[0m 	reconnect.backoff.max.ms = 1000
[33;1mconnect_1             |[0m 	reconnect.backoff.ms = 50
[33;1mconnect_1             |[0m 	request.timeout.ms = 30000
[33;1mconnect_1             |[0m 	retries = 2147483647
[33;1mconnect_1             |[0m 	retry.backoff.ms = 100
[33;1mconnect_1             |[0m 	sasl.jaas.config = null
[33;1mconnect_1             |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[33;1mconnect_1             |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[33;1mconnect_1             |[0m 	sasl.kerberos.service.name = null
[33;1mconnect_1             |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[33;1mconnect_1             |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[33;1mconnect_1             |[0m 	sasl.mechanism = GSSAPI
[33;1mconnect_1             |[0m 	security.protocol = PLAINTEXT
[33;1mconnect_1             |[0m 	send.buffer.bytes = 131072
[33;1mconnect_1             |[0m 	ssl.cipher.suites = null
[33;1mconnect_1             |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[33;1mconnect_1             |[0m 	ssl.endpoint.identification.algorithm = null
[33;1mconnect_1             |[0m 	ssl.key.password = null
[33;1mconnect_1             |[0m 	ssl.keymanager.algorithm = SunX509
[33;1mconnect_1             |[0m 	ssl.keystore.location = null
[33;1mconnect_1             |[0m 	ssl.keystore.password = null
[33;1mconnect_1             |[0m 	ssl.keystore.type = JKS
[33;1mconnect_1             |[0m 	ssl.protocol = TLS
[33;1mconnect_1             |[0m 	ssl.provider = null
[33;1mconnect_1             |[0m 	ssl.secure.random.implementation = null
[33;1mconnect_1             |[0m 	ssl.trustmanager.algorithm = PKIX
[33;1mconnect_1             |[0m 	ssl.truststore.location = null
[33;1mconnect_1             |[0m 	ssl.truststore.password = null
[33;1mconnect_1             |[0m 	ssl.truststore.type = JKS
[33;1mconnect_1             |[0m 	transaction.timeout.ms = 60000
[33;1mconnect_1             |[0m 	transactional.id = null
[33;1mconnect_1             |[0m 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[33;1mconnect_1             |[0m  (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,735] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,735] WARN The configuration 'replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,735] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,735] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,736] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,736] WARN The configuration 'internal.key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,736] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,736] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,736] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,736] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,736] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,736] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,736] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,736] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,736] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,736] WARN The configuration 'internal.key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,737] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,737] WARN The configuration 'internal.value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,737] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,737] WARN The configuration 'internal.value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,737] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,737] WARN The configuration 'log4j.root.loglevel' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,737] INFO Kafka version : 0.11.0.0-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,737] INFO Kafka commitId : 6a8cf706ddc9ab6a (org.apache.kafka.common.utils.AppInfoParser)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,738] INFO ConsumerConfig values: 
[33;1mconnect_1             |[0m 	auto.commit.interval.ms = 5000
[33;1mconnect_1             |[0m 	auto.offset.reset = earliest
[33;1mconnect_1             |[0m 	bootstrap.servers = [kafka:29092]
[33;1mconnect_1             |[0m 	check.crcs = true
[33;1mconnect_1             |[0m 	client.id = 
[33;1mconnect_1             |[0m 	connections.max.idle.ms = 540000
[33;1mconnect_1             |[0m 	enable.auto.commit = false
[33;1mconnect_1             |[0m 	exclude.internal.topics = true
[33;1mconnect_1             |[0m 	fetch.max.bytes = 52428800
[33;1mconnect_1             |[0m 	fetch.max.wait.ms = 500
[33;1mconnect_1             |[0m 	fetch.min.bytes = 1
[33;1mconnect_1             |[0m 	group.id = connect
[33;1mconnect_1             |[0m 	heartbeat.interval.ms = 3000
[33;1mconnect_1             |[0m 	interceptor.classes = null
[33;1mconnect_1             |[0m 	internal.leave.group.on.close = true
[33;1mconnect_1             |[0m 	isolation.level = read_uncommitted
[33;1mconnect_1             |[0m 	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
[33;1mconnect_1             |[0m 	max.partition.fetch.bytes = 1048576
[33;1mconnect_1             |[0m 	max.poll.interval.ms = 300000
[33;1mconnect_1             |[0m 	max.poll.records = 500
[33;1mconnect_1             |[0m 	metadata.max.age.ms = 300000
[33;1mconnect_1             |[0m 	metric.reporters = []
[33;1mconnect_1             |[0m 	metrics.num.samples = 2
[33;1mconnect_1             |[0m 	metrics.recording.level = INFO
[33;1mconnect_1             |[0m 	metrics.sample.window.ms = 30000
[33;1mconnect_1             |[0m 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
[33;1mconnect_1             |[0m 	receive.buffer.bytes = 65536
[33;1mconnect_1             |[0m 	reconnect.backoff.max.ms = 1000
[33;1mconnect_1             |[0m 	reconnect.backoff.ms = 50
[33;1mconnect_1             |[0m 	request.timeout.ms = 305000
[33;1mconnect_1             |[0m 	retry.backoff.ms = 100
[33;1mconnect_1             |[0m 	sasl.jaas.config = null
[33;1mconnect_1             |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[33;1mconnect_1             |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[33;1mconnect_1             |[0m 	sasl.kerberos.service.name = null
[33;1mconnect_1             |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[33;1mconnect_1             |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[33;1mconnect_1             |[0m 	sasl.mechanism = GSSAPI
[33;1mconnect_1             |[0m 	security.protocol = PLAINTEXT
[33;1mconnect_1             |[0m 	send.buffer.bytes = 131072
[33;1mconnect_1             |[0m 	session.timeout.ms = 10000
[33;1mconnect_1             |[0m 	ssl.cipher.suites = null
[33;1mconnect_1             |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[33;1mconnect_1             |[0m 	ssl.endpoint.identification.algorithm = null
[33;1mconnect_1             |[0m 	ssl.key.password = null
[33;1mconnect_1             |[0m 	ssl.keymanager.algorithm = SunX509
[33;1mconnect_1             |[0m 	ssl.keystore.location = null
[33;1mconnect_1             |[0m 	ssl.keystore.password = null
[33;1mconnect_1             |[0m 	ssl.keystore.type = JKS
[33;1mconnect_1             |[0m 	ssl.protocol = TLS
[33;1mconnect_1             |[0m 	ssl.provider = null
[33;1mconnect_1             |[0m 	ssl.secure.random.implementation = null
[33;1mconnect_1             |[0m 	ssl.trustmanager.algorithm = PKIX
[33;1mconnect_1             |[0m 	ssl.truststore.location = null
[33;1mconnect_1             |[0m 	ssl.truststore.password = null
[33;1mconnect_1             |[0m 	ssl.truststore.type = JKS
[33;1mconnect_1             |[0m 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[33;1mconnect_1             |[0m  (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,743] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,743] WARN The configuration 'replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,743] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,743] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,743] WARN The configuration 'internal.key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,743] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,743] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,743] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,743] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,743] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,743] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,743] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,743] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,743] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,743] WARN The configuration 'internal.key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,743] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,743] WARN The configuration 'internal.value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,743] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,743] WARN The configuration 'internal.value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,743] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,744] WARN The configuration 'log4j.root.loglevel' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,744] INFO Kafka version : 0.11.0.0-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,744] INFO Kafka commitId : 6a8cf706ddc9ab6a (org.apache.kafka.common.utils.AppInfoParser)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,793] INFO Discovered coordinator kafka:29092 (id: 2147483646 rack: null) for group connect. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[36;1mksql-datagen-users_1  |[0m 126 --> ([ 126 | 0 | 3 | 6907 | 1508909159793 | 'android' | '(expletive deleted)' ])
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,802] INFO Finished reading KafkaBasedLog for topic connect-config (org.apache.kafka.connect.util.KafkaBasedLog)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,802] INFO Started KafkaBasedLog for topic connect-config (org.apache.kafka.connect.util.KafkaBasedLog)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,803] INFO Started KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,803] INFO Herder started (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,815] INFO Discovered coordinator kafka:29092 (id: 2147483646 rack: null) for group connect. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[33;1mconnect_1             |[0m [2017-10-25 05:25:59,819] INFO (Re-)joining group connect (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[35mkafka_1               |[0m [2017-10-25 05:25:59,833] INFO [GroupCoordinator 1]: Preparing to rebalance group connect with old generation 0 (__consumer_offsets-30) (kafka.coordinator.group.GroupCoordinator)
[36;1mksql-datagen-users_1  |[0m 127 --> ([ 127 | 2 | 3 | 6428 | 1508909159967 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 128 --> ([ 128 | 12 | 3 | 4646 | 1508909160057 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 129 --> ([ 129 | 8 | 2 | 5938 | 1508909160252 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 130 --> ([ 130 | 17 | 3 | 3303 | 1508909160459 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 131 --> ([ 131 | -1 | 1 | 1468 | 1508909160671 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 132 --> ([ 132 | 17 | 2 | 1355 | 1508909160733 | 'iOS' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 133 --> ([ 133 | 12 | 4 | 6425 | 1508909160784 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 134 --> ([ 134 | 2 | 3 | 2527 | 1508909161008 | 'iOS-test' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 135 --> ([ 135 | 13 | 2 | 8091 | 1508909161290 | 'android' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 136 --> ([ 136 | 6 | 1 | 805 | 1508909161549 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 137 --> ([ 137 | 2 | 4 | 173 | 1508909161786 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 138 --> ([ 138 | 4 | 1 | 2843 | 1508909161790 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 139 --> ([ 139 | 1 | 4 | 8948 | 1508909161974 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 140 --> ([ 140 | 1 | 1 | 3298 | 1508909162172 | 'android' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 141 --> ([ 141 | 3 | 1 | 4785 | 1508909162378 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 142 --> ([ 142 | 16 | 1 | 1072 | 1508909162517 | 'web' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 143 --> ([ 143 | 3 | 3 | 3245 | 1508909162713 | 'web' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[35mkafka_1               |[0m [2017-10-25 05:26:02,843] INFO [GroupCoordinator 1]: Stabilized group connect generation 1 (__consumer_offsets-30) (kafka.coordinator.group.GroupCoordinator)
[36;1mksql-datagen-users_1  |[0m 144 --> ([ 144 | 4 | 2 | 6990 | 1508909162847 | 'iOS' | '(expletive deleted)' ])
[35mkafka_1               |[0m [2017-10-25 05:26:02,859] INFO [GroupCoordinator 1]: Assignment received from leader for group connect for generation 1 (kafka.coordinator.group.GroupCoordinator)
[35mkafka_1               |[0m [2017-10-25 05:26:02,866] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-30. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[33;1mconnect_1             |[0m [2017-10-25 05:26:02,896] INFO Successfully joined group connect with generation 1 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[33;1mconnect_1             |[0m [2017-10-25 05:26:02,898] INFO Joined group and got assignment: Assignment{error=0, leader='connect-1-0982ca12-95de-406d-ae51-d169a88870a4', leaderUrl='http://connect:8083/', offset=-1, connectorIds=[], taskIds=[]} (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[33;1mconnect_1             |[0m [2017-10-25 05:26:02,899] INFO Starting connectors and tasks using config offset -1 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[33;1mconnect_1             |[0m [2017-10-25 05:26:02,899] INFO Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[36;1mksql-datagen-users_1  |[0m 145 --> ([ 145 | 12 | 2 | 2579 | 1508909163051 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 146 --> ([ 146 | 16 | 4 | 5919 | 1508909163078 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 147 --> ([ 147 | -1 | 4 | 6841 | 1508909163353 | 'iOS' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 148 --> ([ 148 | 11 | 4 | 8816 | 1508909163642 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 149 --> ([ 149 | 12 | 3 | 6392 | 1508909163837 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 150 --> ([ 150 | 1 | 1 | 2752 | 1508909164124 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 151 --> ([ 151 | 5 | 3 | 46 | 1508909164299 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 152 --> ([ 152 | 17 | 1 | 9365 | 1508909164559 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 153 --> ([ 153 | 14 | 3 | 5609 | 1508909164845 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 154 --> ([ 154 | 10 | 4 | 4818 | 1508909164991 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 155 --> ([ 155 | 1 | 4 | 8099 | 1508909165019 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 156 --> ([ 156 | 0 | 2 | 6693 | 1508909165128 | 'web' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 157 --> ([ 157 | 11 | 1 | 7360 | 1508909165175 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 158 --> ([ 158 | 11 | 2 | 5907 | 1508909165303 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 159 --> ([ 159 | 0 | 2 | 9362 | 1508909165508 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 160 --> ([ 160 | 10 | 4 | 4089 | 1508909165808 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 161 --> ([ 161 | 3 | 1 | 7936 | 1508909165864 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 162 --> ([ 162 | 13 | 3 | 9469 | 1508909166080 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 163 --> ([ 163 | 0 | 3 | 6191 | 1508909166163 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 164 --> ([ 164 | 17 | 4 | 6781 | 1508909166279 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 165 --> ([ 165 | 1 | 1 | 9384 | 1508909166436 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 166 --> ([ 166 | 2 | 2 | 9162 | 1508909166500 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 167 --> ([ 167 | 17 | 4 | 3506 | 1508909166786 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 168 --> ([ 168 | 10 | 2 | 2595 | 1508909166889 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 169 --> ([ 169 | 14 | 2 | 6305 | 1508909167059 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 170 --> ([ 170 | 12 | 2 | 5859 | 1508909167331 | 'android' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 171 --> ([ 171 | 14 | 4 | 68 | 1508909167497 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 172 --> ([ 172 | 8 | 4 | 1545 | 1508909167722 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 173 --> ([ 173 | 12 | 1 | 7842 | 1508909167936 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 174 --> ([ 174 | 19 | 1 | 6088 | 1508909168221 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 175 --> ([ 175 | 17 | 3 | 7434 | 1508909168412 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 176 --> ([ 176 | 9 | 3 | 9381 | 1508909168684 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 177 --> ([ 177 | 13 | 3 | 9870 | 1508909168881 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 178 --> ([ 178 | 19 | 4 | 6931 | 1508909168896 | 'web' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 179 --> ([ 179 | 13 | 1 | 1631 | 1508909169085 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 180 --> ([ 180 | 9 | 4 | 87 | 1508909169100 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 181 --> ([ 181 | 0 | 4 | 2381 | 1508909169200 | 'android' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 182 --> ([ 182 | 14 | 1 | 2356 | 1508909169314 | 'android' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 183 --> ([ 183 | 17 | 2 | 2489 | 1508909169598 | 'iOS' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 184 --> ([ 184 | 11 | 2 | 2296 | 1508909169799 | 'android' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 185 --> ([ 185 | 10 | 1 | 7050 | 1508909170051 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 186 --> ([ 186 | 0 | 2 | 6000 | 1508909170138 | 'android' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 187 --> ([ 187 | 12 | 3 | 5710 | 1508909170418 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 188 --> ([ 188 | 15 | 4 | 4688 | 1508909170461 | 'web' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 189 --> ([ 189 | 5 | 4 | 22 | 1508909170530 | 'ios' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 190 --> ([ 190 | 3 | 1 | 5399 | 1508909170667 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 191 --> ([ 191 | 11 | 4 | 2605 | 1508909170864 | 'iOS' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 192 --> ([ 192 | 18 | 2 | 5308 | 1508909170877 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 193 --> ([ 193 | 2 | 1 | 4865 | 1508909170916 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 194 --> ([ 194 | 9 | 4 | 2659 | 1508909170930 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 195 --> ([ 195 | 2 | 1 | 2531 | 1508909171194 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 196 --> ([ 196 | 1 | 2 | 9835 | 1508909171206 | 'web' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 197 --> ([ 197 | 7 | 2 | 750 | 1508909171501 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 198 --> ([ 198 | 1 | 4 | 6572 | 1508909171666 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 199 --> ([ 199 | -1 | 2 | 3035 | 1508909171724 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 200 --> ([ 200 | 2 | 2 | 3804 | 1508909172006 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 201 --> ([ 201 | 14 | 4 | 790 | 1508909172092 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 202 --> ([ 202 | -1 | 1 | 1186 | 1508909172328 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 203 --> ([ 203 | 16 | 4 | 2680 | 1508909172559 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 204 --> ([ 204 | 12 | 2 | 3759 | 1508909172833 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 205 --> ([ 205 | 1 | 3 | 5611 | 1508909173122 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 206 --> ([ 206 | 16 | 1 | 5804 | 1508909173352 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 207 --> ([ 207 | 1 | 2 | 1573 | 1508909173402 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 208 --> ([ 208 | 1 | 3 | 2365 | 1508909173533 | 'ios' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 209 --> ([ 209 | 10 | 2 | 7093 | 1508909173666 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 210 --> ([ 210 | 15 | 2 | 1452 | 1508909173949 | 'android' | 'is this as good as it gets? really ?' ])
[35mkafka_1               |[0m [2017-10-25 05:26:14,106] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: _confluent-metrics-4. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[36;1mksql-datagen-users_1  |[0m 211 --> ([ 211 | 9 | 1 | 4585 | 1508909174150 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 212 --> ([ 212 | 19 | 3 | 2458 | 1508909174298 | 'iOS' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 213 --> ([ 213 | 2 | 1 | 7313 | 1508909174479 | 'web' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 214 --> ([ 214 | -1 | 3 | 8761 | 1508909174525 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 215 --> ([ 215 | 15 | 4 | 2270 | 1508909174649 | 'web' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 216 --> ([ 216 | 0 | 3 | 5046 | 1508909174715 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 217 --> ([ 217 | 8 | 3 | 7688 | 1508909174921 | 'web' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 218 --> ([ 218 | 11 | 1 | 2332 | 1508909174975 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 219 --> ([ 219 | 16 | 1 | 4478 | 1508909175067 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 220 --> ([ 220 | 4 | 4 | 6244 | 1508909175322 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 221 --> ([ 221 | 8 | 4 | 3312 | 1508909175398 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 222 --> ([ 222 | 15 | 4 | 6505 | 1508909175625 | 'ios' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 223 --> ([ 223 | 18 | 3 | 1659 | 1508909175625 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 224 --> ([ 224 | 18 | 4 | 8169 | 1508909175682 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 225 --> ([ 225 | 1 | 1 | 7478 | 1508909175791 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 226 --> ([ 226 | 10 | 1 | 7611 | 1508909175808 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 227 --> ([ 227 | 1 | 4 | 8869 | 1508909175988 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 228 --> ([ 228 | 19 | 1 | 8768 | 1508909176144 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 229 --> ([ 229 | 19 | 2 | 1415 | 1508909176334 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 230 --> ([ 230 | 16 | 4 | 7646 | 1508909176630 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 231 --> ([ 231 | 19 | 3 | 7471 | 1508909176899 | 'iOS-test' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 232 --> ([ 232 | 19 | 4 | 5600 | 1508909177168 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 233 --> ([ 233 | 16 | 4 | 4197 | 1508909177458 | 'iOS' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 234 --> ([ 234 | 1 | 4 | 4395 | 1508909177673 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 235 --> ([ 235 | 4 | 3 | 9521 | 1508909177837 | 'web' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 236 --> ([ 236 | -1 | 2 | 8554 | 1508909177913 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 237 --> ([ 237 | 3 | 3 | 7870 | 1508909178172 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 238 --> ([ 238 | -1 | 4 | 3007 | 1508909178209 | 'android' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 239 --> ([ 239 | 6 | 3 | 9862 | 1508909178503 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 240 --> ([ 240 | 8 | 2 | 6529 | 1508909178739 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 241 --> ([ 241 | 13 | 1 | 2760 | 1508909179019 | 'android' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 242 --> ([ 242 | 0 | 4 | 4011 | 1508909179091 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 243 --> ([ 243 | 2 | 4 | 911 | 1508909179268 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 244 --> ([ 244 | 6 | 3 | 3035 | 1508909179285 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 245 --> ([ 245 | 12 | 3 | 7971 | 1508909179577 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 246 --> ([ 246 | 2 | 1 | 2778 | 1508909179616 | 'iOS-test' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 247 --> ([ 247 | 15 | 4 | 5129 | 1508909179863 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 248 --> ([ 248 | 17 | 1 | 5761 | 1508909179993 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[32mmysql-db_1            |[0m 2017-10-25T05:26:20.186220Z 3 [Note] Access denied for user 'root'@'172.21.0.6' (using password: NO)
[31mmysql-admin_1         |[0m [Wed Oct 25 05:26:20 2017] ::ffff:172.21.0.1:53416 [403]: /?server=mysql-db&username=root&db=flights
[36;1mksql-datagen-users_1  |[0m 249 --> ([ 249 | 7 | 1 | 2107 | 1508909180201 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 250 --> ([ 250 | 17 | 2 | 5426 | 1508909180424 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 251 --> ([ 251 | 2 | 2 | 207 | 1508909180584 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 252 --> ([ 252 | 11 | 1 | 5079 | 1508909180628 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 253 --> ([ 253 | 15 | 2 | 9144 | 1508909180666 | 'web' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 254 --> ([ 254 | 18 | 2 | 6177 | 1508909180743 | 'android' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 255 --> ([ 255 | -1 | 2 | 1929 | 1508909180879 | 'iOS' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 256 --> ([ 256 | 4 | 2 | 6493 | 1508909180900 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 257 --> ([ 257 | 4 | 2 | 4549 | 1508909181119 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 258 --> ([ 258 | 9 | 2 | 1151 | 1508909181228 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 259 --> ([ 259 | 18 | 1 | 9738 | 1508909181474 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 260 --> ([ 260 | 19 | 2 | 2035 | 1508909181492 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 261 --> ([ 261 | 4 | 4 | 6547 | 1508909181654 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 262 --> ([ 262 | 2 | 4 | 3607 | 1508909181902 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 263 --> ([ 263 | 11 | 4 | 5 | 1508909182125 | 'web' | 'your team here rocks!' ])
[31mmysql-admin_1         |[0m [Wed Oct 25 05:26:22 2017] ::ffff:172.21.0.1:53418 [200]: /?file=favicon.ico&version=4.3.1
[36;1mksql-datagen-users_1  |[0m 264 --> ([ 264 | 12 | 2 | 5602 | 1508909182201 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 265 --> ([ 265 | 8 | 3 | 7616 | 1508909182389 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 266 --> ([ 266 | 1 | 4 | 5642 | 1508909182599 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 267 --> ([ 267 | 7 | 1 | 1969 | 1508909182774 | 'web' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 268 --> ([ 268 | 10 | 4 | 7252 | 1508909182802 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 269 --> ([ 269 | 19 | 4 | 1166 | 1508909183097 | 'android' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 270 --> ([ 270 | 8 | 1 | 1313 | 1508909183332 | 'android' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[31mmysql-admin_1         |[0m [Wed Oct 25 05:26:23 2017] ::ffff:172.21.0.1:53422 [302]: /?server=mysql-db&username=root&db=flights
[31mmysql-admin_1         |[0m [Wed Oct 25 05:26:23 2017] ::ffff:172.21.0.1:53424 [200]: /?server=mysql-db&username=root&db=flights
[36;1mksql-datagen-users_1  |[0m 271 --> ([ 271 | 4 | 1 | 1404 | 1508909183467 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 272 --> ([ 272 | 18 | 3 | 1425 | 1508909183618 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 273 --> ([ 273 | 6 | 1 | 8400 | 1508909183698 | 'iOS' | 'is this as good as it gets? really ?' ])
[31mmysql-admin_1         |[0m [Wed Oct 25 05:26:23 2017] ::ffff:172.21.0.1:53430 [200]: /?server=mysql-db&username=root&db=flights&script=db
[36;1mksql-datagen-users_1  |[0m 274 --> ([ 274 | 8 | 3 | 6222 | 1508909183819 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 275 --> ([ 275 | 4 | 4 | 6958 | 1508909184078 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 276 --> ([ 276 | 13 | 1 | 4124 | 1508909184270 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 277 --> ([ 277 | 18 | 3 | 407 | 1508909184474 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 278 --> ([ 278 | 18 | 2 | 8540 | 1508909184615 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 279 --> ([ 279 | 0 | 2 | 4198 | 1508909184871 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 280 --> ([ 280 | -1 | 3 | 4040 | 1508909184891 | 'android' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 281 --> ([ 281 | 10 | 2 | 401 | 1508909185110 | 'android' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 282 --> ([ 282 | 17 | 2 | 6478 | 1508909185386 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 283 --> ([ 283 | 2 | 1 | 2345 | 1508909185650 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 284 --> ([ 284 | 14 | 1 | 2615 | 1508909185730 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 285 --> ([ 285 | 18 | 4 | 1199 | 1508909185858 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 286 --> ([ 286 | 19 | 4 | 7197 | 1508909186047 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 287 --> ([ 287 | 8 | 1 | 8705 | 1508909186242 | 'web' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 288 --> ([ 288 | 4 | 4 | 8055 | 1508909186332 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 289 --> ([ 289 | 6 | 1 | 2868 | 1508909186447 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 290 --> ([ 290 | 5 | 1 | 2175 | 1508909186634 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 291 --> ([ 291 | 18 | 2 | 5807 | 1508909186731 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 292 --> ([ 292 | 18 | 2 | 8294 | 1508909186900 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 293 --> ([ 293 | 16 | 1 | 5581 | 1508909186922 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 294 --> ([ 294 | 18 | 1 | 4007 | 1508909187007 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 295 --> ([ 295 | 13 | 4 | 6210 | 1508909187143 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 296 --> ([ 296 | 11 | 3 | 7889 | 1508909187226 | 'iOS-test' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 297 --> ([ 297 | 14 | 4 | 2658 | 1508909187350 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 298 --> ([ 298 | 19 | 3 | 9659 | 1508909187591 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 299 --> ([ 299 | 3 | 3 | 40 | 1508909187860 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 300 --> ([ 300 | 12 | 3 | 3875 | 1508909187872 | 'web' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 301 --> ([ 301 | 16 | 1 | 8807 | 1508909187876 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 302 --> ([ 302 | 4 | 3 | 5169 | 1508909188109 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 303 --> ([ 303 | 10 | 2 | 184 | 1508909188109 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 304 --> ([ 304 | 18 | 1 | 5554 | 1508909188306 | 'web' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 305 --> ([ 305 | 19 | 3 | 8567 | 1508909188534 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 306 --> ([ 306 | 11 | 2 | 6775 | 1508909188626 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 307 --> ([ 307 | 0 | 2 | 220 | 1508909188832 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 308 --> ([ 308 | 15 | 1 | 3239 | 1508909189032 | 'android' | 'worst. flight. ever. #neveragain' ])
[35mkafka_1               |[0m [2017-10-25 05:26:29,110] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: _confluent-metrics-7. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[36;1mksql-datagen-users_1  |[0m 309 --> ([ 309 | 19 | 4 | 9933 | 1508909189192 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 310 --> ([ 310 | 9 | 4 | 3517 | 1508909189267 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 311 --> ([ 311 | 3 | 2 | 4 | 1508909189323 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 312 --> ([ 312 | 14 | 2 | 937 | 1508909189570 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 313 --> ([ 313 | 2 | 4 | 8187 | 1508909189598 | 'iOS' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 314 --> ([ 314 | 13 | 3 | 6260 | 1508909189825 | 'web' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 315 --> ([ 315 | 6 | 3 | 3982 | 1508909189877 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 316 --> ([ 316 | 13 | 2 | 7975 | 1508909190119 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 317 --> ([ 317 | 3 | 1 | 4919 | 1508909190415 | 'web' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 318 --> ([ 318 | 1 | 3 | 8177 | 1508909190463 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 319 --> ([ 319 | 7 | 4 | 7177 | 1508909190522 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 320 --> ([ 320 | 14 | 2 | 4585 | 1508909190793 | 'android' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 321 --> ([ 321 | 11 | 2 | 7243 | 1508909190900 | 'iOS' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 322 --> ([ 322 | 7 | 3 | 1669 | 1508909191082 | 'web' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 323 --> ([ 323 | 12 | 4 | 3887 | 1508909191297 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 324 --> ([ 324 | 16 | 2 | 6413 | 1508909191489 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 325 --> ([ 325 | 6 | 2 | 2218 | 1508909191574 | 'iOS-test' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 326 --> ([ 326 | -1 | 4 | 6728 | 1508909191796 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 327 --> ([ 327 | 18 | 4 | 4063 | 1508909191954 | 'android' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 328 --> ([ 328 | 3 | 4 | 2992 | 1508909192000 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 329 --> ([ 329 | 8 | 4 | 1445 | 1508909192001 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 330 --> ([ 330 | 1 | 1 | 5764 | 1508909192165 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 331 --> ([ 331 | -1 | 4 | 2394 | 1508909192174 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 332 --> ([ 332 | -1 | 3 | 5152 | 1508909192288 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 333 --> ([ 333 | 8 | 4 | 879 | 1508909192331 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 334 --> ([ 334 | 15 | 1 | 9852 | 1508909192495 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 335 --> ([ 335 | 2 | 2 | 5085 | 1508909192709 | 'web' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 336 --> ([ 336 | -1 | 2 | 6374 | 1508909192961 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 337 --> ([ 337 | 15 | 3 | 828 | 1508909192985 | 'iOS' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 338 --> ([ 338 | 17 | 2 | 1443 | 1508909193272 | 'iOS' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 339 --> ([ 339 | 14 | 1 | 9326 | 1508909193568 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 340 --> ([ 340 | 0 | 1 | 3098 | 1508909193790 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 341 --> ([ 341 | 8 | 2 | 1140 | 1508909193806 | 'ios' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 342 --> ([ 342 | 11 | 1 | 8504 | 1508909193902 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 343 --> ([ 343 | 2 | 3 | 463 | 1508909194183 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 344 --> ([ 344 | 10 | 3 | 8959 | 1508909194367 | 'iOS' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 345 --> ([ 345 | 18 | 4 | 6602 | 1508909194508 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 346 --> ([ 346 | 9 | 2 | 5128 | 1508909194634 | 'android' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 347 --> ([ 347 | 7 | 3 | 1851 | 1508909194805 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 348 --> ([ 348 | 6 | 2 | 7995 | 1508909194990 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 349 --> ([ 349 | 19 | 1 | 7520 | 1508909195260 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 350 --> ([ 350 | 3 | 4 | 3561 | 1508909195543 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 351 --> ([ 351 | 14 | 3 | 2422 | 1508909195797 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 352 --> ([ 352 | 1 | 4 | 1770 | 1508909196062 | 'ios' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 353 --> ([ 353 | 12 | 3 | 502 | 1508909196158 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 354 --> ([ 354 | 10 | 4 | 2692 | 1508909196322 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 355 --> ([ 355 | -1 | 2 | 6936 | 1508909196337 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 356 --> ([ 356 | 15 | 4 | 5462 | 1508909196598 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 357 --> ([ 357 | 11 | 4 | 4424 | 1508909196813 | 'ios' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 358 --> ([ 358 | 4 | 2 | 7982 | 1508909196896 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 359 --> ([ 359 | 16 | 4 | 3925 | 1508909196999 | 'iOS-test' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 360 --> ([ 360 | 9 | 2 | 7897 | 1508909197274 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 361 --> ([ 361 | 3 | 3 | 6618 | 1508909197343 | 'android' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 362 --> ([ 362 | 14 | 4 | 7847 | 1508909197513 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 363 --> ([ 363 | 18 | 2 | 6338 | 1508909197682 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 364 --> ([ 364 | 18 | 3 | 519 | 1508909197867 | 'web' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 365 --> ([ 365 | 18 | 1 | 4226 | 1508909197878 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 366 --> ([ 366 | 13 | 2 | 1472 | 1508909198153 | 'web' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 367 --> ([ 367 | 3 | 3 | 9883 | 1508909198200 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 368 --> ([ 368 | 15 | 1 | 301 | 1508909198335 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 369 --> ([ 369 | 2 | 1 | 5762 | 1508909198550 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 370 --> ([ 370 | 13 | 2 | 3076 | 1508909198681 | 'android' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 371 --> ([ 371 | 2 | 2 | 4060 | 1508909198885 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 372 --> ([ 372 | 12 | 3 | 1624 | 1508909199050 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 373 --> ([ 373 | 9 | 4 | 7874 | 1508909199284 | 'web' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 374 --> ([ 374 | 9 | 3 | 7630 | 1508909199402 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 375 --> ([ 375 | 8 | 4 | 886 | 1508909199496 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 376 --> ([ 376 | 6 | 4 | 2077 | 1508909199533 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 377 --> ([ 377 | 11 | 1 | 2296 | 1508909199618 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 378 --> ([ 378 | 13 | 2 | 3016 | 1508909199756 | 'android' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 379 --> ([ 379 | 17 | 1 | 6325 | 1508909199971 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 380 --> ([ 380 | 10 | 2 | 7210 | 1508909199995 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 381 --> ([ 381 | 6 | 4 | 7986 | 1508909200179 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 382 --> ([ 382 | 18 | 1 | 3619 | 1508909200278 | 'web' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 383 --> ([ 383 | 0 | 3 | 9409 | 1508909200282 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 384 --> ([ 384 | 8 | 3 | 9247 | 1508909200319 | 'ios' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 385 --> ([ 385 | 13 | 2 | 3484 | 1508909200544 | 'iOS' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 386 --> ([ 386 | 16 | 3 | 387 | 1508909200750 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 387 --> ([ 387 | 3 | 4 | 4052 | 1508909200977 | 'android' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 388 --> ([ 388 | 3 | 4 | 4716 | 1508909201077 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 389 --> ([ 389 | 8 | 2 | 4438 | 1508909201173 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 390 --> ([ 390 | 10 | 1 | 2313 | 1508909201280 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 391 --> ([ 391 | 18 | 3 | 8553 | 1508909201413 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 392 --> ([ 392 | 8 | 3 | 6047 | 1508909201601 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 393 --> ([ 393 | 5 | 2 | 1342 | 1508909201876 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 394 --> ([ 394 | 12 | 4 | 4386 | 1508909202017 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 395 --> ([ 395 | 7 | 3 | 7817 | 1508909202250 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 396 --> ([ 396 | 1 | 3 | 2112 | 1508909202355 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 397 --> ([ 397 | 14 | 1 | 2466 | 1508909202518 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 398 --> ([ 398 | 16 | 1 | 6963 | 1508909202553 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 399 --> ([ 399 | 17 | 3 | 3247 | 1508909202709 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 400 --> ([ 400 | 16 | 3 | 2699 | 1508909202747 | 'iOS-test' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 401 --> ([ 401 | 4 | 1 | 6976 | 1508909202948 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 402 --> ([ 402 | 3 | 2 | 1607 | 1508909202980 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 403 --> ([ 403 | -1 | 3 | 5337 | 1508909203048 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 404 --> ([ 404 | 6 | 2 | 1945 | 1508909203055 | 'iOS-test' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 405 --> ([ 405 | 17 | 3 | 3261 | 1508909203317 | 'iOS' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 406 --> ([ 406 | 18 | 1 | 8352 | 1508909203448 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 407 --> ([ 407 | 15 | 3 | 3976 | 1508909203619 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 408 --> ([ 408 | 7 | 1 | 5703 | 1508909203819 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 409 --> ([ 409 | 3 | 1 | 7436 | 1508909203846 | 'android' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 410 --> ([ 410 | 13 | 2 | 9435 | 1508909203949 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 411 --> ([ 411 | 7 | 1 | 1426 | 1508909203952 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[35mkafka_1               |[0m [2017-10-25 05:26:44,108] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: _confluent-metrics-1. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[36;1mksql-datagen-users_1  |[0m 412 --> ([ 412 | 0 | 4 | 7863 | 1508909204112 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 413 --> ([ 413 | 4 | 3 | 7064 | 1508909204220 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 414 --> ([ 414 | 12 | 4 | 4454 | 1508909204371 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 415 --> ([ 415 | 10 | 1 | 3006 | 1508909204524 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 416 --> ([ 416 | 4 | 4 | 617 | 1508909204671 | 'web' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 417 --> ([ 417 | 4 | 4 | 5053 | 1508909204719 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 418 --> ([ 418 | 17 | 4 | 1975 | 1508909204750 | 'android' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 419 --> ([ 419 | 13 | 3 | 475 | 1508909204971 | 'android' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 420 --> ([ 420 | 11 | 1 | 3656 | 1508909205089 | 'web' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 421 --> ([ 421 | 16 | 1 | 8761 | 1508909205296 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 422 --> ([ 422 | 14 | 1 | 2592 | 1508909205546 | 'web' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 423 --> ([ 423 | 12 | 2 | 3098 | 1508909205573 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 424 --> ([ 424 | 14 | 1 | 7338 | 1508909205820 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 425 --> ([ 425 | 15 | 1 | 9290 | 1508909205839 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 426 --> ([ 426 | 2 | 4 | 8431 | 1508909206134 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 427 --> ([ 427 | 10 | 3 | 1422 | 1508909206311 | 'iOS' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 428 --> ([ 428 | 13 | 4 | 4779 | 1508909206415 | 'iOS' | 'meh' ])
[33;1mconnect_1             |[0m Wed Oct 25 05:26:46 UTC 2017 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
[36;1mksql-datagen-users_1  |[0m 429 --> ([ 429 | 10 | 2 | 5980 | 1508909206623 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 430 --> ([ 430 | 7 | 4 | 4094 | 1508909206650 | 'ios' | '(expletive deleted)' ])
[33;1mconnect_1             |[0m Wed Oct 25 05:26:46 UTC 2017 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
[36;1mksql-datagen-users_1  |[0m 431 --> ([ 431 | 13 | 3 | 8732 | 1508909206947 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[35mkafka_1               |[0m [2017-10-25 05:26:46,985] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: connect-config-0. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[33;1mconnect_1             |[0m [2017-10-25 05:26:47,021] INFO Connector MySQLSource config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[36;1mksql-datagen-users_1  |[0m 432 --> ([ 432 | 19 | 1 | 7893 | 1508909207113 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 433 --> ([ 433 | 7 | 1 | 1090 | 1508909207223 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 434 --> ([ 434 | 10 | 3 | 4046 | 1508909207239 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 435 --> ([ 435 | 15 | 1 | 5220 | 1508909207343 | 'android' | 'more peanuts please' ])
[33;1mconnect_1             |[0m [2017-10-25 05:26:47,520] INFO Rebalance started (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[33;1mconnect_1             |[0m [2017-10-25 05:26:47,520] INFO Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[33;1mconnect_1             |[0m [2017-10-25 05:26:47,521] INFO (Re-)joining group connect (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[35mkafka_1               |[0m [2017-10-25 05:26:47,523] INFO [GroupCoordinator 1]: Preparing to rebalance group connect with old generation 1 (__consumer_offsets-30) (kafka.coordinator.group.GroupCoordinator)
[35mkafka_1               |[0m [2017-10-25 05:26:47,523] INFO [GroupCoordinator 1]: Stabilized group connect generation 2 (__consumer_offsets-30) (kafka.coordinator.group.GroupCoordinator)
[35mkafka_1               |[0m [2017-10-25 05:26:47,526] INFO [GroupCoordinator 1]: Assignment received from leader for group connect for generation 2 (kafka.coordinator.group.GroupCoordinator)
[33;1mconnect_1             |[0m [2017-10-25 05:26:47,528] INFO Successfully joined group connect with generation 2 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[33;1mconnect_1             |[0m [2017-10-25 05:26:47,528] INFO Joined group and got assignment: Assignment{error=0, leader='connect-1-0982ca12-95de-406d-ae51-d169a88870a4', leaderUrl='http://connect:8083/', offset=1, connectorIds=[MySQLSource], taskIds=[]} (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[33;1mconnect_1             |[0m [2017-10-25 05:26:47,529] INFO Starting connectors and tasks using config offset 1 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[33;1mconnect_1             |[0m [2017-10-25 05:26:47,531] INFO Starting connector MySQLSource (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[33;1mconnect_1             |[0m [2017-10-25 05:26:47,532] INFO ConnectorConfig values: 
[33;1mconnect_1             |[0m 	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
[33;1mconnect_1             |[0m 	key.converter = null
[33;1mconnect_1             |[0m 	name = MySQLSource
[33;1mconnect_1             |[0m 	tasks.max = 1
[33;1mconnect_1             |[0m 	transforms = null
[33;1mconnect_1             |[0m 	value.converter = null
[33;1mconnect_1             |[0m  (org.apache.kafka.connect.runtime.ConnectorConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:26:47,534] INFO EnrichedConnectorConfig values: 
[33;1mconnect_1             |[0m 	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
[33;1mconnect_1             |[0m 	key.converter = null
[33;1mconnect_1             |[0m 	name = MySQLSource
[33;1mconnect_1             |[0m 	tasks.max = 1
[33;1mconnect_1             |[0m 	transforms = null
[33;1mconnect_1             |[0m 	value.converter = null
[33;1mconnect_1             |[0m  (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:26:47,536] INFO Creating connector MySQLSource of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker)
[33;1mconnect_1             |[0m [2017-10-25 05:26:47,546] INFO Instantiated connector MySQLSource with version 3.3.0 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker)
[33;1mconnect_1             |[0m [2017-10-25 05:26:47,547] INFO JdbcSourceConnectorConfig values: 
[33;1mconnect_1             |[0m 	batch.max.rows = 100
[33;1mconnect_1             |[0m 	connection.password = [hidden]
[33;1mconnect_1             |[0m 	connection.url = jdbc:mysql://mysql-db:3306/flights
[33;1mconnect_1             |[0m 	connection.user = root
[33;1mconnect_1             |[0m 	incrementing.column.name = 
[33;1mconnect_1             |[0m 	mode = bulk
[33;1mconnect_1             |[0m 	numeric.precision.mapping = false
[33;1mconnect_1             |[0m 	poll.interval.ms = 500
[33;1mconnect_1             |[0m 	query = 
[33;1mconnect_1             |[0m 	schema.pattern = null
[33;1mconnect_1             |[0m 	table.blacklist = []
[33;1mconnect_1             |[0m 	table.poll.interval.ms = 60000
[33;1mconnect_1             |[0m 	table.types = [TABLE]
[33;1mconnect_1             |[0m 	table.whitelist = [airlines, flights, airports, routes, users, trips, planes, countries, facebook, locales]
[33;1mconnect_1             |[0m 	timestamp.column.name = updated
[33;1mconnect_1             |[0m 	timestamp.delay.interval.ms = 0
[33;1mconnect_1             |[0m 	topic.prefix = mysql-
[33;1mconnect_1             |[0m 	validate.non.null = true
[33;1mconnect_1             |[0m  (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig)
[33;1mconnect_1             |[0m Wed Oct 25 05:26:47 UTC 2017 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
[36;1mksql-datagen-users_1  |[0m 436 --> ([ 436 | 13 | 2 | 2394 | 1508909207559 | 'iOS-test' | 'meh' ])
[33;1mconnect_1             |[0m [2017-10-25 05:26:47,607] INFO 172.21.0.1 - - [25/Oct/2017:05:26:46 +0000] "POST /connectors HTTP/1.1" 201 456  1597 (org.apache.kafka.connect.runtime.rest.RestServer)
[33;1mconnect_1             |[0m [2017-10-25 05:26:47,657] INFO Finished creating connector MySQLSource (org.apache.kafka.connect.runtime.Worker)
[33;1mconnect_1             |[0m [2017-10-25 05:26:47,659] INFO SourceConnectorConfig values: 
[33;1mconnect_1             |[0m 	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
[33;1mconnect_1             |[0m 	key.converter = null
[33;1mconnect_1             |[0m 	name = MySQLSource
[33;1mconnect_1             |[0m 	tasks.max = 1
[33;1mconnect_1             |[0m 	transforms = null
[33;1mconnect_1             |[0m 	value.converter = null
[33;1mconnect_1             |[0m  (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:26:47,659] INFO EnrichedConnectorConfig values: 
[33;1mconnect_1             |[0m 	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
[33;1mconnect_1             |[0m 	key.converter = null
[33;1mconnect_1             |[0m 	name = MySQLSource
[33;1mconnect_1             |[0m 	tasks.max = 1
[33;1mconnect_1             |[0m 	transforms = null
[33;1mconnect_1             |[0m 	value.converter = null
[33;1mconnect_1             |[0m  (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[35mkafka_1               |[0m [2017-10-25 05:26:47,662] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: connect-status-2. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[36;1mksql-datagen-users_1  |[0m 437 --> ([ 437 | 0 | 1 | 9683 | 1508909207854 | 'web' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 438 --> ([ 438 | 1 | 2 | 1458 | 1508909207927 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 439 --> ([ 439 | 9 | 4 | 9828 | 1508909207974 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 440 --> ([ 440 | 1 | 1 | 6715 | 1508909208022 | 'android' | 'Exceeded all my expectations. Thank you !' ])
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,036] INFO Tasks [MySQLSource-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[36;1mksql-datagen-users_1  |[0m 441 --> ([ 441 | -1 | 4 | 9711 | 1508909208146 | 'iOS-test' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 442 --> ([ 442 | 0 | 2 | 3485 | 1508909208192 | 'web' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 443 --> ([ 443 | 15 | 1 | 4810 | 1508909208266 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 444 --> ([ 444 | 11 | 1 | 9900 | 1508909208270 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 445 --> ([ 445 | 11 | 3 | 7271 | 1508909208520 | 'ios' | '(expletive deleted)' ])
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,544] INFO Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,544] INFO Rebalance started (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,547] INFO Stopping connector MySQLSource (org.apache.kafka.connect.runtime.Worker)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,547] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,552] INFO Stopped connector MySQLSource (org.apache.kafka.connect.runtime.Worker)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,556] INFO Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,556] INFO (Re-)joining group connect (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[35mkafka_1               |[0m [2017-10-25 05:26:48,557] INFO [GroupCoordinator 1]: Preparing to rebalance group connect with old generation 2 (__consumer_offsets-30) (kafka.coordinator.group.GroupCoordinator)
[35mkafka_1               |[0m [2017-10-25 05:26:48,557] INFO [GroupCoordinator 1]: Stabilized group connect generation 3 (__consumer_offsets-30) (kafka.coordinator.group.GroupCoordinator)
[35mkafka_1               |[0m [2017-10-25 05:26:48,559] INFO [GroupCoordinator 1]: Assignment received from leader for group connect for generation 3 (kafka.coordinator.group.GroupCoordinator)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,561] INFO Successfully joined group connect with generation 3 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,561] INFO Joined group and got assignment: Assignment{error=0, leader='connect-1-0982ca12-95de-406d-ae51-d169a88870a4', leaderUrl='http://connect:8083/', offset=3, connectorIds=[MySQLSource], taskIds=[MySQLSource-0]} (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,562] INFO Starting connectors and tasks using config offset 3 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,563] INFO Starting connector MySQLSource (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,563] INFO ConnectorConfig values: 
[33;1mconnect_1             |[0m 	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
[33;1mconnect_1             |[0m 	key.converter = null
[33;1mconnect_1             |[0m 	name = MySQLSource
[33;1mconnect_1             |[0m 	tasks.max = 1
[33;1mconnect_1             |[0m 	transforms = null
[33;1mconnect_1             |[0m 	value.converter = null
[33;1mconnect_1             |[0m  (org.apache.kafka.connect.runtime.ConnectorConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,564] INFO Starting task MySQLSource-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,564] INFO EnrichedConnectorConfig values: 
[33;1mconnect_1             |[0m 	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
[33;1mconnect_1             |[0m 	key.converter = null
[33;1mconnect_1             |[0m 	name = MySQLSource
[33;1mconnect_1             |[0m 	tasks.max = 1
[33;1mconnect_1             |[0m 	transforms = null
[33;1mconnect_1             |[0m 	value.converter = null
[33;1mconnect_1             |[0m  (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,564] INFO Creating connector MySQLSource of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,566] INFO Instantiated connector MySQLSource with version 3.3.0 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,566] INFO JdbcSourceConnectorConfig values: 
[33;1mconnect_1             |[0m 	batch.max.rows = 100
[33;1mconnect_1             |[0m 	connection.password = [hidden]
[33;1mconnect_1             |[0m 	connection.url = jdbc:mysql://mysql-db:3306/flights
[33;1mconnect_1             |[0m 	connection.user = root
[33;1mconnect_1             |[0m 	incrementing.column.name = 
[33;1mconnect_1             |[0m 	mode = bulk
[33;1mconnect_1             |[0m 	numeric.precision.mapping = false
[33;1mconnect_1             |[0m 	poll.interval.ms = 500
[33;1mconnect_1             |[0m 	query = 
[33;1mconnect_1             |[0m 	schema.pattern = null
[33;1mconnect_1             |[0m 	table.blacklist = []
[33;1mconnect_1             |[0m 	table.poll.interval.ms = 60000
[33;1mconnect_1             |[0m 	table.types = [TABLE]
[33;1mconnect_1             |[0m 	table.whitelist = [airlines, flights, airports, routes, users, trips, planes, countries, facebook, locales]
[33;1mconnect_1             |[0m 	timestamp.column.name = updated
[33;1mconnect_1             |[0m 	timestamp.delay.interval.ms = 0
[33;1mconnect_1             |[0m 	topic.prefix = mysql-
[33;1mconnect_1             |[0m 	validate.non.null = true
[33;1mconnect_1             |[0m  (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig)
[33;1mconnect_1             |[0m Wed Oct 25 05:26:48 UTC 2017 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,568] INFO Creating task MySQLSource-0 (org.apache.kafka.connect.runtime.Worker)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,569] INFO ConnectorConfig values: 
[33;1mconnect_1             |[0m 	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
[33;1mconnect_1             |[0m 	key.converter = null
[33;1mconnect_1             |[0m 	name = MySQLSource
[33;1mconnect_1             |[0m 	tasks.max = 1
[33;1mconnect_1             |[0m 	transforms = null
[33;1mconnect_1             |[0m 	value.converter = null
[33;1mconnect_1             |[0m  (org.apache.kafka.connect.runtime.ConnectorConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,570] INFO EnrichedConnectorConfig values: 
[33;1mconnect_1             |[0m 	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
[33;1mconnect_1             |[0m 	key.converter = null
[33;1mconnect_1             |[0m 	name = MySQLSource
[33;1mconnect_1             |[0m 	tasks.max = 1
[33;1mconnect_1             |[0m 	transforms = null
[33;1mconnect_1             |[0m 	value.converter = null
[33;1mconnect_1             |[0m  (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,573] INFO TaskConfig values: 
[33;1mconnect_1             |[0m 	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
[33;1mconnect_1             |[0m  (org.apache.kafka.connect.runtime.TaskConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,583] INFO Instantiated task MySQLSource-0 with version 3.3.0 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,586] INFO ProducerConfig values: 
[33;1mconnect_1             |[0m 	acks = all
[33;1mconnect_1             |[0m 	batch.size = 16384
[33;1mconnect_1             |[0m 	bootstrap.servers = [kafka:29092]
[33;1mconnect_1             |[0m 	buffer.memory = 33554432
[33;1mconnect_1             |[0m 	client.id = 
[33;1mconnect_1             |[0m 	compression.type = none
[33;1mconnect_1             |[0m 	connections.max.idle.ms = 540000
[33;1mconnect_1             |[0m 	enable.idempotence = false
[33;1mconnect_1             |[0m 	interceptor.classes = [io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor]
[33;1mconnect_1             |[0m 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[33;1mconnect_1             |[0m 	linger.ms = 0
[33;1mconnect_1             |[0m 	max.block.ms = 9223372036854775807
[33;1mconnect_1             |[0m 	max.in.flight.requests.per.connection = 1
[33;1mconnect_1             |[0m 	max.request.size = 1048576
[33;1mconnect_1             |[0m 	metadata.max.age.ms = 300000
[33;1mconnect_1             |[0m 	metric.reporters = []
[33;1mconnect_1             |[0m 	metrics.num.samples = 2
[33;1mconnect_1             |[0m 	metrics.recording.level = INFO
[33;1mconnect_1             |[0m 	metrics.sample.window.ms = 30000
[33;1mconnect_1             |[0m 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
[33;1mconnect_1             |[0m 	receive.buffer.bytes = 32768
[33;1mconnect_1             |[0m 	reconnect.backoff.max.ms = 1000
[33;1mconnect_1             |[0m 	reconnect.backoff.ms = 50
[33;1mconnect_1             |[0m 	request.timeout.ms = 2147483647
[33;1mconnect_1             |[0m 	retries = 2147483647
[33;1mconnect_1             |[0m 	retry.backoff.ms = 100
[33;1mconnect_1             |[0m 	sasl.jaas.config = null
[33;1mconnect_1             |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[33;1mconnect_1             |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[33;1mconnect_1             |[0m 	sasl.kerberos.service.name = null
[33;1mconnect_1             |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[33;1mconnect_1             |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[33;1mconnect_1             |[0m 	sasl.mechanism = GSSAPI
[33;1mconnect_1             |[0m 	security.protocol = PLAINTEXT
[33;1mconnect_1             |[0m 	send.buffer.bytes = 131072
[33;1mconnect_1             |[0m 	ssl.cipher.suites = null
[33;1mconnect_1             |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[33;1mconnect_1             |[0m 	ssl.endpoint.identification.algorithm = null
[33;1mconnect_1             |[0m 	ssl.key.password = null
[33;1mconnect_1             |[0m 	ssl.keymanager.algorithm = SunX509
[33;1mconnect_1             |[0m 	ssl.keystore.location = null
[33;1mconnect_1             |[0m 	ssl.keystore.password = null
[33;1mconnect_1             |[0m 	ssl.keystore.type = JKS
[33;1mconnect_1             |[0m 	ssl.protocol = TLS
[33;1mconnect_1             |[0m 	ssl.provider = null
[33;1mconnect_1             |[0m 	ssl.secure.random.implementation = null
[33;1mconnect_1             |[0m 	ssl.trustmanager.algorithm = PKIX
[33;1mconnect_1             |[0m 	ssl.truststore.location = null
[33;1mconnect_1             |[0m 	ssl.truststore.password = null
[33;1mconnect_1             |[0m 	ssl.truststore.type = JKS
[33;1mconnect_1             |[0m 	transaction.timeout.ms = 60000
[33;1mconnect_1             |[0m 	transactional.id = null
[33;1mconnect_1             |[0m 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[33;1mconnect_1             |[0m  (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,604] INFO Kafka version : 0.11.0.0-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,604] INFO Kafka commitId : 6a8cf706ddc9ab6a (org.apache.kafka.common.utils.AppInfoParser)
[35mkafka_1               |[0m [2017-10-25 05:26:48,621] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: connect-status-0. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,621] INFO Finished creating connector MySQLSource (org.apache.kafka.connect.runtime.Worker)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,622] INFO JdbcSourceTaskConfig values: 
[33;1mconnect_1             |[0m 	batch.max.rows = 100
[33;1mconnect_1             |[0m 	connection.password = [hidden]
[33;1mconnect_1             |[0m 	connection.url = jdbc:mysql://mysql-db:3306/flights
[33;1mconnect_1             |[0m 	connection.user = root
[33;1mconnect_1             |[0m 	incrementing.column.name = 
[33;1mconnect_1             |[0m 	mode = bulk
[33;1mconnect_1             |[0m 	numeric.precision.mapping = false
[33;1mconnect_1             |[0m 	poll.interval.ms = 500
[33;1mconnect_1             |[0m 	query = 
[33;1mconnect_1             |[0m 	schema.pattern = null
[33;1mconnect_1             |[0m 	table.blacklist = []
[33;1mconnect_1             |[0m 	table.poll.interval.ms = 60000
[33;1mconnect_1             |[0m 	table.types = [TABLE]
[33;1mconnect_1             |[0m 	table.whitelist = [airlines, flights, airports, routes, users, trips, planes, countries, facebook, locales]
[33;1mconnect_1             |[0m 	tables = [airlines, airports, countries, facebook, flights, locales, planes, routes, trips, users]
[33;1mconnect_1             |[0m 	timestamp.column.name = updated
[33;1mconnect_1             |[0m 	timestamp.delay.interval.ms = 0
[33;1mconnect_1             |[0m 	topic.prefix = mysql-
[33;1mconnect_1             |[0m 	validate.non.null = true
[33;1mconnect_1             |[0m  (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,622] INFO SourceConnectorConfig values: 
[33;1mconnect_1             |[0m 	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
[33;1mconnect_1             |[0m 	key.converter = null
[33;1mconnect_1             |[0m 	name = MySQLSource
[33;1mconnect_1             |[0m 	tasks.max = 1
[33;1mconnect_1             |[0m 	transforms = null
[33;1mconnect_1             |[0m 	value.converter = null
[33;1mconnect_1             |[0m  (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,624] INFO EnrichedConnectorConfig values: 
[33;1mconnect_1             |[0m 	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
[33;1mconnect_1             |[0m 	key.converter = null
[33;1mconnect_1             |[0m 	name = MySQLSource
[33;1mconnect_1             |[0m 	tasks.max = 1
[33;1mconnect_1             |[0m 	transforms = null
[33;1mconnect_1             |[0m 	value.converter = null
[33;1mconnect_1             |[0m  (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[33;1mconnect_1             |[0m Wed Oct 25 05:26:48 UTC 2017 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,643] INFO Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[36;1mksql-datagen-users_1  |[0m 446 --> ([ 446 | 9 | 2 | 7999 | 1508909208676 | 'ios' | 'worst. flight. ever. #neveragain' ])
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,697] INFO Source task WorkerSourceTask{id=MySQLSource-0} finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask)
[33mzookeeper_1           |[0m [2017-10-25 05:26:48,774] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:setData cxid:0x2c7 zxid:0x174 txntype:-1 reqpath:n/a Error Path:/config/topics/mysql-airlines Error:KeeperErrorCode = NoNode for /config/topics/mysql-airlines (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:26:48,790] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x2c8 zxid:0x175 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:26:48,814] INFO Topic creation {"version":1,"partitions":{"0":[1]}} (kafka.admin.AdminUtils$)
[35mkafka_1               |[0m [2017-10-25 05:26:48,827] INFO [KafkaApi-1] Auto creation of topic mysql-airlines with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,829] WARN Error while fetching metadata with correlation id 1 : {mysql-airlines=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)
[35mkafka_1               |[0m [2017-10-25 05:26:48,832] INFO [Controller 1]: New topics: [Set(mysql-airlines)], deleted topics: [Set()], new partition replica assignment [Map([mysql-airlines,0] -> List(1))] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:26:48,832] INFO [Controller 1]: New topic creation callback for [mysql-airlines,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:26:48,833] INFO [Controller 1]: New partition creation callback for [mysql-airlines,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:26:48,833] INFO [Partition state machine on Controller 1]: Invoking state change to NewPartition for partitions [mysql-airlines,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:26:48,833] TRACE Controller 1 epoch 1 changed partition [mysql-airlines,0] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:48,833] INFO [Replica state machine on controller 1]: Invoking state change to NewReplica for replicas [Topic=mysql-airlines,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:26:48,834] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [mysql-airlines,0] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:48,834] INFO [Partition state machine on Controller 1]: Invoking state change to OnlinePartition for partitions [mysql-airlines,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:26:48,834] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [mysql-airlines,0] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:26:48,834] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [mysql-airlines,0] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:26:48,835] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x2d0 zxid:0x178 txntype:-1 reqpath:n/a Error Path:/brokers/topics/mysql-airlines/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/mysql-airlines/partitions/0 (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 447 --> ([ 447 | 0 | 3 | 7965 | 1508909208840 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[33mzookeeper_1           |[0m [2017-10-25 05:26:48,845] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x2d1 zxid:0x179 txntype:-1 reqpath:n/a Error Path:/brokers/topics/mysql-airlines/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/mysql-airlines/partitions (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:26:48,896] TRACE Controller 1 epoch 1 changed partition [mysql-airlines,0] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:48,896] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [mysql-airlines,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:48,896] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition mysql-airlines-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:48,896] INFO [Replica state machine on controller 1]: Invoking state change to OnlineReplica for replicas [Topic=mysql-airlines,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:26:48,897] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [mysql-airlines,0] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:48,897] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 15 from controller 1 epoch 1 for partition [mysql-airlines,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:48,898] TRACE Broker 1 handling LeaderAndIsr request correlationId 15 from controller 1 epoch 1 starting the become-leader transition for partition mysql-airlines-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:48,898] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions mysql-airlines-0 (kafka.server.ReplicaFetcherManager)
[35mkafka_1               |[0m [2017-10-25 05:26:48,901] INFO Loading producer state from offset 0 for partition mysql-airlines-0 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:26:48,901] INFO Completed load of log mysql-airlines-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:26:48,902] INFO Created log for partition [mysql-airlines,0] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:26:48,902] INFO Partition [mysql-airlines,0] on broker 1: No checkpointed highwatermark is found for partition mysql-airlines-0 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:26:48,903] INFO Partition [mysql-airlines,0] on broker 1: mysql-airlines-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:26:48,903] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 15 for partition mysql-airlines-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:48,903] TRACE Broker 1 completed LeaderAndIsr request correlationId 15 from controller 1 epoch 1 for the become-leader transition for partition mysql-airlines-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:48,904] TRACE Controller 1 epoch 1 received response {error_code=0,partitions=[{topic=mysql-airlines,partition=0,error_code=0}]} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[36;1mksql-datagen-users_1  |[0m 448 --> ([ 448 | 13 | 4 | 3426 | 1508909208905 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[35mkafka_1               |[0m [2017-10-25 05:26:48,906] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition mysql-airlines-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 16 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:48,907] TRACE Controller 1 epoch 1 received response {error_code=0} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:48,943] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: mysql-airlines-0. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[33;1mconnect_1             |[0m [2017-10-25 05:26:48,965] INFO creating interceptor (io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor)
[33;1mconnect_1             |[0m [2017-10-25 05:26:49,061] INFO MonitoringInterceptorConfig values: 
[33;1mconnect_1             |[0m 	confluent.monitoring.interceptor.publishMs = 15000
[33;1mconnect_1             |[0m 	confluent.monitoring.interceptor.topic = _confluent-monitoring
[33;1mconnect_1             |[0m  (io.confluent.monitoring.clients.interceptor.MonitoringInterceptorConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:26:49,087] INFO ProducerConfig values: 
[33;1mconnect_1             |[0m 	acks = all
[33;1mconnect_1             |[0m 	batch.size = 16384
[33;1mconnect_1             |[0m 	bootstrap.servers = [kafka:29092]
[33;1mconnect_1             |[0m 	buffer.memory = 33554432
[33;1mconnect_1             |[0m 	client.id = confluent.monitoring.interceptor.producer-4
[33;1mconnect_1             |[0m 	compression.type = lz4
[33;1mconnect_1             |[0m 	connections.max.idle.ms = 540000
[33;1mconnect_1             |[0m 	enable.idempotence = false
[33;1mconnect_1             |[0m 	interceptor.classes = []
[33;1mconnect_1             |[0m 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[33;1mconnect_1             |[0m 	linger.ms = 500
[33;1mconnect_1             |[0m 	max.block.ms = 60000
[33;1mconnect_1             |[0m 	max.in.flight.requests.per.connection = 1
[33;1mconnect_1             |[0m 	max.request.size = 10485760
[33;1mconnect_1             |[0m 	metadata.max.age.ms = 300000
[33;1mconnect_1             |[0m 	metric.reporters = []
[33;1mconnect_1             |[0m 	metrics.num.samples = 2
[33;1mconnect_1             |[0m 	metrics.recording.level = INFO
[33;1mconnect_1             |[0m 	metrics.sample.window.ms = 30000
[33;1mconnect_1             |[0m 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
[33;1mconnect_1             |[0m 	receive.buffer.bytes = 32768
[33;1mconnect_1             |[0m 	reconnect.backoff.max.ms = 1000
[33;1mconnect_1             |[0m 	reconnect.backoff.ms = 50
[33;1mconnect_1             |[0m 	request.timeout.ms = 30000
[33;1mconnect_1             |[0m 	retries = 10
[33;1mconnect_1             |[0m 	retry.backoff.ms = 500
[33;1mconnect_1             |[0m 	sasl.jaas.config = null
[33;1mconnect_1             |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[33;1mconnect_1             |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[33;1mconnect_1             |[0m 	sasl.kerberos.service.name = null
[33;1mconnect_1             |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[33;1mconnect_1             |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[33;1mconnect_1             |[0m 	sasl.mechanism = GSSAPI
[33;1mconnect_1             |[0m 	security.protocol = PLAINTEXT
[33;1mconnect_1             |[0m 	send.buffer.bytes = 131072
[33;1mconnect_1             |[0m 	ssl.cipher.suites = null
[33;1mconnect_1             |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[33;1mconnect_1             |[0m 	ssl.endpoint.identification.algorithm = null
[33;1mconnect_1             |[0m 	ssl.key.password = null
[33;1mconnect_1             |[0m 	ssl.keymanager.algorithm = SunX509
[33;1mconnect_1             |[0m 	ssl.keystore.location = null
[33;1mconnect_1             |[0m 	ssl.keystore.password = null
[33;1mconnect_1             |[0m 	ssl.keystore.type = JKS
[33;1mconnect_1             |[0m 	ssl.protocol = TLS
[33;1mconnect_1             |[0m 	ssl.provider = null
[33;1mconnect_1             |[0m 	ssl.secure.random.implementation = null
[33;1mconnect_1             |[0m 	ssl.trustmanager.algorithm = PKIX
[33;1mconnect_1             |[0m 	ssl.truststore.location = null
[33;1mconnect_1             |[0m 	ssl.truststore.password = null
[33;1mconnect_1             |[0m 	ssl.truststore.type = JKS
[33;1mconnect_1             |[0m 	transaction.timeout.ms = 60000
[33;1mconnect_1             |[0m 	transactional.id = null
[33;1mconnect_1             |[0m 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[33;1mconnect_1             |[0m  (org.apache.kafka.clients.producer.ProducerConfig)
[33;1mconnect_1             |[0m [2017-10-25 05:26:49,106] INFO Kafka version : 0.11.0.0-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[33;1mconnect_1             |[0m [2017-10-25 05:26:49,106] INFO Kafka commitId : 6a8cf706ddc9ab6a (org.apache.kafka.common.utils.AppInfoParser)
[33;1mconnect_1             |[0m [2017-10-25 05:26:49,108] INFO interceptor=created for client=confluent.monitoring.interceptor.producer-4 (io.confluent.monitoring.clients.interceptor.MonitoringInterceptor)
[36;1mksql-datagen-users_1  |[0m 449 --> ([ 449 | 9 | 3 | 2088 | 1508909209112 | 'android' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 450 --> ([ 450 | 11 | 2 | 6757 | 1508909209357 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[33mzookeeper_1           |[0m [2017-10-25 05:26:49,423] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:setData cxid:0x2d9 zxid:0x17d txntype:-1 reqpath:n/a Error Path:/config/topics/mysql-airports Error:KeeperErrorCode = NoNode for /config/topics/mysql-airports (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:26:49,437] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x2da zxid:0x17e txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:26:49,463] INFO Topic creation {"version":1,"partitions":{"0":[1]}} (kafka.admin.AdminUtils$)
[35mkafka_1               |[0m [2017-10-25 05:26:49,475] INFO [KafkaApi-1] Auto creation of topic mysql-airports with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[33;1mconnect_1             |[0m [2017-10-25 05:26:49,476] WARN Error while fetching metadata with correlation id 137 : {mysql-airports=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)
[35mkafka_1               |[0m [2017-10-25 05:26:49,480] INFO [Controller 1]: New topics: [Set(mysql-airports)], deleted topics: [Set()], new partition replica assignment [Map([mysql-airports,0] -> List(1))] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:26:49,480] INFO [Controller 1]: New topic creation callback for [mysql-airports,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:26:49,482] INFO [Controller 1]: New partition creation callback for [mysql-airports,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:26:49,482] INFO [Partition state machine on Controller 1]: Invoking state change to NewPartition for partitions [mysql-airports,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:26:49,482] TRACE Controller 1 epoch 1 changed partition [mysql-airports,0] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:49,483] INFO [Replica state machine on controller 1]: Invoking state change to NewReplica for replicas [Topic=mysql-airports,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:26:49,485] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [mysql-airports,0] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:49,485] INFO [Partition state machine on Controller 1]: Invoking state change to OnlinePartition for partitions [mysql-airports,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:26:49,486] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [mysql-airports,0] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:26:49,486] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [mysql-airports,0] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:26:49,488] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x2e2 zxid:0x181 txntype:-1 reqpath:n/a Error Path:/brokers/topics/mysql-airports/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/mysql-airports/partitions/0 (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:26:49,502] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x2e3 zxid:0x182 txntype:-1 reqpath:n/a Error Path:/brokers/topics/mysql-airports/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/mysql-airports/partitions (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:26:49,552] TRACE Controller 1 epoch 1 changed partition [mysql-airports,0] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:49,552] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [mysql-airports,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:49,553] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition mysql-airports-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:49,553] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 17 from controller 1 epoch 1 for partition [mysql-airports,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:49,554] TRACE Broker 1 handling LeaderAndIsr request correlationId 17 from controller 1 epoch 1 starting the become-leader transition for partition mysql-airports-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:49,554] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions mysql-airports-0 (kafka.server.ReplicaFetcherManager)
[35mkafka_1               |[0m [2017-10-25 05:26:49,554] INFO [Replica state machine on controller 1]: Invoking state change to OnlineReplica for replicas [Topic=mysql-airports,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:26:49,554] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [mysql-airports,0] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:49,556] INFO Loading producer state from offset 0 for partition mysql-airports-0 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:26:49,557] INFO Completed load of log mysql-airports-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:26:49,557] INFO Created log for partition [mysql-airports,0] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:26:49,560] INFO Partition [mysql-airports,0] on broker 1: No checkpointed highwatermark is found for partition mysql-airports-0 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:26:49,560] INFO Partition [mysql-airports,0] on broker 1: mysql-airports-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:26:49,560] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 17 for partition mysql-airports-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:49,560] TRACE Broker 1 completed LeaderAndIsr request correlationId 17 from controller 1 epoch 1 for the become-leader transition for partition mysql-airports-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:49,561] TRACE Controller 1 epoch 1 received response {error_code=0,partitions=[{topic=mysql-airports,partition=0,error_code=0}]} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:49,562] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition mysql-airports-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 18 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:49,562] TRACE Controller 1 epoch 1 received response {error_code=0} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:49,592] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: mysql-airports-0. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[36;1mksql-datagen-users_1  |[0m 451 --> ([ 451 | 1 | 2 | 7588 | 1508909209601 | 'web' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 452 --> ([ 452 | 14 | 1 | 1968 | 1508909209824 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 453 --> ([ 453 | 16 | 1 | 8166 | 1508909209990 | 'iOS' | '(expletive deleted)' ])
[32mmysql-db_1            |[0m 2017-10-25T05:26:50.022722Z 7 [Note] Aborted connection 7 to db: 'flights' user: 'root' host: '172.21.0.9' (Got an error reading communication packets)
[32mmysql-db_1            |[0m 2017-10-25T05:26:50.025634Z 8 [Note] Aborted connection 8 to db: 'flights' user: 'root' host: '172.21.0.9' (Got an error reading communication packets)
[36;1mksql-datagen-users_1  |[0m 454 --> ([ 454 | 13 | 3 | 22 | 1508909210037 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 455 --> ([ 455 | 4 | 1 | 5101 | 1508909210047 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 456 --> ([ 456 | 18 | 4 | 8122 | 1508909210104 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 457 --> ([ 457 | 6 | 4 | 4662 | 1508909210166 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 458 --> ([ 458 | 7 | 1 | 7620 | 1508909210273 | 'iOS' | 'is this as good as it gets? really ?' ])
[33mzookeeper_1           |[0m [2017-10-25 05:26:50,370] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:setData cxid:0x2eb zxid:0x186 txntype:-1 reqpath:n/a Error Path:/config/topics/mysql-countries Error:KeeperErrorCode = NoNode for /config/topics/mysql-countries (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:26:50,385] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x2ec zxid:0x187 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:26:50,409] INFO Topic creation {"version":1,"partitions":{"0":[1]}} (kafka.admin.AdminUtils$)
[35mkafka_1               |[0m [2017-10-25 05:26:50,423] INFO [KafkaApi-1] Auto creation of topic mysql-countries with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[35mkafka_1               |[0m [2017-10-25 05:26:50,426] INFO [Controller 1]: New topics: [Set(mysql-countries)], deleted topics: [Set()], new partition replica assignment [Map([mysql-countries,0] -> List(1))] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:26:50,426] INFO [Controller 1]: New topic creation callback for [mysql-countries,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:26:50,426] INFO [Controller 1]: New partition creation callback for [mysql-countries,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:26:50,426] INFO [Partition state machine on Controller 1]: Invoking state change to NewPartition for partitions [mysql-countries,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:26:50,426] TRACE Controller 1 epoch 1 changed partition [mysql-countries,0] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:50,427] INFO [Replica state machine on controller 1]: Invoking state change to NewReplica for replicas [Topic=mysql-countries,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:26:50,428] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [mysql-countries,0] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:50,428] INFO [Partition state machine on Controller 1]: Invoking state change to OnlinePartition for partitions [mysql-countries,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:26:50,428] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [mysql-countries,0] are: [List(1)] (kafka.controller.PartitionStateMachine)
[33;1mconnect_1             |[0m [2017-10-25 05:26:50,425] WARN Error while fetching metadata with correlation id 606 : {mysql-countries=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)
[35mkafka_1               |[0m [2017-10-25 05:26:50,428] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [mysql-countries,0] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:26:50,429] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x2f4 zxid:0x18a txntype:-1 reqpath:n/a Error Path:/brokers/topics/mysql-countries/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/mysql-countries/partitions/0 (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:26:50,441] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x2f5 zxid:0x18b txntype:-1 reqpath:n/a Error Path:/brokers/topics/mysql-countries/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/mysql-countries/partitions (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:26:50,489] TRACE Controller 1 epoch 1 changed partition [mysql-countries,0] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:50,489] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [mysql-countries,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:50,489] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition mysql-countries-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:50,489] INFO [Replica state machine on controller 1]: Invoking state change to OnlineReplica for replicas [Topic=mysql-countries,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:26:50,490] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [mysql-countries,0] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:50,490] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 19 from controller 1 epoch 1 for partition [mysql-countries,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:50,492] TRACE Broker 1 handling LeaderAndIsr request correlationId 19 from controller 1 epoch 1 starting the become-leader transition for partition mysql-countries-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:50,492] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions mysql-countries-0 (kafka.server.ReplicaFetcherManager)
[35mkafka_1               |[0m [2017-10-25 05:26:50,496] INFO Loading producer state from offset 0 for partition mysql-countries-0 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:26:50,496] INFO Completed load of log mysql-countries-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:26:50,501] INFO Created log for partition [mysql-countries,0] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:26:50,503] INFO Partition [mysql-countries,0] on broker 1: No checkpointed highwatermark is found for partition mysql-countries-0 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:26:50,503] INFO Partition [mysql-countries,0] on broker 1: mysql-countries-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:26:50,504] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 19 for partition mysql-countries-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:50,504] TRACE Broker 1 completed LeaderAndIsr request correlationId 19 from controller 1 epoch 1 for the become-leader transition for partition mysql-countries-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:50,504] TRACE Controller 1 epoch 1 received response {error_code=0,partitions=[{topic=mysql-countries,partition=0,error_code=0}]} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:50,506] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition mysql-countries-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 20 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:50,506] TRACE Controller 1 epoch 1 received response {error_code=0} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:50,529] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: mysql-countries-0. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[36;1mksql-datagen-users_1  |[0m 459 --> ([ 459 | 17 | 4 | 383 | 1508909210529 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 460 --> ([ 460 | 2 | 1 | 6067 | 1508909210798 | 'iOS-test' | '(expletive deleted)' ])
[33mzookeeper_1           |[0m [2017-10-25 05:26:50,803] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:setData cxid:0x2fd zxid:0x18f txntype:-1 reqpath:n/a Error Path:/config/topics/mysql-routes Error:KeeperErrorCode = NoNode for /config/topics/mysql-routes (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 461 --> ([ 461 | 10 | 3 | 1921 | 1508909210813 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 462 --> ([ 462 | 15 | 1 | 1464 | 1508909210814 | 'ios' | 'more peanuts please' ])
[33mzookeeper_1           |[0m [2017-10-25 05:26:50,815] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x2fe zxid:0x190 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:26:50,837] INFO Topic creation {"version":1,"partitions":{"0":[1]}} (kafka.admin.AdminUtils$)
[35mkafka_1               |[0m [2017-10-25 05:26:50,851] INFO [KafkaApi-1] Auto creation of topic mysql-routes with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[33;1mconnect_1             |[0m [2017-10-25 05:26:50,853] WARN Error while fetching metadata with correlation id 713 : {mysql-routes=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)
[35mkafka_1               |[0m [2017-10-25 05:26:50,856] INFO [Controller 1]: New topics: [Set(mysql-routes)], deleted topics: [Set()], new partition replica assignment [Map([mysql-routes,0] -> List(1))] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:26:50,856] INFO [Controller 1]: New topic creation callback for [mysql-routes,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:26:50,857] INFO [Controller 1]: New partition creation callback for [mysql-routes,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:26:50,857] INFO [Partition state machine on Controller 1]: Invoking state change to NewPartition for partitions [mysql-routes,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:26:50,857] TRACE Controller 1 epoch 1 changed partition [mysql-routes,0] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:50,857] INFO [Replica state machine on controller 1]: Invoking state change to NewReplica for replicas [Topic=mysql-routes,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:26:50,858] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [mysql-routes,0] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:50,859] INFO [Partition state machine on Controller 1]: Invoking state change to OnlinePartition for partitions [mysql-routes,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:26:50,859] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [mysql-routes,0] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:26:50,859] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [mysql-routes,0] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:26:50,860] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x306 zxid:0x193 txntype:-1 reqpath:n/a Error Path:/brokers/topics/mysql-routes/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/mysql-routes/partitions/0 (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:26:50,870] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x307 zxid:0x194 txntype:-1 reqpath:n/a Error Path:/brokers/topics/mysql-routes/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/mysql-routes/partitions (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 463 --> ([ 463 | 1 | 1 | 4796 | 1508909210918 | 'iOS' | 'your team here rocks!' ])
[35mkafka_1               |[0m [2017-10-25 05:26:50,935] TRACE Controller 1 epoch 1 changed partition [mysql-routes,0] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:50,935] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [mysql-routes,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:50,935] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition mysql-routes-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:50,935] INFO [Replica state machine on controller 1]: Invoking state change to OnlineReplica for replicas [Topic=mysql-routes,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:26:50,935] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [mysql-routes,0] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:50,936] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 21 from controller 1 epoch 1 for partition [mysql-routes,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:50,937] TRACE Broker 1 handling LeaderAndIsr request correlationId 21 from controller 1 epoch 1 starting the become-leader transition for partition mysql-routes-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:50,937] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions mysql-routes-0 (kafka.server.ReplicaFetcherManager)
[35mkafka_1               |[0m [2017-10-25 05:26:50,941] INFO Loading producer state from offset 0 for partition mysql-routes-0 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:26:50,942] INFO Completed load of log mysql-routes-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:26:50,943] INFO Created log for partition [mysql-routes,0] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:26:50,944] INFO Partition [mysql-routes,0] on broker 1: No checkpointed highwatermark is found for partition mysql-routes-0 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:26:50,944] INFO Partition [mysql-routes,0] on broker 1: mysql-routes-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:26:50,945] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 21 for partition mysql-routes-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:50,945] TRACE Broker 1 completed LeaderAndIsr request correlationId 21 from controller 1 epoch 1 for the become-leader transition for partition mysql-routes-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:50,946] TRACE Controller 1 epoch 1 received response {error_code=0,partitions=[{topic=mysql-routes,partition=0,error_code=0}]} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:50,947] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition mysql-routes-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 22 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:50,947] TRACE Controller 1 epoch 1 received response {error_code=0} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:50,968] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: mysql-routes-0. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[36;1mksql-datagen-users_1  |[0m 464 --> ([ 464 | 17 | 1 | 9162 | 1508909211047 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 465 --> ([ 465 | 13 | 4 | 7737 | 1508909211313 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 466 --> ([ 466 | 13 | 4 | 4487 | 1508909211552 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 467 --> ([ 467 | 14 | 4 | 6915 | 1508909211581 | 'web' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 468 --> ([ 468 | 17 | 3 | 6510 | 1508909211734 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 469 --> ([ 469 | -1 | 4 | 8800 | 1508909211838 | 'iOS' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 470 --> ([ 470 | 16 | 3 | 1064 | 1508909211931 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 471 --> ([ 471 | 2 | 4 | 5933 | 1508909211988 | 'iOS-test' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 472 --> ([ 472 | 7 | 2 | 3443 | 1508909212147 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 473 --> ([ 473 | 9 | 1 | 2326 | 1508909212357 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 474 --> ([ 474 | 11 | 1 | 3836 | 1508909212565 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 475 --> ([ 475 | 13 | 4 | 7782 | 1508909212684 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 476 --> ([ 476 | 16 | 1 | 975 | 1508909212743 | 'android' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 477 --> ([ 477 | 19 | 1 | 2893 | 1508909212767 | 'iOS-test' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 478 --> ([ 478 | 7 | 4 | 2018 | 1508909212880 | 'iOS' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 479 --> ([ 479 | 2 | 4 | 8936 | 1508909213168 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 480 --> ([ 480 | 7 | 2 | 9181 | 1508909213324 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 481 --> ([ 481 | 19 | 2 | 9482 | 1508909213614 | 'web' | 'meh' ])
[33mzookeeper_1           |[0m [2017-10-25 05:26:53,634] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:setData cxid:0x30f zxid:0x198 txntype:-1 reqpath:n/a Error Path:/config/topics/mysql-users Error:KeeperErrorCode = NoNode for /config/topics/mysql-users (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 482 --> ([ 482 | 2 | 3 | 6118 | 1508909213640 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[33mzookeeper_1           |[0m [2017-10-25 05:26:53,651] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x310 zxid:0x199 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:26:53,676] INFO Topic creation {"version":1,"partitions":{"0":[1]}} (kafka.admin.AdminUtils$)
[35mkafka_1               |[0m [2017-10-25 05:26:53,689] INFO [KafkaApi-1] Auto creation of topic mysql-users with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[33;1mconnect_1             |[0m [2017-10-25 05:26:53,690] WARN Error while fetching metadata with correlation id 2658 : {mysql-users=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)
[35mkafka_1               |[0m [2017-10-25 05:26:53,700] INFO [Controller 1]: New topics: [Set(mysql-users)], deleted topics: [Set()], new partition replica assignment [Map([mysql-users,0] -> List(1))] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:26:53,700] INFO [Controller 1]: New topic creation callback for [mysql-users,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:26:53,701] INFO [Controller 1]: New partition creation callback for [mysql-users,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:26:53,701] INFO [Partition state machine on Controller 1]: Invoking state change to NewPartition for partitions [mysql-users,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:26:53,702] TRACE Controller 1 epoch 1 changed partition [mysql-users,0] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:53,702] INFO [Replica state machine on controller 1]: Invoking state change to NewReplica for replicas [Topic=mysql-users,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:26:53,703] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [mysql-users,0] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:53,703] INFO [Partition state machine on Controller 1]: Invoking state change to OnlinePartition for partitions [mysql-users,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:26:53,703] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [mysql-users,0] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:26:53,703] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [mysql-users,0] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:26:53,704] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x318 zxid:0x19c txntype:-1 reqpath:n/a Error Path:/brokers/topics/mysql-users/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/mysql-users/partitions/0 (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:26:53,715] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x319 zxid:0x19d txntype:-1 reqpath:n/a Error Path:/brokers/topics/mysql-users/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/mysql-users/partitions (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 483 --> ([ 483 | 11 | 2 | 2920 | 1508909213744 | 'ios' | 'meh' ])
[35mkafka_1               |[0m [2017-10-25 05:26:53,774] TRACE Controller 1 epoch 1 changed partition [mysql-users,0] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:53,774] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [mysql-users,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:53,774] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition mysql-users-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:53,774] INFO [Replica state machine on controller 1]: Invoking state change to OnlineReplica for replicas [Topic=mysql-users,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:26:53,774] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [mysql-users,0] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:53,775] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 23 from controller 1 epoch 1 for partition [mysql-users,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:53,776] TRACE Broker 1 handling LeaderAndIsr request correlationId 23 from controller 1 epoch 1 starting the become-leader transition for partition mysql-users-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:53,776] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions mysql-users-0 (kafka.server.ReplicaFetcherManager)
[35mkafka_1               |[0m [2017-10-25 05:26:53,781] INFO Loading producer state from offset 0 for partition mysql-users-0 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:26:53,782] INFO Completed load of log mysql-users-0 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:26:53,783] INFO Created log for partition [mysql-users,0] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:26:53,784] INFO Partition [mysql-users,0] on broker 1: No checkpointed highwatermark is found for partition mysql-users-0 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:26:53,784] INFO Partition [mysql-users,0] on broker 1: mysql-users-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:26:53,784] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 23 for partition mysql-users-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:53,785] TRACE Broker 1 completed LeaderAndIsr request correlationId 23 from controller 1 epoch 1 for the become-leader transition for partition mysql-users-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:53,785] TRACE Controller 1 epoch 1 received response {error_code=0,partitions=[{topic=mysql-users,partition=0,error_code=0}]} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:53,787] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition mysql-users-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 24 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:53,788] TRACE Controller 1 epoch 1 received response {error_code=0} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:26:53,815] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: mysql-users-0. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[36;1mksql-datagen-users_1  |[0m 484 --> ([ 484 | 10 | 1 | 9541 | 1508909213936 | 'android' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 485 --> ([ 485 | 12 | 4 | 7693 | 1508909214077 | 'iOS' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 486 --> ([ 486 | 4 | 2 | 4134 | 1508909214344 | 'android' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 487 --> ([ 487 | -1 | 2 | 4496 | 1508909214535 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 488 --> ([ 488 | 7 | 4 | 1209 | 1508909214686 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 489 --> ([ 489 | 9 | 3 | 2168 | 1508909214699 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 490 --> ([ 490 | 19 | 3 | 4166 | 1508909214992 | 'iOS-test' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 491 --> ([ 491 | 10 | 3 | 6572 | 1508909215207 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 492 --> ([ 492 | 11 | 2 | 6233 | 1508909215319 | 'android' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 493 --> ([ 493 | 3 | 4 | 4205 | 1508909215382 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 494 --> ([ 494 | 13 | 2 | 8496 | 1508909215399 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 495 --> ([ 495 | 19 | 2 | 4781 | 1508909215651 | 'web' | 'worst. flight. ever. #neveragain' ])
[35;1melasticsearch_1       |[0m [2017-10-25T05:26:55,806][INFO ][o.e.c.m.MetaDataCreateIndexService] [-Ryb53P] [.triggered_watches] creating index, cause [auto(bulk api)], templates [triggered_watches], shards [1]/[1], mappings [triggered_watch]
[36;1mksql-datagen-users_1  |[0m 496 --> ([ 496 | 9 | 2 | 8740 | 1508909215866 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 497 --> ([ 497 | 7 | 2 | 9576 | 1508909215984 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 498 --> ([ 498 | 8 | 4 | 4102 | 1508909216051 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 499 --> ([ 499 | 17 | 2 | 4768 | 1508909216086 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 500 --> ([ 500 | 17 | 3 | 3036 | 1508909216219 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 501 --> ([ 501 | 5 | 3 | 1533 | 1508909216251 | 'iOS' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 502 --> ([ 502 | 4 | 3 | 4618 | 1508909216530 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 503 --> ([ 503 | 16 | 3 | 7169 | 1508909216550 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 504 --> ([ 504 | 8 | 3 | 5670 | 1508909216617 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[35;1melasticsearch_1       |[0m [2017-10-25T05:26:56,689][INFO ][o.e.c.m.MetaDataCreateIndexService] [-Ryb53P] [.watcher-history-6-2017.10.25] creating index, cause [auto(bulk api)], templates [.watch-history-6], shards [1]/[1], mappings [doc]
[36;1mksql-datagen-users_1  |[0m 505 --> ([ 505 | 6 | 1 | 3999 | 1508909216800 | 'web' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[35;1melasticsearch_1       |[0m [2017-10-25T05:26:56,885][INFO ][o.e.c.m.MetaDataCreateIndexService] [-Ryb53P] [.monitoring-alerts-6] creating index, cause [auto(bulk api)], templates [.monitoring-alerts], shards [1]/[1], mappings [doc]
[36;1mksql-datagen-users_1  |[0m 506 --> ([ 506 | 1 | 1 | 8743 | 1508909217006 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 507 --> ([ 507 | 17 | 3 | 5288 | 1508909217223 | 'android' | 'Exceeded all my expectations. Thank you !' ])
[35;1melasticsearch_1       |[0m [2017-10-25T05:26:57,342][INFO ][o.e.c.m.MetaDataMappingService] [-Ryb53P] [.watcher-history-6-2017.10.25/1RNrQjgQQ0KLXMKuVcCklw] update_mapping [doc]
[36;1mksql-datagen-users_1  |[0m 508 --> ([ 508 | 6 | 3 | 6441 | 1508909217414 | 'android' | '(expletive deleted)' ])
[35;1melasticsearch_1       |[0m [2017-10-25T05:26:57,574][INFO ][o.e.c.m.MetaDataMappingService] [-Ryb53P] [.watcher-history-6-2017.10.25/1RNrQjgQQ0KLXMKuVcCklw] update_mapping [doc]
[36;1mksql-datagen-users_1  |[0m 509 --> ([ 509 | 18 | 4 | 9980 | 1508909217614 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 510 --> ([ 510 | 19 | 1 | 101 | 1508909217762 | 'iOS' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 511 --> ([ 511 | -1 | 1 | 464 | 1508909217966 | 'web' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 512 --> ([ 512 | 13 | 2 | 3346 | 1508909218114 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 513 --> ([ 513 | 14 | 4 | 153 | 1508909218145 | 'iOS' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 514 --> ([ 514 | 6 | 1 | 3519 | 1508909218440 | 'android' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 515 --> ([ 515 | 8 | 4 | 137 | 1508909218475 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 516 --> ([ 516 | 2 | 3 | 6475 | 1508909218615 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 517 --> ([ 517 | 9 | 3 | 6646 | 1508909218867 | 'iOS-test' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 518 --> ([ 518 | 6 | 4 | 2793 | 1508909218990 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 519 --> ([ 519 | 0 | 4 | 3611 | 1508909219023 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[35mkafka_1               |[0m [2017-10-25 05:26:59,119] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: _confluent-metrics-10. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[36;1mksql-datagen-users_1  |[0m 520 --> ([ 520 | 6 | 1 | 5856 | 1508909219262 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 521 --> ([ 521 | 6 | 1 | 7037 | 1508909219357 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 522 --> ([ 522 | 3 | 2 | 4864 | 1508909219438 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 523 --> ([ 523 | 4 | 4 | 510 | 1508909219552 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 524 --> ([ 524 | 8 | 3 | 8235 | 1508909219589 | 'iOS' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 525 --> ([ 525 | 6 | 2 | 3519 | 1508909219759 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 526 --> ([ 526 | 18 | 1 | 5453 | 1508909219921 | 'iOS' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 527 --> ([ 527 | 1 | 2 | 6925 | 1508909220011 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 528 --> ([ 528 | 14 | 2 | 7814 | 1508909220052 | 'web' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 529 --> ([ 529 | 12 | 3 | 2280 | 1508909220293 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 530 --> ([ 530 | 11 | 2 | 1142 | 1508909220534 | 'iOS' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 531 --> ([ 531 | 1 | 2 | 4270 | 1508909220573 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 532 --> ([ 532 | 3 | 1 | 5921 | 1508909220851 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 533 --> ([ 533 | 1 | 2 | 8069 | 1508909221135 | 'web' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 534 --> ([ 534 | 4 | 2 | 2898 | 1508909221305 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 535 --> ([ 535 | -1 | 4 | 5811 | 1508909221438 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 536 --> ([ 536 | 12 | 2 | 4637 | 1508909221731 | 'android' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 537 --> ([ 537 | 9 | 4 | 8971 | 1508909221884 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 538 --> ([ 538 | 11 | 2 | 6043 | 1508909221971 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 539 --> ([ 539 | 3 | 3 | 5915 | 1508909222005 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 540 --> ([ 540 | 15 | 2 | 9363 | 1508909222070 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 541 --> ([ 541 | 15 | 4 | 6353 | 1508909222100 | 'android' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 542 --> ([ 542 | 14 | 2 | 1671 | 1508909222204 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 543 --> ([ 543 | 9 | 2 | 6977 | 1508909222357 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 544 --> ([ 544 | 7 | 4 | 5334 | 1508909222409 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 545 --> ([ 545 | 3 | 1 | 5028 | 1508909222429 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 546 --> ([ 546 | 2 | 2 | 4500 | 1508909222501 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 547 --> ([ 547 | 3 | 1 | 9049 | 1508909222650 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 548 --> ([ 548 | 16 | 2 | 3069 | 1508909222804 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 549 --> ([ 549 | 14 | 4 | 8601 | 1508909222829 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 550 --> ([ 550 | 16 | 3 | 2191 | 1508909223107 | 'iOS' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 551 --> ([ 551 | 7 | 1 | 7164 | 1508909223132 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 552 --> ([ 552 | 15 | 3 | 7784 | 1508909223382 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 553 --> ([ 553 | 17 | 4 | 5021 | 1508909223664 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 554 --> ([ 554 | -1 | 1 | 1089 | 1508909223917 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 555 --> ([ 555 | 3 | 3 | 1354 | 1508909223933 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 556 --> ([ 556 | 10 | 4 | 7739 | 1508909224083 | 'iOS-test' | 'more peanuts please' ])
[33mzookeeper_1           |[0m [2017-10-25 05:27:04,159] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:setData cxid:0x322 zxid:0x1a1 txntype:-1 reqpath:n/a Error Path:/config/topics/_confluent-monitoring Error:KeeperErrorCode = NoNode for /config/topics/_confluent-monitoring (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:27:04,179] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x323 zxid:0x1a2 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 557 --> ([ 557 | 0 | 1 | 7316 | 1508909224199 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[35mkafka_1               |[0m [2017-10-25 05:27:04,201] INFO Topic creation {"version":1,"partitions":{"0":[1]}} (kafka.admin.AdminUtils$)
[35mkafka_1               |[0m [2017-10-25 05:27:04,212] INFO [KafkaApi-1] Auto creation of topic _confluent-monitoring with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[35mkafka_1               |[0m [2017-10-25 05:27:04,221] INFO [Controller 1]: New topics: [Set(_confluent-monitoring)], deleted topics: [Set()], new partition replica assignment [Map([_confluent-monitoring,0] -> List(1))] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:27:04,221] INFO [Controller 1]: New topic creation callback for [_confluent-monitoring,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:27:04,222] INFO [Controller 1]: New partition creation callback for [_confluent-monitoring,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:27:04,222] INFO [Partition state machine on Controller 1]: Invoking state change to NewPartition for partitions [_confluent-monitoring,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:27:04,222] TRACE Controller 1 epoch 1 changed partition [_confluent-monitoring,0] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:04,223] INFO [Replica state machine on controller 1]: Invoking state change to NewReplica for replicas [Topic=_confluent-monitoring,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:27:04,223] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_confluent-monitoring,0] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:04,224] INFO [Partition state machine on Controller 1]: Invoking state change to OnlinePartition for partitions [_confluent-monitoring,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:27:04,224] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [_confluent-monitoring,0] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:27:04,224] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [_confluent-monitoring,0] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:27:04,225] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x32b zxid:0x1a5 txntype:-1 reqpath:n/a Error Path:/brokers/topics/_confluent-monitoring/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/_confluent-monitoring/partitions/0 (org.apache.zookeeper.server.PrepRequestProcessor)
[33;1mconnect_1             |[0m [2017-10-25 05:27:04,220] WARN Error while fetching metadata with correlation id 1 : {_confluent-monitoring=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)
[36;1mksql-datagen-users_1  |[0m 558 --> ([ 558 | 14 | 2 | 3420 | 1508909224233 | 'android' | 'your team here rocks!' ])
[33mzookeeper_1           |[0m [2017-10-25 05:27:04,246] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x32c zxid:0x1a6 txntype:-1 reqpath:n/a Error Path:/brokers/topics/_confluent-monitoring/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/_confluent-monitoring/partitions (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:27:04,292] TRACE Controller 1 epoch 1 changed partition [_confluent-monitoring,0] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:04,292] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [_confluent-monitoring,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:04,293] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition _confluent-monitoring-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:04,293] INFO [Replica state machine on controller 1]: Invoking state change to OnlineReplica for replicas [Topic=_confluent-monitoring,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:27:04,294] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [_confluent-monitoring,0] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:04,295] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 25 from controller 1 epoch 1 for partition [_confluent-monitoring,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:04,295] TRACE Broker 1 handling LeaderAndIsr request correlationId 25 from controller 1 epoch 1 starting the become-leader transition for partition _confluent-monitoring-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:04,296] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions _confluent-monitoring-0 (kafka.server.ReplicaFetcherManager)
[35mkafka_1               |[0m [2017-10-25 05:27:04,300] INFO Loading producer state from offset 0 for partition _confluent-monitoring-0 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:27:04,301] INFO Completed load of log _confluent-monitoring-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:27:04,301] INFO Created log for partition [_confluent-monitoring,0] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:27:04,302] INFO Partition [_confluent-monitoring,0] on broker 1: No checkpointed highwatermark is found for partition _confluent-monitoring-0 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:27:04,302] INFO Partition [_confluent-monitoring,0] on broker 1: _confluent-monitoring-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:27:04,303] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 25 for partition _confluent-monitoring-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:04,303] TRACE Broker 1 completed LeaderAndIsr request correlationId 25 from controller 1 epoch 1 for the become-leader transition for partition _confluent-monitoring-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:04,303] TRACE Controller 1 epoch 1 received response {error_code=0,partitions=[{topic=_confluent-monitoring,partition=0,error_code=0}]} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:04,313] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition _confluent-monitoring-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 26 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:04,313] TRACE Controller 1 epoch 1 received response {error_code=0} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[36;1mksql-datagen-users_1  |[0m 559 --> ([ 559 | 10 | 2 | 4360 | 1508909224381 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 560 --> ([ 560 | 11 | 1 | 6527 | 1508909224661 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 561 --> ([ 561 | 9 | 1 | 4534 | 1508909224888 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 562 --> ([ 562 | 14 | 4 | 940 | 1508909224900 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 563 --> ([ 563 | 19 | 2 | 2053 | 1508909225126 | 'iOS' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 564 --> ([ 564 | 2 | 2 | 4318 | 1508909225165 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[35mkafka_1               |[0m [2017-10-25 05:27:05,281] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: _confluent-monitoring-0. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[36;1mksql-datagen-users_1  |[0m 565 --> ([ 565 | 3 | 1 | 7870 | 1508909225439 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 566 --> ([ 566 | -1 | 1 | 14 | 1508909225571 | 'web' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 567 --> ([ 567 | 11 | 3 | 1525 | 1508909225689 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[33mzookeeper_1           |[0m [2017-10-25 05:27:05,742] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:setData cxid:0x333 zxid:0x1aa txntype:-1 reqpath:n/a Error Path:/config/topics/ksql__commands Error:KeeperErrorCode = NoNode for /config/topics/ksql__commands (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:27:05,760] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x334 zxid:0x1ab txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:27:05,781] INFO Topic creation {"version":1,"partitions":{"0":[1]}} (kafka.admin.AdminUtils$)
[35mkafka_1               |[0m [2017-10-25 05:27:05,800] INFO [Controller 1]: New topics: [Set(ksql__commands)], deleted topics: [Set()], new partition replica assignment [Map([ksql__commands,0] -> List(1))] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:27:05,800] INFO [Controller 1]: New topic creation callback for [ksql__commands,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:27:05,801] INFO [Controller 1]: New partition creation callback for [ksql__commands,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:27:05,801] INFO [Partition state machine on Controller 1]: Invoking state change to NewPartition for partitions [ksql__commands,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:27:05,801] TRACE Controller 1 epoch 1 changed partition [ksql__commands,0] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:05,802] INFO [Replica state machine on controller 1]: Invoking state change to NewReplica for replicas [Topic=ksql__commands,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:27:05,804] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [ksql__commands,0] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:05,804] INFO [Partition state machine on Controller 1]: Invoking state change to OnlinePartition for partitions [ksql__commands,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:27:05,804] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [ksql__commands,0] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:27:05,804] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [ksql__commands,0] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:27:05,805] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x33c zxid:0x1ae txntype:-1 reqpath:n/a Error Path:/brokers/topics/ksql__commands/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/ksql__commands/partitions/0 (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:27:05,815] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x33d zxid:0x1af txntype:-1 reqpath:n/a Error Path:/brokers/topics/ksql__commands/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/ksql__commands/partitions (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:27:05,857] TRACE Controller 1 epoch 1 changed partition [ksql__commands,0] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:05,857] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [ksql__commands,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:05,857] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition ksql__commands-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:05,857] INFO [Replica state machine on controller 1]: Invoking state change to OnlineReplica for replicas [Topic=ksql__commands,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:27:05,857] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [ksql__commands,0] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:05,858] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 27 from controller 1 epoch 1 for partition [ksql__commands,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:05,859] TRACE Broker 1 handling LeaderAndIsr request correlationId 27 from controller 1 epoch 1 starting the become-leader transition for partition ksql__commands-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:05,859] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions ksql__commands-0 (kafka.server.ReplicaFetcherManager)
[35mkafka_1               |[0m [2017-10-25 05:27:05,862] INFO Loading producer state from offset 0 for partition ksql__commands-0 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:27:05,862] INFO Completed load of log ksql__commands-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:27:05,863] INFO Created log for partition [ksql__commands,0] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:27:05,863] INFO Partition [ksql__commands,0] on broker 1: No checkpointed highwatermark is found for partition ksql__commands-0 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:27:05,864] INFO Partition [ksql__commands,0] on broker 1: ksql__commands-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:27:05,864] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 27 for partition ksql__commands-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:05,864] TRACE Broker 1 completed LeaderAndIsr request correlationId 27 from controller 1 epoch 1 for the become-leader transition for partition ksql__commands-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:05,865] TRACE Controller 1 epoch 1 received response {error_code=0,partitions=[{topic=ksql__commands,partition=0,error_code=0}]} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:05,866] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition ksql__commands-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 28 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:05,868] TRACE Controller 1 epoch 1 received response {error_code=0} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[36;1mksql-datagen-users_1  |[0m 568 --> ([ 568 | 15 | 1 | 3039 | 1508909225979 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 569 --> ([ 569 | 1 | 2 | 7001 | 1508909226044 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 570 --> ([ 570 | 2 | 3 | 3150 | 1508909226308 | 'iOS' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 571 --> ([ 571 | 8 | 2 | 1896 | 1508909226500 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 572 --> ([ 572 | 10 | 2 | 511 | 1508909226762 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 573 --> ([ 573 | 16 | 3 | 1600 | 1508909226904 | 'web' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 574 --> ([ 574 | 15 | 2 | 1982 | 1508909227027 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 575 --> ([ 575 | 0 | 3 | 2807 | 1508909227059 | 'web' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 576 --> ([ 576 | 17 | 4 | 8516 | 1508909227356 | 'iOS' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 577 --> ([ 577 | 9 | 4 | 3058 | 1508909227425 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 578 --> ([ 578 | 0 | 3 | 6685 | 1508909227514 | 'iOS-test' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 579 --> ([ 579 | 15 | 1 | 5614 | 1508909227767 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 580 --> ([ 580 | 1 | 3 | 9827 | 1508909228063 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 581 --> ([ 581 | 17 | 3 | 1399 | 1508909228109 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 582 --> ([ 582 | 12 | 4 | 6850 | 1508909228290 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 583 --> ([ 583 | -1 | 2 | 2270 | 1508909228414 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 584 --> ([ 584 | 9 | 2 | 9165 | 1508909228639 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 585 --> ([ 585 | 14 | 1 | 2384 | 1508909228754 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 586 --> ([ 586 | 19 | 1 | 8792 | 1508909229007 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 587 --> ([ 587 | 1 | 3 | 6905 | 1508909229048 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 588 --> ([ 588 | 6 | 1 | 3116 | 1508909229127 | 'android' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 589 --> ([ 589 | 19 | 1 | 9817 | 1508909229150 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 590 --> ([ 590 | 10 | 4 | 4315 | 1508909229253 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 591 --> ([ 591 | 11 | 2 | 5107 | 1508909229479 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 592 --> ([ 592 | 0 | 4 | 3980 | 1508909229731 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 593 --> ([ 593 | 1 | 1 | 8049 | 1508909229976 | 'iOS-test' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 594 --> ([ 594 | 7 | 2 | 5906 | 1508909230024 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 595 --> ([ 595 | 17 | 1 | 5891 | 1508909230171 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 596 --> ([ 596 | 15 | 4 | 2586 | 1508909230358 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 597 --> ([ 597 | -1 | 2 | 9660 | 1508909230519 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 598 --> ([ 598 | 6 | 3 | 3399 | 1508909230637 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 599 --> ([ 599 | 16 | 4 | 1368 | 1508909230887 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 600 --> ([ 600 | -1 | 1 | 5025 | 1508909230903 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 601 --> ([ 601 | 10 | 3 | 3362 | 1508909230904 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 602 --> ([ 602 | 0 | 1 | 7056 | 1508909230932 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 603 --> ([ 603 | 11 | 1 | 9099 | 1508909231058 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 604 --> ([ 604 | 0 | 4 | 8351 | 1508909231226 | 'web' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 605 --> ([ 605 | 0 | 1 | 7073 | 1508909231344 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 606 --> ([ 606 | -1 | 4 | 2117 | 1508909231433 | 'web' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 607 --> ([ 607 | 18 | 3 | 2170 | 1508909231495 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 608 --> ([ 608 | 2 | 4 | 221 | 1508909231591 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 609 --> ([ 609 | -1 | 3 | 1210 | 1508909231644 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 610 --> ([ 610 | 16 | 4 | 9775 | 1508909231840 | 'android' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 611 --> ([ 611 | 7 | 2 | 1926 | 1508909231878 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 612 --> ([ 612 | 14 | 2 | 7463 | 1508909231967 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 613 --> ([ 613 | 7 | 4 | 7148 | 1508909232036 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 614 --> ([ 614 | 11 | 3 | 9623 | 1508909232106 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 615 --> ([ 615 | 16 | 3 | 8431 | 1508909232181 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 616 --> ([ 616 | 2 | 2 | 8626 | 1508909232185 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 617 --> ([ 617 | 3 | 3 | 5908 | 1508909232275 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 618 --> ([ 618 | 4 | 2 | 8513 | 1508909232468 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 619 --> ([ 619 | 3 | 1 | 3610 | 1508909232685 | 'web' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 620 --> ([ 620 | 9 | 1 | 5437 | 1508909232719 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 621 --> ([ 621 | 17 | 3 | 2014 | 1508909232934 | 'iOS' | 'meh' ])
[35mkafka_1               |[0m [2017-10-25 05:27:12,983] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[36;1mksql-datagen-users_1  |[0m 622 --> ([ 622 | 19 | 2 | 887 | 1508909233088 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 623 --> ([ 623 | 10 | 1 | 2083 | 1508909233337 | 'web' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 624 --> ([ 624 | 5 | 4 | 8224 | 1508909233445 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 625 --> ([ 625 | 10 | 4 | 7550 | 1508909233706 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 626 --> ([ 626 | 17 | 4 | 5536 | 1508909233726 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 627 --> ([ 627 | -1 | 2 | 4704 | 1508909233729 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 628 --> ([ 628 | 12 | 3 | 288 | 1508909233815 | 'iOS' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 629 --> ([ 629 | 9 | 3 | 1433 | 1508909233861 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 630 --> ([ 630 | 9 | 4 | 3825 | 1508909233862 | 'iOS-test' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 631 --> ([ 631 | 7 | 1 | 9790 | 1508909233920 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 632 --> ([ 632 | 15 | 2 | 6199 | 1508909234023 | 'iOS-test' | 'your team here rocks!' ])
[35mkafka_1               |[0m [2017-10-25 05:27:14,164] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: _confluent-metrics-9. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[36;1mksql-datagen-users_1  |[0m 633 --> ([ 633 | 7 | 3 | 8844 | 1508909234287 | 'web' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 634 --> ([ 634 | 13 | 1 | 4339 | 1508909234573 | 'android' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 635 --> ([ 635 | 8 | 4 | 7045 | 1508909234776 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 636 --> ([ 636 | 14 | 1 | 9066 | 1508909234914 | 'web' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 637 --> ([ 637 | 9 | 4 | 5897 | 1508909234951 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 638 --> ([ 638 | 2 | 4 | 8986 | 1508909235016 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 639 --> ([ 639 | 18 | 1 | 9178 | 1508909235035 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 640 --> ([ 640 | 14 | 4 | 1727 | 1508909235157 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 641 --> ([ 641 | 13 | 4 | 4465 | 1508909235396 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 642 --> ([ 642 | 13 | 2 | 5556 | 1508909235524 | 'android' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 643 --> ([ 643 | 7 | 2 | 6223 | 1508909235779 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 644 --> ([ 644 | 10 | 1 | 6893 | 1508909235951 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 645 --> ([ 645 | -1 | 2 | 2227 | 1508909236168 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 646 --> ([ 646 | 12 | 4 | 1696 | 1508909236415 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 647 --> ([ 647 | 10 | 1 | 7546 | 1508909236504 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 648 --> ([ 648 | 3 | 4 | 1251 | 1508909236684 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 649 --> ([ 649 | 8 | 2 | 2897 | 1508909236810 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 650 --> ([ 650 | 12 | 4 | 5365 | 1508909237013 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 651 --> ([ 651 | 6 | 2 | 6554 | 1508909237092 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 652 --> ([ 652 | 6 | 4 | 8484 | 1508909237265 | 'iOS' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 653 --> ([ 653 | 19 | 1 | 1658 | 1508909237526 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 654 --> ([ 654 | 19 | 4 | 6711 | 1508909237723 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 655 --> ([ 655 | -1 | 3 | 3095 | 1508909237724 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 656 --> ([ 656 | 10 | 1 | 6270 | 1508909237795 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 657 --> ([ 657 | 16 | 1 | 781 | 1508909238054 | 'web' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 658 --> ([ 658 | 9 | 2 | 6399 | 1508909238216 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 659 --> ([ 659 | 8 | 3 | 8550 | 1508909238414 | 'ios' | 'more peanuts please' ])
[35mkafka_1               |[0m [2017-10-25 05:27:18,451] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: ksql__commands-0. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[36;1mksql-datagen-users_1  |[0m 660 --> ([ 660 | 3 | 4 | 2193 | 1508909238549 | 'web' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 661 --> ([ 661 | 4 | 3 | 447 | 1508909238708 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 662 --> ([ 662 | 4 | 1 | 5389 | 1508909238987 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 663 --> ([ 663 | 8 | 2 | 8638 | 1508909239090 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 664 --> ([ 664 | 14 | 3 | 3625 | 1508909239102 | 'iOS-test' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 665 --> ([ 665 | 9 | 1 | 9759 | 1508909239117 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 666 --> ([ 666 | 10 | 1 | 1153 | 1508909239294 | 'iOS-test' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 667 --> ([ 667 | -1 | 1 | 8890 | 1508909239321 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 668 --> ([ 668 | 18 | 2 | 6537 | 1508909239452 | 'web' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 669 --> ([ 669 | 14 | 2 | 2962 | 1508909239752 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 670 --> ([ 670 | 13 | 4 | 5338 | 1508909239937 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 671 --> ([ 671 | 10 | 4 | 7836 | 1508909240038 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 672 --> ([ 672 | 14 | 2 | 8270 | 1508909240170 | 'ios' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 673 --> ([ 673 | 18 | 1 | 5827 | 1508909240340 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 674 --> ([ 674 | 1 | 4 | 7885 | 1508909240391 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 675 --> ([ 675 | 12 | 2 | 2889 | 1508909240437 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 676 --> ([ 676 | 13 | 1 | 9236 | 1508909240579 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 677 --> ([ 677 | 9 | 4 | 7277 | 1508909240719 | 'android' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 678 --> ([ 678 | 14 | 1 | 2578 | 1508909240880 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 679 --> ([ 679 | 17 | 1 | 3906 | 1508909240999 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 680 --> ([ 680 | 18 | 3 | 8573 | 1508909241131 | 'android' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 681 --> ([ 681 | 3 | 1 | 1166 | 1508909241324 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 682 --> ([ 682 | 11 | 3 | 9042 | 1508909241513 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 683 --> ([ 683 | 18 | 4 | 9468 | 1508909241678 | 'android' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 684 --> ([ 684 | 8 | 1 | 8743 | 1508909241699 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 685 --> ([ 685 | 17 | 3 | 3275 | 1508909241871 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 686 --> ([ 686 | 15 | 4 | 6199 | 1508909242131 | 'iOS' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 687 --> ([ 687 | 12 | 2 | 844 | 1508909242274 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 688 --> ([ 688 | 19 | 2 | 4081 | 1508909242339 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 689 --> ([ 689 | -1 | 3 | 8498 | 1508909242556 | 'android' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 690 --> ([ 690 | 5 | 2 | 2151 | 1508909242767 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 691 --> ([ 691 | 15 | 4 | 8634 | 1508909242845 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 692 --> ([ 692 | 9 | 2 | 8275 | 1508909242948 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 693 --> ([ 693 | 8 | 1 | 3232 | 1508909243160 | 'ios' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 694 --> ([ 694 | 18 | 3 | 5519 | 1508909243317 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 695 --> ([ 695 | 10 | 4 | 7148 | 1508909243558 | 'iOS-test' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 696 --> ([ 696 | -1 | 1 | 5813 | 1508909243699 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[35mkafka_1               |[0m [2017-10-25 05:27:23,707] INFO [GroupCoordinator 1]: Preparing to rebalance group ksql_transient_3354176758863856004_1508909243329 with old generation 0 (__consumer_offsets-10) (kafka.coordinator.group.GroupCoordinator)
[36;1mksql-datagen-users_1  |[0m 697 --> ([ 697 | 17 | 1 | 7935 | 1508909243917 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 698 --> ([ 698 | 3 | 1 | 3231 | 1508909244175 | 'web' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 699 --> ([ 699 | 5 | 1 | 1437 | 1508909244282 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 700 --> ([ 700 | 16 | 2 | 2352 | 1508909244527 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 701 --> ([ 701 | 10 | 1 | 4923 | 1508909244561 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 702 --> ([ 702 | 13 | 2 | 9740 | 1508909244590 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 703 --> ([ 703 | 19 | 3 | 2043 | 1508909244867 | 'iOS' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 704 --> ([ 704 | 15 | 4 | 9045 | 1508909245157 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 705 --> ([ 705 | 11 | 1 | 6315 | 1508909245200 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 706 --> ([ 706 | 2 | 1 | 2709 | 1508909245494 | 'iOS-test' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 707 --> ([ 707 | 1 | 4 | 2446 | 1508909245576 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 708 --> ([ 708 | 6 | 4 | 5822 | 1508909245630 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 709 --> ([ 709 | 12 | 4 | 7213 | 1508909245693 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 710 --> ([ 710 | 13 | 4 | 115 | 1508909245729 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 711 --> ([ 711 | 8 | 2 | 4191 | 1508909245830 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 712 --> ([ 712 | 7 | 3 | 8589 | 1508909245853 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 713 --> ([ 713 | -1 | 1 | 9051 | 1508909245991 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 714 --> ([ 714 | 3 | 4 | 3821 | 1508909246221 | 'android' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 715 --> ([ 715 | 14 | 4 | 4726 | 1508909246495 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 716 --> ([ 716 | -1 | 3 | 5404 | 1508909246571 | 'web' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 717 --> ([ 717 | 8 | 2 | 8155 | 1508909246774 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 718 --> ([ 718 | 16 | 2 | 4588 | 1508909247017 | 'android' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 719 --> ([ 719 | 6 | 3 | 2922 | 1508909247298 | 'iOS' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 720 --> ([ 720 | -1 | 4 | 6575 | 1508909247315 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 721 --> ([ 721 | 0 | 2 | 4484 | 1508909247476 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 722 --> ([ 722 | 8 | 3 | 3521 | 1508909247682 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 723 --> ([ 723 | 13 | 4 | 7300 | 1508909247710 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 724 --> ([ 724 | 4 | 3 | 3715 | 1508909247915 | 'android' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 725 --> ([ 725 | 16 | 3 | 5582 | 1508909247938 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 726 --> ([ 726 | 3 | 2 | 4973 | 1508909248143 | 'web' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 727 --> ([ 727 | 10 | 1 | 1948 | 1508909248183 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 728 --> ([ 728 | 19 | 2 | 2738 | 1508909248300 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 729 --> ([ 729 | 15 | 4 | 717 | 1508909248329 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 730 --> ([ 730 | 8 | 2 | 5890 | 1508909248440 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 731 --> ([ 731 | 5 | 2 | 5126 | 1508909248458 | 'web' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 732 --> ([ 732 | 13 | 1 | 5801 | 1508909248532 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 733 --> ([ 733 | 5 | 1 | 208 | 1508909248589 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 734 --> ([ 734 | 2 | 3 | 6600 | 1508909248773 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 735 --> ([ 735 | 19 | 2 | 5584 | 1508909248858 | 'iOS' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 736 --> ([ 736 | 2 | 4 | 7197 | 1508909249005 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 737 --> ([ 737 | 0 | 4 | 4965 | 1508909249065 | 'iOS-test' | 'more peanuts please' ])
[35mkafka_1               |[0m [2017-10-25 05:27:29,105] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: _confluent-metrics-3. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[36;1mksql-datagen-users_1  |[0m 738 --> ([ 738 | 1 | 1 | 8047 | 1508909249303 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 739 --> ([ 739 | 0 | 4 | 1561 | 1508909249339 | 'iOS' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 740 --> ([ 740 | 0 | 3 | 8179 | 1508909249549 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 741 --> ([ 741 | 7 | 3 | 1215 | 1508909249652 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 742 --> ([ 742 | 8 | 3 | 478 | 1508909249710 | 'iOS' | 'more peanuts please' ])
[35mkafka_1               |[0m [2017-10-25 05:27:29,711] INFO [GroupCoordinator 1]: Stabilized group ksql_transient_3354176758863856004_1508909243329 generation 1 (__consumer_offsets-10) (kafka.coordinator.group.GroupCoordinator)
[33mzookeeper_1           |[0m [2017-10-25 05:27:29,837] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:setData cxid:0x344 zxid:0x1b3 txntype:-1 reqpath:n/a Error Path:/config/topics/ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition Error:KeeperErrorCode = NoNode for /config/topics/ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:27:29,851] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x345 zxid:0x1b4 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:27:29,877] INFO Topic creation {"version":1,"partitions":{"0":[1]}} (kafka.admin.AdminUtils$)
[35mkafka_1               |[0m [2017-10-25 05:27:29,889] INFO [Controller 1]: New topics: [Set(ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition)], deleted topics: [Set()], new partition replica assignment [Map([ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition,0] -> List(1))] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:27:29,890] INFO [Controller 1]: New topic creation callback for [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:27:29,891] INFO [Controller 1]: New partition creation callback for [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:27:29,891] INFO [Partition state machine on Controller 1]: Invoking state change to NewPartition for partitions [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:27:29,891] TRACE Controller 1 epoch 1 changed partition [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition,0] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:29,891] INFO [Replica state machine on controller 1]: Invoking state change to NewReplica for replicas [Topic=ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:27:29,892] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition,0] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:29,893] INFO [Partition state machine on Controller 1]: Invoking state change to OnlinePartition for partitions [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:27:29,893] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition,0] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:27:29,893] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition,0] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:27:29,895] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x34d zxid:0x1b7 txntype:-1 reqpath:n/a Error Path:/brokers/topics/ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition/partitions/0 (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:27:29,907] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x34e zxid:0x1b8 txntype:-1 reqpath:n/a Error Path:/brokers/topics/ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition/partitions (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:27:29,961] TRACE Controller 1 epoch 1 changed partition [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition,0] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:29,961] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:29,962] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:29,962] INFO [Replica state machine on controller 1]: Invoking state change to OnlineReplica for replicas [Topic=ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:27:29,962] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition,0] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:29,962] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 29 from controller 1 epoch 1 for partition [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:29,963] TRACE Broker 1 handling LeaderAndIsr request correlationId 29 from controller 1 epoch 1 starting the become-leader transition for partition ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:29,963] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition-0 (kafka.server.ReplicaFetcherManager)
[36;1mksql-datagen-users_1  |[0m 743 --> ([ 743 | 15 | 3 | 7477 | 1508909249970 | 'web' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[35mkafka_1               |[0m [2017-10-25 05:27:29,971] INFO Loading producer state from offset 0 for partition ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition-0 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:27:29,972] INFO Completed load of log ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition-0 with 1 log segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:27:29,973] INFO Created log for partition [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition,0] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> delete, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:27:29,974] INFO Partition [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition,0] on broker 1: No checkpointed highwatermark is found for partition ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition-0 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:27:29,989] INFO Partition [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition,0] on broker 1: ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:27:29,989] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 29 for partition ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:29,989] TRACE Broker 1 completed LeaderAndIsr request correlationId 29 from controller 1 epoch 1 for the become-leader transition for partition ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:29,990] TRACE Controller 1 epoch 1 received response {error_code=0,partitions=[{topic=ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition,partition=0,error_code=0}]} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:29,990] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition ksql_transient_3354176758863856004_1508909243329-USERS_statestore-repartition-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 30 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:29,992] TRACE Controller 1 epoch 1 received response {error_code=0} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[33mzookeeper_1           |[0m [2017-10-25 05:27:30,153] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:setData cxid:0x355 zxid:0x1bc txntype:-1 reqpath:n/a Error Path:/config/topics/ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog Error:KeeperErrorCode = NoNode for /config/topics/ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 744 --> ([ 744 | 10 | 4 | 5746 | 1508909250156 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[33mzookeeper_1           |[0m [2017-10-25 05:27:30,164] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x356 zxid:0x1bd txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:27:30,191] INFO Topic creation {"version":1,"partitions":{"0":[1]}} (kafka.admin.AdminUtils$)
[35mkafka_1               |[0m [2017-10-25 05:27:30,206] INFO [Controller 1]: New topics: [Set(ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog)], deleted topics: [Set()], new partition replica assignment [Map([ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog,0] -> List(1))] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:27:30,206] INFO [Controller 1]: New topic creation callback for [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:27:30,208] INFO [Controller 1]: New partition creation callback for [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:27:30,208] INFO [Partition state machine on Controller 1]: Invoking state change to NewPartition for partitions [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:27:30,208] TRACE Controller 1 epoch 1 changed partition [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog,0] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:30,208] INFO [Replica state machine on controller 1]: Invoking state change to NewReplica for replicas [Topic=ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:27:30,209] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog,0] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:30,209] INFO [Partition state machine on Controller 1]: Invoking state change to OnlinePartition for partitions [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:27:30,210] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog,0] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:27:30,210] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog,0] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:27:30,211] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x35e zxid:0x1c0 txntype:-1 reqpath:n/a Error Path:/brokers/topics/ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog/partitions/0 (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:27:30,223] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x35f zxid:0x1c1 txntype:-1 reqpath:n/a Error Path:/brokers/topics/ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog/partitions (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:27:30,266] TRACE Controller 1 epoch 1 changed partition [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog,0] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:30,266] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:30,266] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:30,267] INFO [Replica state machine on controller 1]: Invoking state change to OnlineReplica for replicas [Topic=ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:27:30,267] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 31 from controller 1 epoch 1 for partition [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:30,267] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog,0] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:30,267] TRACE Broker 1 handling LeaderAndIsr request correlationId 31 from controller 1 epoch 1 starting the become-leader transition for partition ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:30,267] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog-0 (kafka.server.ReplicaFetcherManager)
[35mkafka_1               |[0m [2017-10-25 05:27:30,270] INFO Loading producer state from offset 0 for partition ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog-0 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:27:30,270] INFO Completed load of log ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:27:30,270] INFO Created log for partition [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog,0] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:27:30,271] INFO Partition [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog,0] on broker 1: No checkpointed highwatermark is found for partition ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog-0 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:27:30,271] INFO Partition [ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog,0] on broker 1: ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:27:30,271] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 31 for partition ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:30,271] TRACE Broker 1 completed LeaderAndIsr request correlationId 31 from controller 1 epoch 1 for the become-leader transition for partition ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:30,271] TRACE Controller 1 epoch 1 received response {error_code=0,partitions=[{topic=ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog,partition=0,error_code=0}]} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:30,272] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition ksql_transient_3354176758863856004_1508909243329-USERS_statestore-changelog-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 32 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:27:30,274] TRACE Controller 1 epoch 1 received response {error_code=0} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[36;1mksql-datagen-users_1  |[0m 745 --> ([ 745 | 3 | 4 | 5410 | 1508909250326 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[35mkafka_1               |[0m [2017-10-25 05:27:30,347] INFO [GroupCoordinator 1]: Assignment received from leader for group ksql_transient_3354176758863856004_1508909243329 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[35mkafka_1               |[0m [2017-10-25 05:27:30,347] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-10. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[35mkafka_1               |[0m [2017-10-25 05:27:30,405] INFO [GroupCoordinator 1]: Preparing to rebalance group ksql_transient_3354176758863856004_1508909243329 with old generation 1 (__consumer_offsets-10) (kafka.coordinator.group.GroupCoordinator)
[36;1mksql-datagen-users_1  |[0m 746 --> ([ 746 | 2 | 4 | 8554 | 1508909250517 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 747 --> ([ 747 | 3 | 3 | 3339 | 1508909250721 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 748 --> ([ 748 | 9 | 4 | 363 | 1508909250907 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 749 --> ([ 749 | 19 | 1 | 4359 | 1508909251009 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 750 --> ([ 750 | 14 | 2 | 7944 | 1508909251176 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 751 --> ([ 751 | 0 | 2 | 1817 | 1508909251317 | 'web' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 752 --> ([ 752 | 19 | 4 | 7627 | 1508909251535 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 753 --> ([ 753 | 18 | 2 | 5531 | 1508909251548 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 754 --> ([ 754 | 13 | 3 | 8322 | 1508909251734 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 755 --> ([ 755 | 9 | 3 | 8036 | 1508909251793 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 756 --> ([ 756 | 18 | 4 | 8893 | 1508909251987 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 757 --> ([ 757 | 15 | 3 | 2564 | 1508909252212 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 758 --> ([ 758 | 9 | 4 | 7839 | 1508909252502 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 759 --> ([ 759 | 16 | 1 | 7057 | 1508909252688 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 760 --> ([ 760 | 4 | 3 | 5821 | 1508909252855 | 'android' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 761 --> ([ 761 | 14 | 4 | 963 | 1508909253026 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 762 --> ([ 762 | 4 | 1 | 4026 | 1508909253262 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 763 --> ([ 763 | 0 | 4 | 4045 | 1508909253474 | 'android' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 764 --> ([ 764 | 12 | 1 | 3885 | 1508909253478 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[35mkafka_1               |[0m [2017-10-25 05:27:33,526] INFO [GroupCoordinator 1]: Stabilized group ksql_transient_3354176758863856004_1508909243329 generation 2 (__consumer_offsets-10) (kafka.coordinator.group.GroupCoordinator)
[36;1mksql-datagen-users_1  |[0m 765 --> ([ 765 | 5 | 4 | 9005 | 1508909253708 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[35mkafka_1               |[0m [2017-10-25 05:27:33,743] INFO [GroupCoordinator 1]: Assignment received from leader for group ksql_transient_3354176758863856004_1508909243329 for generation 2 (kafka.coordinator.group.GroupCoordinator)
[36;1mksql-datagen-users_1  |[0m 766 --> ([ 766 | 16 | 2 | 6148 | 1508909253963 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 767 --> ([ 767 | 0 | 1 | 2146 | 1508909254237 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 768 --> ([ 768 | 12 | 4 | 9826 | 1508909254344 | 'web' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 769 --> ([ 769 | 2 | 1 | 8831 | 1508909254474 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 770 --> ([ 770 | 12 | 2 | 4006 | 1508909254764 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 771 --> ([ 771 | 3 | 3 | 3044 | 1508909254829 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 772 --> ([ 772 | 9 | 4 | 8045 | 1508909254944 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 773 --> ([ 773 | 17 | 1 | 2727 | 1508909255203 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 774 --> ([ 774 | 19 | 2 | 8169 | 1508909255223 | 'iOS-test' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 775 --> ([ 775 | 4 | 4 | 5233 | 1508909255233 | 'iOS-test' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 776 --> ([ 776 | 0 | 1 | 947 | 1508909255517 | 'web' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 777 --> ([ 777 | 12 | 2 | 9609 | 1508909255736 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 778 --> ([ 778 | 16 | 4 | 6679 | 1508909256005 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 779 --> ([ 779 | 6 | 4 | 4966 | 1508909256275 | 'iOS' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 780 --> ([ 780 | 19 | 1 | 8216 | 1508909256375 | 'android' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 781 --> ([ 781 | 0 | 2 | 6713 | 1508909256626 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 782 --> ([ 782 | 10 | 4 | 3169 | 1508909256804 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 783 --> ([ 783 | 0 | 2 | 4905 | 1508909257001 | 'android' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 784 --> ([ 784 | 6 | 1 | 68 | 1508909257290 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 785 --> ([ 785 | 11 | 3 | 1455 | 1508909257371 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 786 --> ([ 786 | 17 | 4 | 1451 | 1508909257613 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 787 --> ([ 787 | 3 | 4 | 5473 | 1508909257869 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 788 --> ([ 788 | 11 | 1 | 9401 | 1508909258153 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 789 --> ([ 789 | 6 | 2 | 8504 | 1508909258204 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 790 --> ([ 790 | 10 | 1 | 1537 | 1508909258485 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 791 --> ([ 791 | 2 | 1 | 3892 | 1508909258544 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 792 --> ([ 792 | 19 | 3 | 7945 | 1508909258710 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 793 --> ([ 793 | 8 | 4 | 7056 | 1508909258803 | 'iOS' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 794 --> ([ 794 | 10 | 3 | 2113 | 1508909258899 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 795 --> ([ 795 | 14 | 4 | 5924 | 1508909258930 | 'android' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 796 --> ([ 796 | 14 | 3 | 1307 | 1508909259058 | 'iOS-test' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 797 --> ([ 797 | 14 | 2 | 3282 | 1508909259245 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 798 --> ([ 798 | 1 | 1 | 4450 | 1508909259522 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 799 --> ([ 799 | 6 | 4 | 5298 | 1508909259536 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 800 --> ([ 800 | 11 | 2 | 7990 | 1508909259793 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 801 --> ([ 801 | 2 | 2 | 2987 | 1508909260074 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 802 --> ([ 802 | 18 | 2 | 2765 | 1508909260103 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 803 --> ([ 803 | 6 | 1 | 5846 | 1508909260356 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 804 --> ([ 804 | 9 | 1 | 7363 | 1508909260562 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 805 --> ([ 805 | 2 | 2 | 9464 | 1508909260738 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 806 --> ([ 806 | 2 | 2 | 6027 | 1508909260951 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 807 --> ([ 807 | 4 | 1 | 1319 | 1508909261169 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 808 --> ([ 808 | 10 | 3 | 1548 | 1508909261196 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 809 --> ([ 809 | 1 | 1 | 1541 | 1508909261481 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 810 --> ([ 810 | 1 | 4 | 6210 | 1508909261629 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 811 --> ([ 811 | 11 | 3 | 7907 | 1508909261709 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 812 --> ([ 812 | 16 | 1 | 6273 | 1508909261802 | 'web' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 813 --> ([ 813 | 13 | 3 | 6810 | 1508909261976 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 814 --> ([ 814 | 8 | 4 | 7988 | 1508909262018 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 815 --> ([ 815 | 13 | 1 | 4459 | 1508909262295 | 'web' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 816 --> ([ 816 | 13 | 4 | 5677 | 1508909262587 | 'android' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 817 --> ([ 817 | 19 | 2 | 5082 | 1508909262737 | 'android' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 818 --> ([ 818 | 12 | 4 | 4875 | 1508909262788 | 'iOS' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 819 --> ([ 819 | 17 | 3 | 5383 | 1508909262915 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 820 --> ([ 820 | 2 | 4 | 6033 | 1508909263007 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 821 --> ([ 821 | -1 | 2 | 7004 | 1508909263279 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 822 --> ([ 822 | 12 | 3 | 6812 | 1508909263512 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 823 --> ([ 823 | 13 | 2 | 1260 | 1508909263704 | 'web' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 824 --> ([ 824 | 6 | 3 | 3446 | 1508909263946 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[35mkafka_1               |[0m [2017-10-25 05:27:44,113] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: _confluent-metrics-6. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[36;1mksql-datagen-users_1  |[0m 825 --> ([ 825 | 16 | 2 | 2895 | 1508909264240 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 826 --> ([ 826 | 10 | 4 | 4206 | 1508909264389 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 827 --> ([ 827 | 7 | 3 | 2133 | 1508909264660 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 828 --> ([ 828 | 0 | 3 | 472 | 1508909264842 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 829 --> ([ 829 | 11 | 4 | 6759 | 1508909264959 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 830 --> ([ 830 | 4 | 2 | 3396 | 1508909265044 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 831 --> ([ 831 | 13 | 1 | 8512 | 1508909265259 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 832 --> ([ 832 | 19 | 2 | 6701 | 1508909265445 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 833 --> ([ 833 | 17 | 2 | 520 | 1508909265669 | 'iOS' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 834 --> ([ 834 | 6 | 1 | 8586 | 1508909265706 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 835 --> ([ 835 | 14 | 3 | 5063 | 1508909265827 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 836 --> ([ 836 | 0 | 4 | 8278 | 1508909265901 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 837 --> ([ 837 | 10 | 1 | 3412 | 1508909266095 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 838 --> ([ 838 | 18 | 3 | 5211 | 1508909266392 | 'iOS' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 839 --> ([ 839 | -1 | 4 | 1986 | 1508909266528 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 840 --> ([ 840 | 5 | 2 | 1363 | 1508909266820 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 841 --> ([ 841 | -1 | 3 | 1609 | 1508909267059 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 842 --> ([ 842 | 13 | 3 | 9742 | 1508909267068 | 'android' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 843 --> ([ 843 | 19 | 1 | 9104 | 1508909267177 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 844 --> ([ 844 | 18 | 1 | 4544 | 1508909267353 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 845 --> ([ 845 | 0 | 1 | 3013 | 1508909267482 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 846 --> ([ 846 | 16 | 1 | 962 | 1508909267685 | 'iOS' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 847 --> ([ 847 | 7 | 4 | 7641 | 1508909267737 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 848 --> ([ 848 | 10 | 3 | 5198 | 1508909267872 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 849 --> ([ 849 | 17 | 4 | 2265 | 1508909268014 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 850 --> ([ 850 | 9 | 2 | 7649 | 1508909268303 | 'ios' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 851 --> ([ 851 | 0 | 4 | 4684 | 1508909268391 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 852 --> ([ 852 | 2 | 2 | 4788 | 1508909268412 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 853 --> ([ 853 | -1 | 1 | 930 | 1508909268660 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 854 --> ([ 854 | 11 | 3 | 7746 | 1508909268703 | 'web' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 855 --> ([ 855 | 19 | 1 | 5357 | 1508909268998 | 'web' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 856 --> ([ 856 | 1 | 1 | 8111 | 1508909269283 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 857 --> ([ 857 | 0 | 3 | 7891 | 1508909269505 | 'iOS-test' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 858 --> ([ 858 | 9 | 1 | 3941 | 1508909269553 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 859 --> ([ 859 | 3 | 2 | 7684 | 1508909269695 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 860 --> ([ 860 | 11 | 2 | 1495 | 1508909269814 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 861 --> ([ 861 | 14 | 2 | 4937 | 1508909269965 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 862 --> ([ 862 | 2 | 3 | 713 | 1508909270236 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 863 --> ([ 863 | 9 | 4 | 5146 | 1508909270514 | 'ios' | 'is this as good as it gets? really ?' ])
[35mkafka_1               |[0m [2017-10-25 05:27:50,559] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: connect-offsets-24. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[36;1mksql-datagen-users_1  |[0m 864 --> ([ 864 | 5 | 3 | 4925 | 1508909270580 | 'android' | 'worst. flight. ever. #neveragain' ])
[35mkafka_1               |[0m [2017-10-25 05:27:50,587] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: connect-offsets-2. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[35mkafka_1               |[0m [2017-10-25 05:27:50,611] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: connect-offsets-6. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[35mkafka_1               |[0m [2017-10-25 05:27:50,631] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: connect-offsets-5. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[35mkafka_1               |[0m [2017-10-25 05:27:50,655] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: connect-offsets-21. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[33;1mconnect_1             |[0m [2017-10-25 05:27:50,681] INFO Finished WorkerSourceTask{id=MySQLSource-0} commitOffsets successfully in 2071 ms (org.apache.kafka.connect.runtime.WorkerSourceTask)
[36;1mksql-datagen-users_1  |[0m 865 --> ([ 865 | 10 | 1 | 2537 | 1508909270776 | 'iOS-test' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 866 --> ([ 866 | 15 | 2 | 7816 | 1508909270963 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 867 --> ([ 867 | 14 | 4 | 1931 | 1508909270993 | 'web' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 868 --> ([ 868 | 9 | 3 | 3143 | 1508909271025 | 'web' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 869 --> ([ 869 | 7 | 4 | 818 | 1508909271089 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 870 --> ([ 870 | 3 | 2 | 5215 | 1508909271126 | 'iOS-test' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 871 --> ([ 871 | 13 | 2 | 5881 | 1508909271254 | 'web' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 872 --> ([ 872 | 11 | 1 | 661 | 1508909271343 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 873 --> ([ 873 | 9 | 4 | 738 | 1508909271367 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 874 --> ([ 874 | 13 | 3 | 4453 | 1508909271493 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 875 --> ([ 875 | 1 | 4 | 4145 | 1508909271521 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 876 --> ([ 876 | 12 | 1 | 5743 | 1508909271787 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 877 --> ([ 877 | 18 | 2 | 451 | 1508909271845 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 878 --> ([ 878 | 9 | 3 | 1478 | 1508909272072 | 'web' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 879 --> ([ 879 | 17 | 4 | 6508 | 1508909272268 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 880 --> ([ 880 | 11 | 2 | 8323 | 1508909272514 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 881 --> ([ 881 | 4 | 1 | 1867 | 1508909272757 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[35mkafka_1               |[0m [2017-10-25 05:27:52,972] INFO Rolled new log segment for 'mysql-routes-0' in 5 ms. (kafka.log.Log)
[36;1mksql-datagen-users_1  |[0m 882 --> ([ 882 | 17 | 3 | 5908 | 1508909272979 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 883 --> ([ 883 | 6 | 1 | 1432 | 1508909273037 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 884 --> ([ 884 | 14 | 4 | 6870 | 1508909273074 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 885 --> ([ 885 | 0 | 4 | 2414 | 1508909273193 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 886 --> ([ 886 | 14 | 1 | 1747 | 1508909273278 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 887 --> ([ 887 | 0 | 3 | 9776 | 1508909273331 | 'iOS-test' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 888 --> ([ 888 | 2 | 4 | 9190 | 1508909273433 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 889 --> ([ 889 | 10 | 2 | 6981 | 1508909273585 | 'iOS' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 890 --> ([ 890 | 16 | 2 | 4533 | 1508909273810 | 'web' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 891 --> ([ 891 | 8 | 4 | 1238 | 1508909274096 | 'ios' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 892 --> ([ 892 | 3 | 1 | 2194 | 1508909274279 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 893 --> ([ 893 | 17 | 1 | 5077 | 1508909274392 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 894 --> ([ 894 | 11 | 3 | 9950 | 1508909274590 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 895 --> ([ 895 | 18 | 1 | 8452 | 1508909274642 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 896 --> ([ 896 | 11 | 4 | 5584 | 1508909274661 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 897 --> ([ 897 | 17 | 3 | 8162 | 1508909274744 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 898 --> ([ 898 | 9 | 1 | 8875 | 1508909274864 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 899 --> ([ 899 | 2 | 2 | 3828 | 1508909275109 | 'iOS' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 900 --> ([ 900 | 3 | 4 | 4513 | 1508909275192 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 901 --> ([ 901 | 0 | 2 | 2199 | 1508909275323 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 902 --> ([ 902 | 19 | 3 | 3864 | 1508909275461 | 'ios' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 903 --> ([ 903 | 6 | 2 | 4042 | 1508909275717 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 904 --> ([ 904 | 5 | 4 | 6163 | 1508909275951 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 905 --> ([ 905 | 7 | 2 | 369 | 1508909276164 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 906 --> ([ 906 | -1 | 2 | 2096 | 1508909276390 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 907 --> ([ 907 | 0 | 1 | 7254 | 1508909276513 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 908 --> ([ 908 | 13 | 1 | 5291 | 1508909276652 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 909 --> ([ 909 | 4 | 1 | 1019 | 1508909276788 | 'ios' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 910 --> ([ 910 | 6 | 3 | 9754 | 1508909276794 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 911 --> ([ 911 | 19 | 3 | 8009 | 1508909276809 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 912 --> ([ 912 | 11 | 1 | 3704 | 1508909277033 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 913 --> ([ 913 | 17 | 4 | 5772 | 1508909277245 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 914 --> ([ 914 | 0 | 3 | 2864 | 1508909277401 | 'web' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 915 --> ([ 915 | 19 | 3 | 4829 | 1508909277608 | 'iOS-test' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 916 --> ([ 916 | 15 | 4 | 8687 | 1508909277622 | 'android' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 917 --> ([ 917 | 11 | 1 | 2857 | 1508909277706 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 918 --> ([ 918 | 17 | 4 | 9550 | 1508909277809 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 919 --> ([ 919 | 6 | 2 | 5505 | 1508909277829 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 920 --> ([ 920 | 16 | 2 | 5382 | 1508909278089 | 'ios' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 921 --> ([ 921 | 16 | 1 | 614 | 1508909278229 | 'web' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 922 --> ([ 922 | 1 | 3 | 8777 | 1508909278469 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 923 --> ([ 923 | 5 | 4 | 5090 | 1508909278644 | 'android' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 924 --> ([ 924 | 15 | 4 | 8091 | 1508909278783 | 'iOS' | '(expletive deleted)' ])
[35mkafka_1               |[0m [2017-10-25 05:27:58,849] INFO [GroupCoordinator 1]: Member ksql_transient_3354176758863856004_1508909243329-cba51e6c-3246-46fb-85e6-827c80c383a8-StreamThread-2-consumer-26b9fc25-eaad-427b-8dd4-bc7f1162e1b3 in group ksql_transient_3354176758863856004_1508909243329 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[35mkafka_1               |[0m [2017-10-25 05:27:58,851] INFO [GroupCoordinator 1]: Preparing to rebalance group ksql_transient_3354176758863856004_1508909243329 with old generation 2 (__consumer_offsets-10) (kafka.coordinator.group.GroupCoordinator)
[35mkafka_1               |[0m [2017-10-25 05:27:58,876] INFO [GroupCoordinator 1]: Member ksql_transient_3354176758863856004_1508909243329-cba51e6c-3246-46fb-85e6-827c80c383a8-StreamThread-4-consumer-33aad105-9682-4bea-9b54-7ed0fb626a11 in group ksql_transient_3354176758863856004_1508909243329 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[35mkafka_1               |[0m [2017-10-25 05:27:58,952] INFO [GroupCoordinator 1]: Member ksql_transient_3354176758863856004_1508909243329-cba51e6c-3246-46fb-85e6-827c80c383a8-StreamThread-3-consumer-b1d04e60-4bdd-4851-a931-567a8edf1d62 in group ksql_transient_3354176758863856004_1508909243329 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[36;1mksql-datagen-users_1  |[0m 925 --> ([ 925 | 10 | 2 | 6208 | 1508909279009 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 926 --> ([ 926 | -1 | 4 | 498 | 1508909279023 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 927 --> ([ 927 | 14 | 2 | 1578 | 1508909279078 | 'ios' | '(expletive deleted)' ])
[35mkafka_1               |[0m [2017-10-25 05:27:59,119] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: _confluent-metrics-0. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[36;1mksql-datagen-users_1  |[0m 928 --> ([ 928 | 10 | 1 | 5555 | 1508909279226 | 'iOS-test' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 929 --> ([ 929 | 0 | 3 | 4541 | 1508909279344 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 930 --> ([ 930 | -1 | 2 | 2269 | 1508909279434 | 'iOS' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 931 --> ([ 931 | 19 | 3 | 4278 | 1508909279511 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 932 --> ([ 932 | 17 | 3 | 5151 | 1508909279661 | 'iOS' | 'is this as good as it gets? really ?' ])
[35mkafka_1               |[0m [2017-10-25 05:27:59,716] INFO [GroupCoordinator 1]: Member ksql_transient_3354176758863856004_1508909243329-cba51e6c-3246-46fb-85e6-827c80c383a8-StreamThread-1-consumer-5f33db0f-7944-4bbb-b502-fa837ae6a539 in group ksql_transient_3354176758863856004_1508909243329 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[35mkafka_1               |[0m [2017-10-25 05:27:59,718] INFO [GroupCoordinator 1]: Group ksql_transient_3354176758863856004_1508909243329 with generation 3 is now empty (__consumer_offsets-10) (kafka.coordinator.group.GroupCoordinator)
[36;1mksql-datagen-users_1  |[0m 933 --> ([ 933 | 3 | 1 | 2340 | 1508909279877 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 934 --> ([ 934 | 1 | 4 | 7310 | 1508909279998 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 935 --> ([ 935 | 19 | 3 | 9898 | 1508909280141 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 936 --> ([ 936 | 5 | 2 | 1051 | 1508909280302 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 937 --> ([ 937 | 6 | 1 | 2462 | 1508909280549 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 938 --> ([ 938 | 2 | 3 | 9004 | 1508909280685 | 'ios' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 939 --> ([ 939 | 8 | 2 | 1305 | 1508909280765 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 940 --> ([ 940 | 5 | 4 | 1316 | 1508909280825 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 941 --> ([ 941 | 7 | 2 | 3382 | 1508909280996 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 942 --> ([ 942 | 13 | 1 | 6841 | 1508909281154 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 943 --> ([ 943 | 13 | 4 | 3985 | 1508909281416 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 944 --> ([ 944 | 6 | 2 | 8942 | 1508909281671 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 945 --> ([ 945 | 11 | 2 | 8881 | 1508909281938 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 946 --> ([ 946 | 4 | 1 | 8807 | 1508909282013 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 947 --> ([ 947 | 18 | 1 | 5218 | 1508909282311 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 948 --> ([ 948 | 19 | 4 | 1858 | 1508909282462 | 'iOS' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 949 --> ([ 949 | 9 | 3 | 2929 | 1508909282522 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 950 --> ([ 950 | 6 | 4 | 5229 | 1508909282546 | 'ios' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 951 --> ([ 951 | 11 | 1 | 917 | 1508909282699 | 'android' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 952 --> ([ 952 | 12 | 1 | 6841 | 1508909282717 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 953 --> ([ 953 | 16 | 4 | 4755 | 1508909282959 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 954 --> ([ 954 | 18 | 4 | 9050 | 1508909283173 | 'iOS' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 955 --> ([ 955 | 0 | 3 | 5183 | 1508909283267 | 'web' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 956 --> ([ 956 | 2 | 3 | 2417 | 1508909283371 | 'iOS' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 957 --> ([ 957 | 2 | 4 | 9130 | 1508909283497 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 958 --> ([ 958 | 13 | 4 | 7563 | 1508909283517 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 959 --> ([ 959 | 7 | 1 | 7556 | 1508909283784 | 'iOS' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 960 --> ([ 960 | 6 | 2 | 5670 | 1508909283836 | 'web' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 961 --> ([ 961 | 15 | 2 | 6830 | 1508909283976 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 962 --> ([ 962 | 7 | 4 | 578 | 1508909284067 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 963 --> ([ 963 | 11 | 2 | 2947 | 1508909284178 | 'iOS-test' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 964 --> ([ 964 | 3 | 1 | 8024 | 1508909284263 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 965 --> ([ 965 | 7 | 4 | 296 | 1508909284305 | 'android' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 966 --> ([ 966 | 12 | 4 | 40 | 1508909284514 | 'android' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 967 --> ([ 967 | 8 | 4 | 9274 | 1508909284631 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 968 --> ([ 968 | 4 | 4 | 4954 | 1508909284868 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 969 --> ([ 969 | 13 | 2 | 9492 | 1508909284923 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 970 --> ([ 970 | 3 | 4 | 6099 | 1508909285117 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 971 --> ([ 971 | 8 | 3 | 7805 | 1508909285357 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 972 --> ([ 972 | 1 | 1 | 4644 | 1508909285641 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 973 --> ([ 973 | 0 | 3 | 2974 | 1508909285933 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 974 --> ([ 974 | 19 | 3 | 4617 | 1508909286014 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 975 --> ([ 975 | 1 | 4 | 1170 | 1508909286029 | 'iOS' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 976 --> ([ 976 | 13 | 4 | 601 | 1508909286251 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 977 --> ([ 977 | 2 | 3 | 9113 | 1508909286494 | 'iOS-test' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 978 --> ([ 978 | 8 | 1 | 1115 | 1508909286647 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 979 --> ([ 979 | 1 | 2 | 4085 | 1508909286924 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 980 --> ([ 980 | 10 | 1 | 3198 | 1508909287197 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 981 --> ([ 981 | 8 | 4 | 2566 | 1508909287215 | 'iOS' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 982 --> ([ 982 | 18 | 3 | 6180 | 1508909287390 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 983 --> ([ 983 | -1 | 2 | 1781 | 1508909287641 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 984 --> ([ 984 | 10 | 4 | 5180 | 1508909287742 | 'web' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 985 --> ([ 985 | 10 | 3 | 894 | 1508909287969 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 986 --> ([ 986 | 3 | 2 | 7308 | 1508909288200 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 987 --> ([ 987 | 0 | 1 | 1102 | 1508909288384 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 988 --> ([ 988 | 19 | 1 | 6102 | 1508909288601 | 'web' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 989 --> ([ 989 | 18 | 4 | 6410 | 1508909288637 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 990 --> ([ 990 | 10 | 2 | 2194 | 1508909288756 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 991 --> ([ 991 | -1 | 2 | 9320 | 1508909288967 | 'web' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 992 --> ([ 992 | 14 | 4 | 717 | 1508909289178 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 993 --> ([ 993 | 2 | 1 | 8207 | 1508909289338 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 994 --> ([ 994 | -1 | 4 | 8464 | 1508909289587 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 995 --> ([ 995 | 10 | 1 | 5639 | 1508909289869 | 'iOS' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 996 --> ([ 996 | 5 | 1 | 6306 | 1508909289923 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 997 --> ([ 997 | 7 | 2 | 9382 | 1508909289991 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 998 --> ([ 998 | 8 | 2 | 4863 | 1508909290249 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 999 --> ([ 999 | 11 | 3 | 388 | 1508909290272 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1000 --> ([ 1000 | 10 | 4 | 3964 | 1508909290517 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1001 --> ([ 1001 | 7 | 2 | 4457 | 1508909290750 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1002 --> ([ 1002 | 18 | 3 | 8872 | 1508909290999 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1003 --> ([ 1003 | 15 | 2 | 6916 | 1508909291249 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1004 --> ([ 1004 | 2 | 1 | 8814 | 1508909291283 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1005 --> ([ 1005 | 5 | 2 | 1992 | 1508909291503 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1006 --> ([ 1006 | 17 | 3 | 5789 | 1508909291795 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1007 --> ([ 1007 | 12 | 1 | 1189 | 1508909292038 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1008 --> ([ 1008 | 10 | 1 | 4190 | 1508909292108 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1009 --> ([ 1009 | 6 | 3 | 7404 | 1508909292125 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1010 --> ([ 1010 | 12 | 4 | 4238 | 1508909292264 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1011 --> ([ 1011 | 1 | 2 | 4059 | 1508909292335 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1012 --> ([ 1012 | 9 | 1 | 4733 | 1508909292436 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1013 --> ([ 1013 | 9 | 1 | 972 | 1508909292505 | 'web' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1014 --> ([ 1014 | -1 | 2 | 3132 | 1508909292650 | 'web' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1015 --> ([ 1015 | 6 | 3 | 1261 | 1508909292808 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1016 --> ([ 1016 | 16 | 4 | 3465 | 1508909292989 | 'web' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1017 --> ([ 1017 | 4 | 2 | 6029 | 1508909293233 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1018 --> ([ 1018 | 15 | 3 | 5086 | 1508909293346 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1019 --> ([ 1019 | 11 | 1 | 6072 | 1508909293518 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1020 --> ([ 1020 | -1 | 2 | 7787 | 1508909293637 | 'android' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1021 --> ([ 1021 | 19 | 1 | 2379 | 1508909293835 | 'ios' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1022 --> ([ 1022 | 5 | 4 | 1263 | 1508909294084 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1023 --> ([ 1023 | 5 | 4 | 9692 | 1508909294368 | 'iOS-test' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1024 --> ([ 1024 | 1 | 4 | 9436 | 1508909294517 | 'android' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1025 --> ([ 1025 | 17 | 3 | 9868 | 1508909294575 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[35mkafka_1               |[0m [2017-10-25 05:28:14,607] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: _confluent-metrics-8. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[36;1mksql-datagen-users_1  |[0m 1026 --> ([ 1026 | 10 | 3 | 2675 | 1508909294675 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1027 --> ([ 1027 | 14 | 2 | 4306 | 1508909294842 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1028 --> ([ 1028 | 5 | 2 | 3372 | 1508909294932 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1029 --> ([ 1029 | -1 | 4 | 7192 | 1508909295200 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1030 --> ([ 1030 | 10 | 1 | 2927 | 1508909295487 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1031 --> ([ 1031 | 0 | 4 | 3843 | 1508909295709 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1032 --> ([ 1032 | 8 | 4 | 622 | 1508909295741 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1033 --> ([ 1033 | 5 | 3 | 866 | 1508909295961 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1034 --> ([ 1034 | 13 | 2 | 7401 | 1508909296176 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1035 --> ([ 1035 | 13 | 4 | 7432 | 1508909296344 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1036 --> ([ 1036 | 1 | 2 | 70 | 1508909296494 | 'ios' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1037 --> ([ 1037 | 8 | 1 | 9404 | 1508909296582 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1038 --> ([ 1038 | 8 | 2 | 3899 | 1508909296629 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[35mkafka_1               |[0m [2017-10-25 05:28:16,665] INFO [GroupCoordinator 1]: Preparing to rebalance group ksql_transient_4864640722177837871_1508909296472 with old generation 0 (__consumer_offsets-0) (kafka.coordinator.group.GroupCoordinator)
[36;1mksql-datagen-users_1  |[0m 1039 --> ([ 1039 | 12 | 1 | 562 | 1508909296867 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1040 --> ([ 1040 | 19 | 2 | 4183 | 1508909297127 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1041 --> ([ 1041 | 17 | 1 | 7707 | 1508909297153 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1042 --> ([ 1042 | 12 | 2 | 9709 | 1508909297172 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1043 --> ([ 1043 | 6 | 2 | 8911 | 1508909297203 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1044 --> ([ 1044 | 9 | 4 | 3693 | 1508909297399 | 'iOS-test' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1045 --> ([ 1045 | 9 | 2 | 6616 | 1508909297461 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1046 --> ([ 1046 | 7 | 3 | 145 | 1508909297543 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1047 --> ([ 1047 | 9 | 3 | 9910 | 1508909297576 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1048 --> ([ 1048 | 19 | 3 | 6214 | 1508909297594 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1049 --> ([ 1049 | 9 | 3 | 2686 | 1508909297666 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1050 --> ([ 1050 | 14 | 1 | 3499 | 1508909297849 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1051 --> ([ 1051 | 1 | 3 | 6963 | 1508909297913 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1052 --> ([ 1052 | 17 | 3 | 3816 | 1508909298198 | 'android' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1053 --> ([ 1053 | 4 | 3 | 3138 | 1508909298313 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1054 --> ([ 1054 | 10 | 1 | 7473 | 1508909298593 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1055 --> ([ 1055 | 15 | 2 | 1388 | 1508909298787 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1056 --> ([ 1056 | 6 | 2 | 4527 | 1508909299059 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1057 --> ([ 1057 | 19 | 2 | 1027 | 1508909299348 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1058 --> ([ 1058 | 2 | 4 | 2032 | 1508909299383 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1059 --> ([ 1059 | 6 | 3 | 7040 | 1508909299588 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1060 --> ([ 1060 | 13 | 4 | 2804 | 1508909299627 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1061 --> ([ 1061 | 3 | 2 | 5578 | 1508909299825 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1062 --> ([ 1062 | 17 | 3 | 3703 | 1508909299991 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1063 --> ([ 1063 | 11 | 1 | 2643 | 1508909300232 | 'iOS' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1064 --> ([ 1064 | 0 | 1 | 3927 | 1508909300476 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1065 --> ([ 1065 | 16 | 4 | 2747 | 1508909300603 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1066 --> ([ 1066 | 9 | 4 | 6341 | 1508909300726 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1067 --> ([ 1067 | 18 | 1 | 6795 | 1508909300872 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1068 --> ([ 1068 | 16 | 2 | 7351 | 1508909300996 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1069 --> ([ 1069 | 6 | 2 | 947 | 1508909301156 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1070 --> ([ 1070 | 18 | 3 | 3314 | 1508909301276 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1071 --> ([ 1071 | 17 | 2 | 700 | 1508909301397 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1072 --> ([ 1072 | 19 | 1 | 4003 | 1508909301589 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1073 --> ([ 1073 | 8 | 1 | 6994 | 1508909301633 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1074 --> ([ 1074 | 0 | 3 | 6144 | 1508909301774 | 'ios' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1075 --> ([ 1075 | -1 | 3 | 3556 | 1508909302048 | 'android' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1076 --> ([ 1076 | 17 | 4 | 2513 | 1508909302329 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1077 --> ([ 1077 | 14 | 4 | 7242 | 1508909302461 | 'web' | 'your team here rocks!' ])
[35mkafka_1               |[0m [2017-10-25 05:28:22,666] INFO [GroupCoordinator 1]: Stabilized group ksql_transient_4864640722177837871_1508909296472 generation 1 (__consumer_offsets-0) (kafka.coordinator.group.GroupCoordinator)
[36;1mksql-datagen-users_1  |[0m 1078 --> ([ 1078 | 8 | 2 | 4605 | 1508909302732 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[33mzookeeper_1           |[0m [2017-10-25 05:28:22,778] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:setData cxid:0x366 zxid:0x1c5 txntype:-1 reqpath:n/a Error Path:/config/topics/ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition Error:KeeperErrorCode = NoNode for /config/topics/ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:28:22,794] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x367 zxid:0x1c6 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:28:22,818] INFO Topic creation {"version":1,"partitions":{"0":[1]}} (kafka.admin.AdminUtils$)
[35mkafka_1               |[0m [2017-10-25 05:28:22,832] INFO [Controller 1]: New topics: [Set(ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition)], deleted topics: [Set()], new partition replica assignment [Map([ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition,0] -> List(1))] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:28:22,832] INFO [Controller 1]: New topic creation callback for [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:28:22,833] INFO [Controller 1]: New partition creation callback for [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:28:22,833] INFO [Partition state machine on Controller 1]: Invoking state change to NewPartition for partitions [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:28:22,833] TRACE Controller 1 epoch 1 changed partition [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition,0] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:28:22,834] INFO [Replica state machine on controller 1]: Invoking state change to NewReplica for replicas [Topic=ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:28:22,834] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition,0] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:28:22,835] INFO [Partition state machine on Controller 1]: Invoking state change to OnlinePartition for partitions [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:28:22,835] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition,0] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:28:22,835] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition,0] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:28:22,836] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x36f zxid:0x1c9 txntype:-1 reqpath:n/a Error Path:/brokers/topics/ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition/partitions/0 (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:28:22,846] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x370 zxid:0x1ca txntype:-1 reqpath:n/a Error Path:/brokers/topics/ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition/partitions (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 1079 --> ([ 1079 | 0 | 4 | 5678 | 1508909302848 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[35mkafka_1               |[0m [2017-10-25 05:28:22,888] TRACE Controller 1 epoch 1 changed partition [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition,0] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:28:22,888] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:28:22,888] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:28:22,889] INFO [Replica state machine on controller 1]: Invoking state change to OnlineReplica for replicas [Topic=ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:28:22,889] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition,0] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:28:22,890] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 33 from controller 1 epoch 1 for partition [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:28:22,890] TRACE Broker 1 handling LeaderAndIsr request correlationId 33 from controller 1 epoch 1 starting the become-leader transition for partition ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:28:22,890] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition-0 (kafka.server.ReplicaFetcherManager)
[35mkafka_1               |[0m [2017-10-25 05:28:22,895] INFO Loading producer state from offset 0 for partition ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition-0 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:28:22,895] INFO Completed load of log ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:28:22,895] INFO Created log for partition [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition,0] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> delete, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:28:22,896] INFO Partition [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition,0] on broker 1: No checkpointed highwatermark is found for partition ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition-0 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:28:22,896] INFO Partition [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition,0] on broker 1: ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:28:22,896] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 33 for partition ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:28:22,896] TRACE Broker 1 completed LeaderAndIsr request correlationId 33 from controller 1 epoch 1 for the become-leader transition for partition ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:28:22,899] TRACE Controller 1 epoch 1 received response {error_code=0,partitions=[{topic=ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition,partition=0,error_code=0}]} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:28:22,902] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition ksql_transient_4864640722177837871_1508909296472-USERS_statestore-repartition-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 34 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:28:22,904] TRACE Controller 1 epoch 1 received response {error_code=0} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[36;1mksql-datagen-users_1  |[0m 1080 --> ([ 1080 | 17 | 1 | 9406 | 1508909302914 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[33mzookeeper_1           |[0m [2017-10-25 05:28:23,062] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:setData cxid:0x377 zxid:0x1ce txntype:-1 reqpath:n/a Error Path:/config/topics/ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog Error:KeeperErrorCode = NoNode for /config/topics/ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:28:23,073] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x378 zxid:0x1cf txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[35mkafka_1               |[0m [2017-10-25 05:28:23,094] INFO Topic creation {"version":1,"partitions":{"0":[1]}} (kafka.admin.AdminUtils$)
[35mkafka_1               |[0m [2017-10-25 05:28:23,108] INFO [Controller 1]: New topics: [Set(ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog)], deleted topics: [Set()], new partition replica assignment [Map([ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog,0] -> List(1))] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:28:23,108] INFO [Controller 1]: New topic creation callback for [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:28:23,109] INFO [Controller 1]: New partition creation callback for [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog,0] (kafka.controller.KafkaController)
[35mkafka_1               |[0m [2017-10-25 05:28:23,109] INFO [Partition state machine on Controller 1]: Invoking state change to NewPartition for partitions [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:28:23,109] TRACE Controller 1 epoch 1 changed partition [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog,0] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:28:23,110] INFO [Replica state machine on controller 1]: Invoking state change to NewReplica for replicas [Topic=ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:28:23,110] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog,0] from NonExistentReplica to NewReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:28:23,111] INFO [Partition state machine on Controller 1]: Invoking state change to OnlinePartition for partitions [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog,0] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:28:23,111] DEBUG [Partition state machine on Controller 1]: Live assigned replicas for partition [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog,0] are: [List(1)] (kafka.controller.PartitionStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:28:23,111] DEBUG [Partition state machine on Controller 1]: Initializing leader and isr for partition [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog,0] to (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[33mzookeeper_1           |[0m [2017-10-25 05:28:23,112] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x380 zxid:0x1d2 txntype:-1 reqpath:n/a Error Path:/brokers/topics/ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog/partitions/0 (org.apache.zookeeper.server.PrepRequestProcessor)
[33mzookeeper_1           |[0m [2017-10-25 05:28:23,124] INFO Got user-level KeeperException when processing sessionid:0x15f51fe0e370001 type:create cxid:0x381 zxid:0x1d3 txntype:-1 reqpath:n/a Error Path:/brokers/topics/ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog/partitions (org.apache.zookeeper.server.PrepRequestProcessor)
[36;1mksql-datagen-users_1  |[0m 1081 --> ([ 1081 | 19 | 4 | 8898 | 1508909303174 | 'iOS' | 'your team here rocks!' ])
[35mkafka_1               |[0m [2017-10-25 05:28:23,200] TRACE Controller 1 epoch 1 changed partition [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog,0] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:28:23,200] TRACE Controller 1 epoch 1 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to broker 1 for partition [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:28:23,201] TRACE Controller 1 epoch 1 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1) to brokers Set(1) for partition ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:28:23,201] INFO [Replica state machine on controller 1]: Invoking state change to OnlineReplica for replicas [Topic=ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog,Partition=0,Replica=1] (kafka.controller.ReplicaStateMachine)
[35mkafka_1               |[0m [2017-10-25 05:28:23,201] TRACE Controller 1 epoch 1 changed state of replica 1 for partition [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog,0] from NewReplica to OnlineReplica (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:28:23,202] TRACE Broker 1 received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1) correlation id 35 from controller 1 epoch 1 for partition [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog,0] (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:28:23,202] TRACE Broker 1 handling LeaderAndIsr request correlationId 35 from controller 1 epoch 1 starting the become-leader transition for partition ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:28:23,203] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog-0 (kafka.server.ReplicaFetcherManager)
[35mkafka_1               |[0m [2017-10-25 05:28:23,206] INFO Loading producer state from offset 0 for partition ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog-0 with message format version 2 (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:28:23,210] INFO Completed load of log ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog-0 with 1 log segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[35mkafka_1               |[0m [2017-10-25 05:28:23,211] INFO Created log for partition [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog,0] in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[35mkafka_1               |[0m [2017-10-25 05:28:23,211] INFO Partition [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog,0] on broker 1: No checkpointed highwatermark is found for partition ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog-0 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:28:23,212] INFO Partition [ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog,0] on broker 1: ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[35mkafka_1               |[0m [2017-10-25 05:28:23,213] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 35 for partition ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:28:23,214] TRACE Broker 1 completed LeaderAndIsr request correlationId 35 from controller 1 epoch 1 for the become-leader transition for partition ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog-0 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:28:23,215] TRACE Controller 1 epoch 1 received response {error_code=0,partitions=[{topic=ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog,partition=0,error_code=0}]} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:28:23,216] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:1) for partition ksql_transient_4864640722177837871_1508909296472-USERS_statestore-changelog-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 36 (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:28:23,218] TRACE Controller 1 epoch 1 received response {error_code=0} for a request sent to broker kafka:29092 (id: 1 rack: null) (state.change.logger)
[35mkafka_1               |[0m [2017-10-25 05:28:23,278] INFO [GroupCoordinator 1]: Assignment received from leader for group ksql_transient_4864640722177837871_1508909296472 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[36;1mksql-datagen-users_1  |[0m 1082 --> ([ 1082 | 5 | 2 | 3244 | 1508909303457 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1083 --> ([ 1083 | 19 | 4 | 4288 | 1508909303488 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1084 --> ([ 1084 | 0 | 4 | 8234 | 1508909303630 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1085 --> ([ 1085 | 10 | 4 | 6556 | 1508909303870 | 'ios' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1086 --> ([ 1086 | 8 | 4 | 9360 | 1508909304154 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1087 --> ([ 1087 | 9 | 3 | 7014 | 1508909304306 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1088 --> ([ 1088 | 19 | 4 | 2641 | 1508909304481 | 'iOS' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1089 --> ([ 1089 | 19 | 4 | 8908 | 1508909304743 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1090 --> ([ 1090 | 6 | 4 | 7436 | 1508909304861 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1091 --> ([ 1091 | 17 | 4 | 8076 | 1508909304895 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1092 --> ([ 1092 | 18 | 3 | 8390 | 1508909305048 | 'iOS-test' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1093 --> ([ 1093 | 0 | 1 | 664 | 1508909305097 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1094 --> ([ 1094 | 7 | 4 | 4998 | 1508909305385 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1095 --> ([ 1095 | 13 | 1 | 2181 | 1508909305614 | 'android' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1096 --> ([ 1096 | 5 | 3 | 9332 | 1508909305746 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1097 --> ([ 1097 | 4 | 3 | 4649 | 1508909306000 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1098 --> ([ 1098 | 19 | 3 | 4929 | 1508909306233 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1099 --> ([ 1099 | 17 | 2 | 1090 | 1508909306436 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1100 --> ([ 1100 | 14 | 4 | 2315 | 1508909306727 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1101 --> ([ 1101 | -1 | 3 | 9118 | 1508909306747 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1102 --> ([ 1102 | 16 | 1 | 9232 | 1508909307025 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1103 --> ([ 1103 | -1 | 3 | 7906 | 1508909307155 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1104 --> ([ 1104 | 13 | 4 | 6030 | 1508909307160 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1105 --> ([ 1105 | 14 | 1 | 5254 | 1508909307297 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1106 --> ([ 1106 | 10 | 4 | 9235 | 1508909307536 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1107 --> ([ 1107 | 1 | 4 | 4048 | 1508909307617 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1108 --> ([ 1108 | 15 | 4 | 8832 | 1508909307654 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1109 --> ([ 1109 | 3 | 1 | 9852 | 1508909307782 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1110 --> ([ 1110 | -1 | 2 | 7235 | 1508909307980 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1111 --> ([ 1111 | 15 | 4 | 3902 | 1508909308221 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1112 --> ([ 1112 | 13 | 2 | 474 | 1508909308312 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1113 --> ([ 1113 | 11 | 3 | 8224 | 1508909308323 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1114 --> ([ 1114 | 15 | 2 | 4596 | 1508909308409 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1115 --> ([ 1115 | 13 | 4 | 2201 | 1508909308457 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1116 --> ([ 1116 | 3 | 3 | 2799 | 1508909308613 | 'iOS' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1117 --> ([ 1117 | 9 | 1 | 2270 | 1508909308764 | 'android' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1118 --> ([ 1118 | -1 | 3 | 1288 | 1508909309060 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1119 --> ([ 1119 | 2 | 4 | 9071 | 1508909309098 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1120 --> ([ 1120 | 19 | 3 | 4385 | 1508909309128 | 'android' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1121 --> ([ 1121 | 19 | 2 | 8765 | 1508909309192 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1122 --> ([ 1122 | 0 | 3 | 1876 | 1508909309388 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1123 --> ([ 1123 | -1 | 2 | 7793 | 1508909309564 | 'android' | 'meh' ])
[35mkafka_1               |[0m [2017-10-25 05:28:29,608] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: _confluent-metrics-2. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[36;1mksql-datagen-users_1  |[0m 1124 --> ([ 1124 | 18 | 2 | 4124 | 1508909309847 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1125 --> ([ 1125 | 12 | 4 | 2839 | 1508909310103 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1126 --> ([ 1126 | 0 | 3 | 2944 | 1508909310313 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1127 --> ([ 1127 | 14 | 3 | 243 | 1508909310380 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1128 --> ([ 1128 | 17 | 3 | 8748 | 1508909310604 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1129 --> ([ 1129 | 9 | 3 | 1636 | 1508909310688 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1130 --> ([ 1130 | 6 | 2 | 6978 | 1508909310712 | 'android' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1131 --> ([ 1131 | 5 | 3 | 5038 | 1508909310764 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1132 --> ([ 1132 | 14 | 3 | 9312 | 1508909310976 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1133 --> ([ 1133 | 18 | 1 | 3193 | 1508909311204 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1134 --> ([ 1134 | 16 | 3 | 7685 | 1508909311307 | 'iOS' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1135 --> ([ 1135 | 1 | 2 | 3382 | 1508909311584 | 'android' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1136 --> ([ 1136 | 7 | 1 | 1601 | 1508909311634 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1137 --> ([ 1137 | 7 | 2 | 5492 | 1508909311838 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1138 --> ([ 1138 | 10 | 3 | 1757 | 1508909311847 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1139 --> ([ 1139 | 7 | 4 | 7586 | 1508909311923 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1140 --> ([ 1140 | 17 | 4 | 5497 | 1508909312198 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1141 --> ([ 1141 | 10 | 2 | 726 | 1508909312213 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1142 --> ([ 1142 | 7 | 3 | 2083 | 1508909312288 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1143 --> ([ 1143 | -1 | 3 | 7315 | 1508909312343 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1144 --> ([ 1144 | 19 | 4 | 8298 | 1508909312550 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1145 --> ([ 1145 | 12 | 2 | 9736 | 1508909312823 | 'android' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1146 --> ([ 1146 | 12 | 4 | 7411 | 1508909312893 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1147 --> ([ 1147 | 14 | 4 | 8497 | 1508909313111 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1148 --> ([ 1148 | 13 | 1 | 6965 | 1508909313215 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1149 --> ([ 1149 | 11 | 3 | 1682 | 1508909313310 | 'android' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1150 --> ([ 1150 | 0 | 4 | 3450 | 1508909313446 | 'iOS-test' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1151 --> ([ 1151 | 4 | 4 | 9952 | 1508909313597 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1152 --> ([ 1152 | 19 | 2 | 4321 | 1508909313853 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1153 --> ([ 1153 | 9 | 4 | 3861 | 1508909314054 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1154 --> ([ 1154 | 8 | 3 | 411 | 1508909314110 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1155 --> ([ 1155 | 16 | 4 | 2710 | 1508909314314 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1156 --> ([ 1156 | 10 | 2 | 4201 | 1508909314350 | 'android' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1157 --> ([ 1157 | 3 | 4 | 6760 | 1508909314639 | 'iOS' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1158 --> ([ 1158 | 18 | 3 | 2006 | 1508909314720 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1159 --> ([ 1159 | 1 | 3 | 7007 | 1508909314774 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1160 --> ([ 1160 | 6 | 4 | 540 | 1508909315068 | 'iOS-test' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1161 --> ([ 1161 | 3 | 1 | 9289 | 1508909315106 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1162 --> ([ 1162 | 11 | 2 | 889 | 1508909315370 | 'android' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1163 --> ([ 1163 | 12 | 4 | 2012 | 1508909315591 | 'android' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1164 --> ([ 1164 | 0 | 2 | 5409 | 1508909315726 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1165 --> ([ 1165 | 16 | 2 | 9204 | 1508909315880 | 'iOS-test' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1166 --> ([ 1166 | 0 | 4 | 5531 | 1508909316100 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1167 --> ([ 1167 | 4 | 2 | 3211 | 1508909316185 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1168 --> ([ 1168 | 5 | 2 | 1003 | 1508909316324 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1169 --> ([ 1169 | -1 | 3 | 3734 | 1508909316502 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1170 --> ([ 1170 | 6 | 4 | 3561 | 1508909316575 | 'android' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1171 --> ([ 1171 | 7 | 4 | 2515 | 1508909316869 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1172 --> ([ 1172 | 12 | 2 | 557 | 1508909317150 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1173 --> ([ 1173 | 15 | 4 | 5398 | 1508909317211 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1174 --> ([ 1174 | 9 | 1 | 6526 | 1508909317264 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1175 --> ([ 1175 | 8 | 1 | 6043 | 1508909317443 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1176 --> ([ 1176 | 8 | 2 | 3226 | 1508909317692 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1177 --> ([ 1177 | 7 | 3 | 8026 | 1508909317971 | 'web' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1178 --> ([ 1178 | 8 | 1 | 1128 | 1508909318147 | 'iOS' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1179 --> ([ 1179 | 12 | 1 | 4619 | 1508909318434 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1180 --> ([ 1180 | 18 | 1 | 7900 | 1508909318535 | 'ios' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1181 --> ([ 1181 | 2 | 1 | 7963 | 1508909318758 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1182 --> ([ 1182 | 0 | 3 | 1871 | 1508909318960 | 'web' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1183 --> ([ 1183 | 15 | 4 | 1946 | 1508909319200 | 'iOS-test' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1184 --> ([ 1184 | 2 | 2 | 970 | 1508909319241 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1185 --> ([ 1185 | 8 | 4 | 5477 | 1508909319421 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1186 --> ([ 1186 | 4 | 3 | 186 | 1508909319706 | 'web' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1187 --> ([ 1187 | 12 | 3 | 5488 | 1508909319998 | 'android' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1188 --> ([ 1188 | -1 | 3 | 3025 | 1508909320207 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1189 --> ([ 1189 | 1 | 2 | 1357 | 1508909320433 | 'iOS-test' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1190 --> ([ 1190 | 18 | 2 | 4978 | 1508909320458 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1191 --> ([ 1191 | 1 | 1 | 5046 | 1508909320634 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1192 --> ([ 1192 | -1 | 3 | 9520 | 1508909320720 | 'web' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1193 --> ([ 1193 | 15 | 2 | 3711 | 1508909320743 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1194 --> ([ 1194 | 2 | 3 | 9632 | 1508909320786 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1195 --> ([ 1195 | 15 | 2 | 4020 | 1508909321020 | 'web' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1196 --> ([ 1196 | 11 | 1 | 369 | 1508909321123 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1197 --> ([ 1197 | 18 | 4 | 9959 | 1508909321221 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1198 --> ([ 1198 | 13 | 4 | 460 | 1508909321497 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1199 --> ([ 1199 | 14 | 3 | 8378 | 1508909321691 | 'web' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1200 --> ([ 1200 | 14 | 3 | 3482 | 1508909321759 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1201 --> ([ 1201 | 4 | 2 | 9019 | 1508909321768 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1202 --> ([ 1202 | 3 | 2 | 1582 | 1508909321777 | 'web' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1203 --> ([ 1203 | 8 | 2 | 8314 | 1508909321881 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1204 --> ([ 1204 | 3 | 3 | 7265 | 1508909321919 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1205 --> ([ 1205 | 14 | 1 | 8683 | 1508909322107 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1206 --> ([ 1206 | 12 | 3 | 1278 | 1508909322208 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1207 --> ([ 1207 | 2 | 2 | 7602 | 1508909322385 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1208 --> ([ 1208 | 6 | 1 | 7049 | 1508909322453 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1209 --> ([ 1209 | 10 | 4 | 3203 | 1508909322636 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1210 --> ([ 1210 | 17 | 1 | 4759 | 1508909322669 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1211 --> ([ 1211 | 18 | 1 | 5646 | 1508909322772 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1212 --> ([ 1212 | 4 | 2 | 473 | 1508909322996 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1213 --> ([ 1213 | 8 | 2 | 6055 | 1508909323169 | 'web' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1214 --> ([ 1214 | 5 | 3 | 8404 | 1508909323425 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1215 --> ([ 1215 | 10 | 1 | 7225 | 1508909323516 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1216 --> ([ 1216 | 4 | 2 | 6799 | 1508909323679 | 'iOS' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1217 --> ([ 1217 | 7 | 3 | 2916 | 1508909323867 | 'web' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1218 --> ([ 1218 | 9 | 1 | 425 | 1508909324053 | 'android' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1219 --> ([ 1219 | 0 | 4 | 4935 | 1508909324182 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1220 --> ([ 1220 | 13 | 1 | 5203 | 1508909324272 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1221 --> ([ 1221 | 15 | 3 | 9818 | 1508909324561 | 'iOS' | 'more peanuts please' ])
[35mkafka_1               |[0m [2017-10-25 05:28:44,667] INFO Rolled new log segment for 'mysql-routes-0' in 2 ms. (kafka.log.Log)
[36;1mksql-datagen-users_1  |[0m 1222 --> ([ 1222 | -1 | 1 | 2985 | 1508909324780 | 'iOS-test' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1223 --> ([ 1223 | 18 | 3 | 9322 | 1508909324834 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1224 --> ([ 1224 | 0 | 4 | 6968 | 1508909324954 | 'iOS-test' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1225 --> ([ 1225 | 4 | 2 | 1517 | 1508909325094 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1226 --> ([ 1226 | 3 | 4 | 771 | 1508909325301 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1227 --> ([ 1227 | 3 | 4 | 1718 | 1508909325477 | 'iOS' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1228 --> ([ 1228 | 6 | 2 | 6663 | 1508909325552 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1229 --> ([ 1229 | 12 | 4 | 5032 | 1508909325713 | 'iOS' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1230 --> ([ 1230 | 5 | 1 | 990 | 1508909325725 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1231 --> ([ 1231 | 18 | 2 | 4758 | 1508909325873 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1232 --> ([ 1232 | 2 | 3 | 7239 | 1508909326095 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1233 --> ([ 1233 | 12 | 2 | 3409 | 1508909326122 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1234 --> ([ 1234 | 3 | 1 | 9832 | 1508909326192 | 'web' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1235 --> ([ 1235 | 13 | 4 | 5614 | 1508909326207 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1236 --> ([ 1236 | 7 | 4 | 4085 | 1508909326325 | 'ios' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1237 --> ([ 1237 | 14 | 2 | 4657 | 1508909326325 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1238 --> ([ 1238 | 14 | 1 | 374 | 1508909326589 | 'iOS-test' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1239 --> ([ 1239 | 12 | 4 | 6904 | 1508909326763 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1240 --> ([ 1240 | 1 | 3 | 3023 | 1508909326997 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1241 --> ([ 1241 | 18 | 2 | 890 | 1508909327154 | 'android' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1242 --> ([ 1242 | 16 | 2 | 6903 | 1508909327316 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1243 --> ([ 1243 | 5 | 2 | 306 | 1508909327488 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1244 --> ([ 1244 | 18 | 2 | 7253 | 1508909327692 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1245 --> ([ 1245 | 7 | 3 | 2661 | 1508909327951 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1246 --> ([ 1246 | 14 | 1 | 737 | 1508909328021 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1247 --> ([ 1247 | 18 | 4 | 7864 | 1508909328206 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1248 --> ([ 1248 | 12 | 2 | 8240 | 1508909328383 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1249 --> ([ 1249 | 2 | 1 | 3494 | 1508909328510 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1250 --> ([ 1250 | 10 | 2 | 2432 | 1508909328785 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1251 --> ([ 1251 | 19 | 2 | 5998 | 1508909328799 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1252 --> ([ 1252 | 0 | 1 | 5548 | 1508909329056 | 'ios' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1253 --> ([ 1253 | 14 | 2 | 9607 | 1508909329220 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1254 --> ([ 1254 | 3 | 1 | 5481 | 1508909329350 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1255 --> ([ 1255 | 12 | 3 | 7193 | 1508909329492 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1256 --> ([ 1256 | 8 | 2 | 613 | 1508909329632 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1257 --> ([ 1257 | 11 | 1 | 9839 | 1508909329899 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1258 --> ([ 1258 | 11 | 1 | 6434 | 1508909330128 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1259 --> ([ 1259 | 18 | 2 | 4399 | 1508909330359 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1260 --> ([ 1260 | 0 | 4 | 9469 | 1508909330449 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1261 --> ([ 1261 | 17 | 3 | 760 | 1508909330571 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1262 --> ([ 1262 | 12 | 1 | 6470 | 1508909330659 | 'web' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1263 --> ([ 1263 | 9 | 1 | 3116 | 1508909330812 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1264 --> ([ 1264 | -1 | 1 | 9429 | 1508909330949 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1265 --> ([ 1265 | 14 | 2 | 1979 | 1508909331118 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1266 --> ([ 1266 | 4 | 4 | 9409 | 1508909331378 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1267 --> ([ 1267 | 0 | 1 | 6182 | 1508909331635 | 'iOS' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1268 --> ([ 1268 | 14 | 1 | 4679 | 1508909331793 | 'iOS' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1269 --> ([ 1269 | 16 | 4 | 6663 | 1508909331884 | 'iOS' | '(expletive deleted)' ])
[33;1mconnect_1             |[0m [2017-10-25 05:28:51,923] INFO Finished WorkerSourceTask{id=MySQLSource-0} commitOffsets successfully in 1237 ms (org.apache.kafka.connect.runtime.WorkerSourceTask)
[36;1mksql-datagen-users_1  |[0m 1270 --> ([ 1270 | 1 | 3 | 1185 | 1508909331979 | 'ios' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1271 --> ([ 1271 | 17 | 2 | 6250 | 1508909332137 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1272 --> ([ 1272 | 8 | 4 | 5851 | 1508909332247 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1273 --> ([ 1273 | 12 | 4 | 2233 | 1508909332410 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1274 --> ([ 1274 | 4 | 2 | 5737 | 1508909332670 | 'iOS-test' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1275 --> ([ 1275 | 5 | 3 | 964 | 1508909332852 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1276 --> ([ 1276 | 3 | 2 | 8524 | 1508909332933 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1277 --> ([ 1277 | 19 | 2 | 6255 | 1508909332936 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1278 --> ([ 1278 | 4 | 2 | 2380 | 1508909333093 | 'android' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1279 --> ([ 1279 | 19 | 2 | 5826 | 1508909333179 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1280 --> ([ 1280 | 1 | 3 | 5676 | 1508909333352 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1281 --> ([ 1281 | 3 | 1 | 2548 | 1508909333598 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1282 --> ([ 1282 | 2 | 1 | 9589 | 1508909333659 | 'android' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1283 --> ([ 1283 | 9 | 1 | 9767 | 1508909333886 | 'iOS-test' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1284 --> ([ 1284 | 19 | 4 | 9077 | 1508909333906 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1285 --> ([ 1285 | 11 | 1 | 2454 | 1508909334105 | 'android' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1286 --> ([ 1286 | 10 | 1 | 252 | 1508909334353 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1287 --> ([ 1287 | 4 | 2 | 8407 | 1508909334391 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1288 --> ([ 1288 | 0 | 2 | 4634 | 1508909334545 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1289 --> ([ 1289 | 4 | 4 | 6336 | 1508909334590 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1290 --> ([ 1290 | 12 | 4 | 3344 | 1508909334876 | 'android' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1291 --> ([ 1291 | 8 | 3 | 5 | 1508909335170 | 'android' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1292 --> ([ 1292 | 15 | 4 | 6670 | 1508909335395 | 'iOS' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1293 --> ([ 1293 | 15 | 2 | 252 | 1508909335661 | 'iOS-test' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1294 --> ([ 1294 | 13 | 3 | 7081 | 1508909335670 | 'iOS' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1295 --> ([ 1295 | 3 | 2 | 9112 | 1508909335881 | 'android' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1296 --> ([ 1296 | 16 | 2 | 6163 | 1508909336047 | 'iOS-test' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1297 --> ([ 1297 | -1 | 3 | 2823 | 1508909336238 | 'android' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1298 --> ([ 1298 | 5 | 1 | 8484 | 1508909336334 | 'iOS-test' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1299 --> ([ 1299 | 4 | 3 | 2931 | 1508909336377 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1300 --> ([ 1300 | 7 | 4 | 7919 | 1508909336482 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1301 --> ([ 1301 | 8 | 3 | 7314 | 1508909336603 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1302 --> ([ 1302 | 12 | 1 | 4410 | 1508909336861 | 'iOS-test' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1303 --> ([ 1303 | 2 | 4 | 5336 | 1508909337132 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1304 --> ([ 1304 | 9 | 2 | 1697 | 1508909337310 | 'iOS-test' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1305 --> ([ 1305 | 17 | 2 | 3892 | 1508909337425 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1306 --> ([ 1306 | 3 | 3 | 9014 | 1508909337671 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1307 --> ([ 1307 | 13 | 4 | 3630 | 1508909337678 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1308 --> ([ 1308 | 4 | 1 | 2269 | 1508909337729 | 'android' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1309 --> ([ 1309 | 17 | 2 | 7081 | 1508909337804 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1310 --> ([ 1310 | 5 | 4 | 2681 | 1508909337962 | 'web' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1311 --> ([ 1311 | 6 | 2 | 2966 | 1508909338082 | 'android' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1312 --> ([ 1312 | 3 | 2 | 3444 | 1508909338296 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1313 --> ([ 1313 | 8 | 3 | 3429 | 1508909338323 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1314 --> ([ 1314 | 11 | 4 | 7363 | 1508909338409 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1315 --> ([ 1315 | 3 | 2 | 3478 | 1508909338574 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1316 --> ([ 1316 | 4 | 4 | 1017 | 1508909338638 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1317 --> ([ 1317 | 0 | 1 | 4350 | 1508909338893 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1318 --> ([ 1318 | 19 | 4 | 3409 | 1508909339157 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1319 --> ([ 1319 | 13 | 2 | 5106 | 1508909339291 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1320 --> ([ 1320 | 8 | 1 | 9674 | 1508909339468 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1321 --> ([ 1321 | 10 | 2 | 9172 | 1508909339561 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1322 --> ([ 1322 | 8 | 1 | 2795 | 1508909339659 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1323 --> ([ 1323 | 17 | 4 | 5114 | 1508909339917 | 'android' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1324 --> ([ 1324 | 19 | 3 | 8849 | 1508909339944 | 'web' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1325 --> ([ 1325 | 17 | 1 | 3382 | 1508909340132 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1326 --> ([ 1326 | 16 | 1 | 3834 | 1508909340141 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1327 --> ([ 1327 | 19 | 1 | 3301 | 1508909340371 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1328 --> ([ 1328 | 3 | 4 | 2506 | 1508909340559 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1329 --> ([ 1329 | 0 | 3 | 7053 | 1508909340800 | 'android' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1330 --> ([ 1330 | -1 | 2 | 2313 | 1508909341052 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1331 --> ([ 1331 | 13 | 1 | 9912 | 1508909341243 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1332 --> ([ 1332 | 7 | 2 | 7880 | 1508909341278 | 'web' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1333 --> ([ 1333 | 15 | 3 | 8490 | 1508909341551 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1334 --> ([ 1334 | 14 | 2 | 4080 | 1508909341566 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1335 --> ([ 1335 | 3 | 1 | 1250 | 1508909341810 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1336 --> ([ 1336 | 12 | 2 | 8175 | 1508909342084 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1337 --> ([ 1337 | 3 | 3 | 8046 | 1508909342180 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1338 --> ([ 1338 | -1 | 1 | 4076 | 1508909342373 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1339 --> ([ 1339 | 1 | 1 | 7957 | 1508909342549 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1340 --> ([ 1340 | 3 | 4 | 4176 | 1508909342630 | 'iOS-test' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1341 --> ([ 1341 | 13 | 1 | 7146 | 1508909342710 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1342 --> ([ 1342 | 2 | 3 | 6812 | 1508909342743 | 'iOS-test' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1343 --> ([ 1343 | 10 | 1 | 1032 | 1508909342938 | 'ios' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1344 --> ([ 1344 | 17 | 2 | 9834 | 1508909343086 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1345 --> ([ 1345 | 2 | 3 | 7510 | 1508909343244 | 'ios' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1346 --> ([ 1346 | 5 | 2 | 3298 | 1508909343391 | 'iOS-test' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1347 --> ([ 1347 | 15 | 2 | 503 | 1508909343664 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1348 --> ([ 1348 | 0 | 1 | 6790 | 1508909343720 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1349 --> ([ 1349 | -1 | 3 | 5561 | 1508909343887 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1350 --> ([ 1350 | 9 | 1 | 9399 | 1508909343932 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1351 --> ([ 1351 | 14 | 4 | 1824 | 1508909343992 | 'ios' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1352 --> ([ 1352 | 1 | 3 | 2829 | 1508909344168 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1353 --> ([ 1353 | 8 | 4 | 7037 | 1508909344200 | 'ios' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1354 --> ([ 1354 | 12 | 3 | 663 | 1508909344446 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1355 --> ([ 1355 | 15 | 2 | 4952 | 1508909344520 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1356 --> ([ 1356 | 13 | 1 | 9301 | 1508909344625 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1357 --> ([ 1357 | 0 | 4 | 6794 | 1508909344718 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1358 --> ([ 1358 | 10 | 3 | 8565 | 1508909344728 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1359 --> ([ 1359 | 17 | 4 | 1125 | 1508909344934 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1360 --> ([ 1360 | 16 | 1 | 2478 | 1508909345149 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1361 --> ([ 1361 | 3 | 4 | 8800 | 1508909345185 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1362 --> ([ 1362 | -1 | 1 | 3228 | 1508909345238 | 'android' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1363 --> ([ 1363 | 13 | 1 | 2329 | 1508909345377 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1364 --> ([ 1364 | 3 | 1 | 3844 | 1508909345427 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1365 --> ([ 1365 | 12 | 1 | 1340 | 1508909345505 | 'web' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1366 --> ([ 1366 | 5 | 2 | 7553 | 1508909345689 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1367 --> ([ 1367 | 4 | 3 | 8470 | 1508909345824 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1368 --> ([ 1368 | 12 | 4 | 4360 | 1508909346094 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1369 --> ([ 1369 | 8 | 4 | 2393 | 1508909346157 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1370 --> ([ 1370 | -1 | 1 | 7834 | 1508909346197 | 'iOS-test' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1371 --> ([ 1371 | 5 | 4 | 7241 | 1508909346221 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1372 --> ([ 1372 | 0 | 1 | 7639 | 1508909346416 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1373 --> ([ 1373 | 16 | 3 | 229 | 1508909346448 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1374 --> ([ 1374 | 19 | 3 | 5781 | 1508909346554 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1375 --> ([ 1375 | 10 | 3 | 1665 | 1508909346717 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1376 --> ([ 1376 | 13 | 4 | 7099 | 1508909346810 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1377 --> ([ 1377 | 3 | 1 | 4178 | 1508909346894 | 'android' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1378 --> ([ 1378 | 4 | 1 | 7015 | 1508909346895 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1379 --> ([ 1379 | 5 | 4 | 361 | 1508909347019 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1380 --> ([ 1380 | 15 | 3 | 2033 | 1508909347245 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1381 --> ([ 1381 | 6 | 3 | 620 | 1508909347492 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1382 --> ([ 1382 | 3 | 4 | 999 | 1508909347582 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1383 --> ([ 1383 | 2 | 2 | 5486 | 1508909347791 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1384 --> ([ 1384 | 4 | 1 | 7644 | 1508909347834 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1385 --> ([ 1385 | 14 | 1 | 9137 | 1508909348012 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1386 --> ([ 1386 | 10 | 3 | 7404 | 1508909348273 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1387 --> ([ 1387 | 5 | 2 | 3385 | 1508909348470 | 'iOS' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1388 --> ([ 1388 | 4 | 1 | 4022 | 1508909348680 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1389 --> ([ 1389 | 10 | 2 | 3899 | 1508909348729 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1390 --> ([ 1390 | 12 | 3 | 3948 | 1508909348892 | 'iOS' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1391 --> ([ 1391 | 2 | 1 | 2920 | 1508909349082 | 'web' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1392 --> ([ 1392 | 8 | 1 | 9053 | 1508909349245 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1393 --> ([ 1393 | -1 | 3 | 5210 | 1508909349422 | 'web' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1394 --> ([ 1394 | 16 | 3 | 644 | 1508909349598 | 'ios' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1395 --> ([ 1395 | 6 | 4 | 6514 | 1508909349716 | 'web' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1396 --> ([ 1396 | 2 | 1 | 296 | 1508909349965 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1397 --> ([ 1397 | 4 | 2 | 6274 | 1508909349998 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1398 --> ([ 1398 | 13 | 2 | 5522 | 1508909350159 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1399 --> ([ 1399 | 15 | 2 | 8873 | 1508909350399 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1400 --> ([ 1400 | 9 | 3 | 777 | 1508909350643 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1401 --> ([ 1401 | 8 | 3 | 145 | 1508909350871 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1402 --> ([ 1402 | 12 | 3 | 6788 | 1508909351124 | 'web' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1403 --> ([ 1403 | 1 | 4 | 8418 | 1508909351246 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1404 --> ([ 1404 | 16 | 1 | 3761 | 1508909351487 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1405 --> ([ 1405 | 12 | 4 | 1870 | 1508909351598 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1406 --> ([ 1406 | 7 | 2 | 6420 | 1508909351885 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1407 --> ([ 1407 | -1 | 2 | 6399 | 1508909352158 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1408 --> ([ 1408 | 17 | 3 | 1440 | 1508909352262 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1409 --> ([ 1409 | 17 | 3 | 840 | 1508909352384 | 'web' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1410 --> ([ 1410 | 15 | 2 | 223 | 1508909352515 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1411 --> ([ 1411 | -1 | 4 | 8712 | 1508909352717 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1412 --> ([ 1412 | 16 | 2 | 2583 | 1508909352746 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1413 --> ([ 1413 | 1 | 1 | 8321 | 1508909352829 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1414 --> ([ 1414 | -1 | 4 | 9309 | 1508909353060 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1415 --> ([ 1415 | 12 | 2 | 5414 | 1508909353072 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1416 --> ([ 1416 | 18 | 3 | 4127 | 1508909353329 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1417 --> ([ 1417 | 15 | 1 | 5193 | 1508909353626 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1418 --> ([ 1418 | 1 | 1 | 8058 | 1508909353718 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1419 --> ([ 1419 | 2 | 3 | 2922 | 1508909354005 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1420 --> ([ 1420 | 9 | 1 | 1713 | 1508909354100 | 'iOS' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1421 --> ([ 1421 | 18 | 3 | 4875 | 1508909354322 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1422 --> ([ 1422 | 2 | 3 | 3480 | 1508909354585 | 'web' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1423 --> ([ 1423 | 3 | 4 | 3919 | 1508909354597 | 'iOS-test' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1424 --> ([ 1424 | 17 | 1 | 4166 | 1508909354600 | 'iOS-test' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1425 --> ([ 1425 | 16 | 3 | 6447 | 1508909354861 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1426 --> ([ 1426 | 15 | 1 | 1055 | 1508909355159 | 'iOS-test' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1427 --> ([ 1427 | 7 | 2 | 9952 | 1508909355275 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1428 --> ([ 1428 | 5 | 3 | 6923 | 1508909355452 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1429 --> ([ 1429 | 13 | 1 | 2980 | 1508909355710 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1430 --> ([ 1430 | 15 | 1 | 2005 | 1508909355854 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1431 --> ([ 1431 | 17 | 2 | 1724 | 1508909356004 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1432 --> ([ 1432 | 11 | 2 | 9407 | 1508909356134 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1433 --> ([ 1433 | 7 | 4 | 35 | 1508909356134 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1434 --> ([ 1434 | 10 | 4 | 7014 | 1508909356330 | 'web' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1435 --> ([ 1435 | 5 | 3 | 2072 | 1508909356597 | 'ios' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1436 --> ([ 1436 | 8 | 4 | 1342 | 1508909356623 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1437 --> ([ 1437 | 2 | 4 | 9383 | 1508909356790 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1438 --> ([ 1438 | 1 | 3 | 1397 | 1508909357067 | 'iOS' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1439 --> ([ 1439 | 6 | 1 | 6662 | 1508909357257 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1440 --> ([ 1440 | 0 | 4 | 3014 | 1508909357455 | 'ios' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1441 --> ([ 1441 | 19 | 2 | 2918 | 1508909357486 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1442 --> ([ 1442 | 9 | 2 | 6700 | 1508909357684 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1443 --> ([ 1443 | 12 | 1 | 7363 | 1508909357812 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1444 --> ([ 1444 | 19 | 4 | 4482 | 1508909357989 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1445 --> ([ 1445 | 7 | 4 | 1735 | 1508909358093 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1446 --> ([ 1446 | -1 | 2 | 3769 | 1508909358333 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1447 --> ([ 1447 | 13 | 1 | 3318 | 1508909358549 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1448 --> ([ 1448 | 1 | 3 | 8734 | 1508909358755 | 'web' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1449 --> ([ 1449 | 16 | 4 | 5447 | 1508909359001 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1450 --> ([ 1450 | 17 | 3 | 2240 | 1508909359148 | 'web' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1451 --> ([ 1451 | 9 | 3 | 4713 | 1508909359249 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1452 --> ([ 1452 | 14 | 4 | 3781 | 1508909359384 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1453 --> ([ 1453 | 2 | 1 | 6666 | 1508909359672 | 'android' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1454 --> ([ 1454 | 7 | 2 | 1492 | 1508909359810 | 'iOS' | 'your team here rocks!' ])
[35mkafka_1               |[0m [2017-10-25 05:29:19,987] INFO [GroupCoordinator 1]: Preparing to rebalance group “Consumer-Group-1” with old generation 0 (__consumer_offsets-13) (kafka.coordinator.group.GroupCoordinator)
[36;1mksql-datagen-users_1  |[0m 1455 --> ([ 1455 | 12 | 1 | 4243 | 1508909360103 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1456 --> ([ 1456 | 19 | 2 | 8698 | 1508909360391 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1457 --> ([ 1457 | 0 | 2 | 5695 | 1508909360458 | 'iOS' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1458 --> ([ 1458 | 1 | 2 | 8034 | 1508909360600 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1459 --> ([ 1459 | 10 | 2 | 1271 | 1508909360705 | 'web' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1460 --> ([ 1460 | 3 | 4 | 8625 | 1508909360844 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1461 --> ([ 1461 | 8 | 2 | 404 | 1508909361021 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1462 --> ([ 1462 | 19 | 4 | 4350 | 1508909361117 | 'iOS-test' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1463 --> ([ 1463 | 16 | 3 | 3678 | 1508909361140 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1464 --> ([ 1464 | 10 | 3 | 3690 | 1508909361162 | 'iOS' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1465 --> ([ 1465 | 15 | 1 | 6309 | 1508909361181 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1466 --> ([ 1466 | 9 | 1 | 4169 | 1508909361330 | 'android' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1467 --> ([ 1467 | 15 | 4 | 2421 | 1508909361603 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1468 --> ([ 1468 | 15 | 4 | 6893 | 1508909361745 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1469 --> ([ 1469 | 8 | 3 | 1851 | 1508909361997 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1470 --> ([ 1470 | 6 | 1 | 9202 | 1508909362140 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1471 --> ([ 1471 | 11 | 4 | 3771 | 1508909362334 | 'web' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1472 --> ([ 1472 | 11 | 1 | 5934 | 1508909362517 | 'android' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1473 --> ([ 1473 | 14 | 4 | 3853 | 1508909362668 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1474 --> ([ 1474 | 10 | 2 | 2316 | 1508909362924 | 'web' | 'meh' ])
[35mkafka_1               |[0m [2017-10-25 05:29:22,989] INFO [GroupCoordinator 1]: Stabilized group “Consumer-Group-1” generation 1 (__consumer_offsets-13) (kafka.coordinator.group.GroupCoordinator)
[35mkafka_1               |[0m [2017-10-25 05:29:22,994] INFO [GroupCoordinator 1]: Assignment received from leader for group “Consumer-Group-1” for generation 1 (kafka.coordinator.group.GroupCoordinator)
[35mkafka_1               |[0m [2017-10-25 05:29:22,995] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-13. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[36;1mksql-datagen-users_1  |[0m 1475 --> ([ 1475 | -1 | 2 | 4258 | 1508909363220 | 'iOS' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1476 --> ([ 1476 | 0 | 4 | 3981 | 1508909363467 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1477 --> ([ 1477 | 8 | 1 | 8192 | 1508909363691 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1478 --> ([ 1478 | 4 | 3 | 2141 | 1508909363882 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1479 --> ([ 1479 | 15 | 2 | 3304 | 1508909364002 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1480 --> ([ 1480 | 8 | 4 | 4383 | 1508909364198 | 'iOS-test' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1481 --> ([ 1481 | 15 | 3 | 656 | 1508909364298 | 'web' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1482 --> ([ 1482 | -1 | 3 | 4146 | 1508909364505 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1483 --> ([ 1483 | 1 | 4 | 8554 | 1508909364667 | 'android' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1484 --> ([ 1484 | 14 | 4 | 4338 | 1508909364898 | 'android' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1485 --> ([ 1485 | 9 | 2 | 2988 | 1508909365020 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1486 --> ([ 1486 | 19 | 2 | 3143 | 1508909365129 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1487 --> ([ 1487 | 2 | 3 | 8907 | 1508909365362 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1488 --> ([ 1488 | 14 | 3 | 2494 | 1508909365403 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1489 --> ([ 1489 | 0 | 2 | 9151 | 1508909365619 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1490 --> ([ 1490 | -1 | 3 | 2240 | 1508909365893 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1491 --> ([ 1491 | 3 | 1 | 233 | 1508909365935 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1492 --> ([ 1492 | 5 | 2 | 5502 | 1508909366136 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1493 --> ([ 1493 | 13 | 3 | 6764 | 1508909366161 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1494 --> ([ 1494 | 3 | 2 | 3240 | 1508909366176 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1495 --> ([ 1495 | 8 | 4 | 8786 | 1508909366279 | 'ios' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1496 --> ([ 1496 | 7 | 4 | 8515 | 1508909366561 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1497 --> ([ 1497 | 3 | 3 | 8737 | 1508909366613 | 'web' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1498 --> ([ 1498 | 6 | 4 | 3490 | 1508909366735 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1499 --> ([ 1499 | 7 | 4 | 7331 | 1508909366935 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1500 --> ([ 1500 | 14 | 2 | 4630 | 1508909367081 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1501 --> ([ 1501 | 4 | 4 | 116 | 1508909367340 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1502 --> ([ 1502 | 3 | 2 | 4165 | 1508909367407 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1503 --> ([ 1503 | 7 | 4 | 8936 | 1508909367455 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1504 --> ([ 1504 | 5 | 2 | 8271 | 1508909367579 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1505 --> ([ 1505 | 16 | 3 | 5104 | 1508909367658 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1506 --> ([ 1506 | 6 | 1 | 5012 | 1508909367918 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1507 --> ([ 1507 | 5 | 2 | 3485 | 1508909368176 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1508 --> ([ 1508 | 3 | 4 | 7531 | 1508909368436 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1509 --> ([ 1509 | 11 | 3 | 9296 | 1508909368494 | 'android' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1510 --> ([ 1510 | 13 | 1 | 520 | 1508909368504 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1511 --> ([ 1511 | 6 | 4 | 1868 | 1508909368794 | 'web' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1512 --> ([ 1512 | 9 | 2 | 443 | 1508909369026 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1513 --> ([ 1513 | 2 | 3 | 2094 | 1508909369043 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1514 --> ([ 1514 | 7 | 4 | 2554 | 1508909369265 | 'android' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1515 --> ([ 1515 | 4 | 2 | 2883 | 1508909369348 | 'web' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1516 --> ([ 1516 | 10 | 2 | 203 | 1508909369633 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1517 --> ([ 1517 | 10 | 3 | 4319 | 1508909369886 | 'iOS' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1518 --> ([ 1518 | 11 | 3 | 4424 | 1508909369911 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1519 --> ([ 1519 | 13 | 3 | 7820 | 1508909370185 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1520 --> ([ 1520 | 0 | 1 | 4347 | 1508909370328 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1521 --> ([ 1521 | 2 | 3 | 6035 | 1508909370520 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1522 --> ([ 1522 | 8 | 3 | 1403 | 1508909370520 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1523 --> ([ 1523 | 10 | 4 | 7749 | 1508909370786 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1524 --> ([ 1524 | 12 | 1 | 2860 | 1508909370946 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1525 --> ([ 1525 | 1 | 1 | 9546 | 1508909371009 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1526 --> ([ 1526 | 18 | 4 | 2684 | 1508909371270 | 'iOS' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1527 --> ([ 1527 | -1 | 1 | 8495 | 1508909371296 | 'android' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1528 --> ([ 1528 | 9 | 4 | 233 | 1508909371495 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1529 --> ([ 1529 | 10 | 4 | 2603 | 1508909371771 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1530 --> ([ 1530 | 2 | 2 | 9792 | 1508909371990 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1531 --> ([ 1531 | 1 | 1 | 8313 | 1508909372085 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1532 --> ([ 1532 | 19 | 1 | 6471 | 1508909372210 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1533 --> ([ 1533 | 5 | 2 | 6794 | 1508909372496 | 'web' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1534 --> ([ 1534 | 8 | 2 | 6439 | 1508909372671 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1535 --> ([ 1535 | 2 | 3 | 3265 | 1508909372714 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1536 --> ([ 1536 | 12 | 1 | 2048 | 1508909372739 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1537 --> ([ 1537 | 10 | 1 | 6148 | 1508909372950 | 'iOS' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1538 --> ([ 1538 | 13 | 3 | 4814 | 1508909373180 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1539 --> ([ 1539 | 19 | 3 | 5274 | 1508909373374 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1540 --> ([ 1540 | 13 | 1 | 1792 | 1508909373626 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1541 --> ([ 1541 | 2 | 3 | 916 | 1508909373736 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1542 --> ([ 1542 | 6 | 3 | 3332 | 1508909374033 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1543 --> ([ 1543 | 11 | 3 | 7897 | 1508909374283 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1544 --> ([ 1544 | 10 | 4 | 2736 | 1508909374351 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1545 --> ([ 1545 | 8 | 2 | 1555 | 1508909374579 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1546 --> ([ 1546 | 8 | 4 | 821 | 1508909374852 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1547 --> ([ 1547 | 15 | 2 | 4775 | 1508909375145 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1548 --> ([ 1548 | 18 | 2 | 896 | 1508909375300 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1549 --> ([ 1549 | 4 | 4 | 7908 | 1508909375306 | 'iOS' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1550 --> ([ 1550 | 7 | 2 | 2156 | 1508909375491 | 'web' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1551 --> ([ 1551 | 3 | 1 | 5781 | 1508909375511 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1552 --> ([ 1552 | 2 | 2 | 2010 | 1508909375683 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1553 --> ([ 1553 | 12 | 1 | 8581 | 1508909375749 | 'iOS' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1554 --> ([ 1554 | 5 | 1 | 340 | 1508909375869 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1555 --> ([ 1555 | 11 | 2 | 5875 | 1508909376002 | 'ios' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1556 --> ([ 1556 | 19 | 4 | 1209 | 1508909376197 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1557 --> ([ 1557 | 0 | 4 | 5505 | 1508909376356 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1558 --> ([ 1558 | 18 | 1 | 22 | 1508909376520 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1559 --> ([ 1559 | 1 | 4 | 1639 | 1508909376813 | 'android' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1560 --> ([ 1560 | 7 | 4 | 6641 | 1508909376842 | 'iOS-test' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1561 --> ([ 1561 | 16 | 3 | 8706 | 1508909377069 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1562 --> ([ 1562 | -1 | 4 | 7475 | 1508909377276 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1563 --> ([ 1563 | 16 | 2 | 518 | 1508909377367 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1564 --> ([ 1564 | 3 | 2 | 2042 | 1508909377610 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[35mkafka_1               |[0m [2017-10-25 05:29:37,738] INFO Rolled new log segment for 'mysql-routes-0' in 1 ms. (kafka.log.Log)
[36;1mksql-datagen-users_1  |[0m 1565 --> ([ 1565 | 1 | 1 | 6268 | 1508909377769 | 'android' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1566 --> ([ 1566 | 14 | 2 | 2010 | 1508909377902 | 'iOS' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1567 --> ([ 1567 | 16 | 3 | 7750 | 1508909377934 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1568 --> ([ 1568 | 11 | 4 | 3241 | 1508909378083 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1569 --> ([ 1569 | 8 | 4 | 4478 | 1508909378139 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1570 --> ([ 1570 | 15 | 4 | 3827 | 1508909378343 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1571 --> ([ 1571 | 16 | 2 | 6001 | 1508909378396 | 'iOS-test' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1572 --> ([ 1572 | 19 | 3 | 3627 | 1508909378588 | 'android' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1573 --> ([ 1573 | 16 | 1 | 3187 | 1508909378634 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1574 --> ([ 1574 | 6 | 1 | 3332 | 1508909378841 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1575 --> ([ 1575 | 16 | 4 | 2679 | 1508909378988 | 'iOS-test' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1576 --> ([ 1576 | 16 | 3 | 3543 | 1508909379238 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1577 --> ([ 1577 | 11 | 2 | 1351 | 1508909379390 | 'android' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1578 --> ([ 1578 | 3 | 4 | 6883 | 1508909379405 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1579 --> ([ 1579 | -1 | 2 | 9954 | 1508909379620 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1580 --> ([ 1580 | 15 | 4 | 8644 | 1508909379904 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1581 --> ([ 1581 | -1 | 4 | 7772 | 1508909380113 | 'ios' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1582 --> ([ 1582 | 14 | 3 | 3988 | 1508909380332 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1583 --> ([ 1583 | 19 | 2 | 2484 | 1508909380491 | 'iOS' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1584 --> ([ 1584 | 1 | 4 | 2187 | 1508909380628 | 'iOS' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1585 --> ([ 1585 | 16 | 4 | 2042 | 1508909380792 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1586 --> ([ 1586 | 6 | 3 | 944 | 1508909380857 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1587 --> ([ 1587 | 0 | 4 | 7802 | 1508909380924 | 'web' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1588 --> ([ 1588 | 13 | 1 | 6264 | 1508909380998 | 'iOS' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1589 --> ([ 1589 | 10 | 3 | 659 | 1508909381039 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1590 --> ([ 1590 | 14 | 4 | 5924 | 1508909381044 | 'web' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[35mkafka_1               |[0m [2017-10-25 05:29:41,060] INFO [GroupCoordinator 1]: Preparing to rebalance group “Consumer-Group-1” with old generation 1 (__consumer_offsets-13) (kafka.coordinator.group.GroupCoordinator)
[35mkafka_1               |[0m [2017-10-25 05:29:41,060] INFO [GroupCoordinator 1]: Group “Consumer-Group-1” with generation 2 is now empty (__consumer_offsets-13) (kafka.coordinator.group.GroupCoordinator)
[36;1mksql-datagen-users_1  |[0m 1591 --> ([ 1591 | 14 | 1 | 4445 | 1508909381280 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1592 --> ([ 1592 | 1 | 3 | 7000 | 1508909381403 | 'web' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1593 --> ([ 1593 | 6 | 4 | 3370 | 1508909381622 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1594 --> ([ 1594 | 3 | 3 | 517 | 1508909381655 | 'android' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1595 --> ([ 1595 | 12 | 4 | 2735 | 1508909381924 | 'iOS' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1596 --> ([ 1596 | 1 | 4 | 8671 | 1508909381962 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1597 --> ([ 1597 | -1 | 2 | 4687 | 1508909382191 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1598 --> ([ 1598 | 5 | 3 | 2692 | 1508909382219 | 'iOS-test' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1599 --> ([ 1599 | 1 | 1 | 3284 | 1508909382228 | 'web' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1600 --> ([ 1600 | 12 | 1 | 1787 | 1508909382469 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1601 --> ([ 1601 | 14 | 3 | 5604 | 1508909382648 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1602 --> ([ 1602 | 4 | 1 | 6234 | 1508909382909 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1603 --> ([ 1603 | 17 | 4 | 8182 | 1508909383184 | 'ios' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1604 --> ([ 1604 | 15 | 1 | 104 | 1508909383190 | 'web' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1605 --> ([ 1605 | 0 | 2 | 514 | 1508909383227 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1606 --> ([ 1606 | 14 | 1 | 1606 | 1508909383461 | 'web' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1607 --> ([ 1607 | 0 | 4 | 9883 | 1508909383554 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1608 --> ([ 1608 | 5 | 2 | 1601 | 1508909383760 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1609 --> ([ 1609 | 9 | 4 | 4308 | 1508909383788 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1610 --> ([ 1610 | 9 | 3 | 3259 | 1508909384082 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1611 --> ([ 1611 | 17 | 2 | 2788 | 1508909384218 | 'iOS-test' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1612 --> ([ 1612 | 13 | 2 | 8382 | 1508909384515 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1613 --> ([ 1613 | 8 | 4 | 4695 | 1508909384794 | 'iOS' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1614 --> ([ 1614 | 2 | 2 | 8399 | 1508909384952 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1615 --> ([ 1615 | 7 | 4 | 2512 | 1508909385195 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1616 --> ([ 1616 | 1 | 3 | 541 | 1508909385357 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1617 --> ([ 1617 | 18 | 3 | 4686 | 1508909385592 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1618 --> ([ 1618 | 5 | 3 | 2190 | 1508909385798 | 'android' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1619 --> ([ 1619 | 11 | 1 | 7869 | 1508909385848 | 'iOS' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1620 --> ([ 1620 | -1 | 4 | 8948 | 1508909386116 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1621 --> ([ 1621 | 0 | 3 | 296 | 1508909386337 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1622 --> ([ 1622 | 3 | 2 | 4391 | 1508909386412 | 'ios' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1623 --> ([ 1623 | 7 | 3 | 536 | 1508909386640 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1624 --> ([ 1624 | 16 | 4 | 2023 | 1508909386754 | 'android' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1625 --> ([ 1625 | 14 | 4 | 8427 | 1508909386912 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1626 --> ([ 1626 | 2 | 2 | 2471 | 1508909387051 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1627 --> ([ 1627 | 9 | 2 | 7401 | 1508909387073 | 'ios' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1628 --> ([ 1628 | 5 | 1 | 1396 | 1508909387318 | 'iOS-test' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1629 --> ([ 1629 | 5 | 1 | 9968 | 1508909387463 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1630 --> ([ 1630 | 4 | 1 | 6131 | 1508909387576 | 'android' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1631 --> ([ 1631 | 5 | 2 | 647 | 1508909387774 | 'iOS-test' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1632 --> ([ 1632 | 19 | 3 | 1571 | 1508909387781 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1633 --> ([ 1633 | 0 | 3 | 3057 | 1508909387848 | 'android' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1634 --> ([ 1634 | 0 | 2 | 8301 | 1508909387853 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1635 --> ([ 1635 | 9 | 2 | 477 | 1508909388041 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1636 --> ([ 1636 | 17 | 4 | 8496 | 1508909388289 | 'ios' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1637 --> ([ 1637 | 12 | 2 | 1567 | 1508909388355 | 'iOS' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1638 --> ([ 1638 | 9 | 2 | 7567 | 1508909388605 | 'android' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1639 --> ([ 1639 | 7 | 1 | 5582 | 1508909388786 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1640 --> ([ 1640 | 14 | 4 | 2098 | 1508909388823 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1641 --> ([ 1641 | 10 | 1 | 3774 | 1508909389017 | 'android' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1642 --> ([ 1642 | 10 | 3 | 7045 | 1508909389052 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1643 --> ([ 1643 | 16 | 2 | 6278 | 1508909389185 | 'android' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1644 --> ([ 1644 | 11 | 4 | 7746 | 1508909389270 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1645 --> ([ 1645 | 12 | 2 | 6226 | 1508909389453 | 'ios' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1646 --> ([ 1646 | 8 | 3 | 4704 | 1508909389516 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1647 --> ([ 1647 | 18 | 1 | 5345 | 1508909389665 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1648 --> ([ 1648 | 9 | 1 | 1686 | 1508909389942 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1649 --> ([ 1649 | 3 | 1 | 7802 | 1508909390119 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1650 --> ([ 1650 | 11 | 3 | 134 | 1508909390154 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1651 --> ([ 1651 | 8 | 3 | 1559 | 1508909390170 | 'web' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1652 --> ([ 1652 | 14 | 3 | 5389 | 1508909390276 | 'web' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1653 --> ([ 1653 | 14 | 4 | 8969 | 1508909390282 | 'iOS' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1654 --> ([ 1654 | 16 | 2 | 2754 | 1508909390389 | 'android' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1655 --> ([ 1655 | 18 | 1 | 924 | 1508909390488 | 'web' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1656 --> ([ 1656 | 2 | 3 | 3874 | 1508909390771 | 'web' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1657 --> ([ 1657 | 12 | 4 | 1237 | 1508909390980 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1658 --> ([ 1658 | 6 | 2 | 6476 | 1508909391010 | 'iOS-test' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1659 --> ([ 1659 | 9 | 2 | 598 | 1508909391183 | 'iOS-test' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1660 --> ([ 1660 | 5 | 2 | 2954 | 1508909391310 | 'ios' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1661 --> ([ 1661 | 19 | 4 | 1798 | 1508909391413 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1662 --> ([ 1662 | 9 | 4 | 5803 | 1508909391452 | 'iOS' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1663 --> ([ 1663 | 7 | 4 | 512 | 1508909391736 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1664 --> ([ 1664 | 4 | 3 | 440 | 1508909392020 | 'ios' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1665 --> ([ 1665 | 8 | 4 | 8382 | 1508909392253 | 'ios' | 'thank you for the most friendly, helpful experience today at your new lounge' ])
[36;1mksql-datagen-users_1  |[0m 1666 --> ([ 1666 | 10 | 4 | 8373 | 1508909392517 | 'iOS' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1667 --> ([ 1667 | 2 | 1 | 7682 | 1508909392583 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1668 --> ([ 1668 | 6 | 3 | 6284 | 1508909392745 | 'android' | '(expletive deleted)' ])
[36;1mksql-datagen-users_1  |[0m 1669 --> ([ 1669 | 6 | 3 | 9286 | 1508909392979 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1670 --> ([ 1670 | 14 | 4 | 8964 | 1508909393001 | 'web' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1671 --> ([ 1671 | 14 | 2 | 8230 | 1508909393273 | 'web' | 'why is it so difficult to keep the bathrooms clean ?' ])
[33;1mconnect_1             |[0m [2017-10-25 05:29:53,401] INFO Finished WorkerSourceTask{id=MySQLSource-0} commitOffsets successfully in 1477 ms (org.apache.kafka.connect.runtime.WorkerSourceTask)
[36;1mksql-datagen-users_1  |[0m 1672 --> ([ 1672 | 14 | 1 | 6678 | 1508909393539 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1673 --> ([ 1673 | 0 | 3 | 3611 | 1508909393697 | 'ios' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1674 --> ([ 1674 | 6 | 3 | 9918 | 1508909393811 | 'iOS-test' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1675 --> ([ 1675 | 14 | 4 | 2078 | 1508909394109 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1676 --> ([ 1676 | 16 | 2 | 7078 | 1508909394159 | 'iOS' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1677 --> ([ 1677 | 17 | 3 | 6685 | 1508909394236 | 'android' | 'meh' ])
[36;1mksql-datagen-users_1  |[0m 1678 --> ([ 1678 | 14 | 1 | 3039 | 1508909394276 | 'iOS' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1679 --> ([ 1679 | 6 | 4 | 5353 | 1508909394384 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1680 --> ([ 1680 | 11 | 3 | 9194 | 1508909394639 | 'iOS-test' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1681 --> ([ 1681 | 17 | 2 | 5696 | 1508909394889 | 'web' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1682 --> ([ 1682 | 13 | 4 | 7908 | 1508909394928 | 'android' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1683 --> ([ 1683 | 15 | 4 | 5489 | 1508909395226 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1684 --> ([ 1684 | 6 | 3 | 6915 | 1508909395460 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1685 --> ([ 1685 | 5 | 4 | 507 | 1508909395539 | 'ios' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1686 --> ([ 1686 | 9 | 3 | 1673 | 1508909395812 | 'ios' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1687 --> ([ 1687 | 11 | 2 | 4015 | 1508909395823 | 'ios' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1688 --> ([ 1688 | -1 | 3 | 6178 | 1508909395829 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1689 --> ([ 1689 | 0 | 1 | 4666 | 1508909395880 | 'android' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1690 --> ([ 1690 | 9 | 4 | 5252 | 1508909396115 | 'web' | 'is this as good as it gets? really ?' ])
[36;1mksql-datagen-users_1  |[0m 1691 --> ([ 1691 | 15 | 1 | 2042 | 1508909396192 | 'iOS-test' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1692 --> ([ 1692 | 11 | 1 | 3791 | 1508909396221 | 'android' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1693 --> ([ 1693 | -1 | 3 | 1627 | 1508909396292 | 'ios' | 'airport refurb looks great, will fly outta here more!' ])
[36;1mksql-datagen-users_1  |[0m 1694 --> ([ 1694 | 10 | 1 | 1014 | 1508909396541 | 'iOS' | 'your team here rocks!' ])
[36;1mksql-datagen-users_1  |[0m 1695 --> ([ 1695 | 1 | 1 | 3447 | 1508909396790 | 'iOS' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1696 --> ([ 1696 | 15 | 2 | 190 | 1508909396854 | 'android' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1697 --> ([ 1697 | 8 | 4 | 1145 | 1508909397080 | 'web' | 'Surprisingly good, maybe you are getting your mojo back at long last!' ])
[36;1mksql-datagen-users_1  |[0m 1698 --> ([ 1698 | 7 | 4 | 6876 | 1508909397090 | 'android' | 'more peanuts please' ])
[36;1mksql-datagen-users_1  |[0m 1699 --> ([ 1699 | 18 | 1 | 749 | 1508909397377 | 'ios' | 'Exceeded all my expectations. Thank you !' ])
[36;1mksql-datagen-users_1  |[0m 1700 --> ([ 1700 | 8 | 3 | 1152 | 1508909397396 | 'web' | 'worst. flight. ever. #neveragain' ])
[36;1mksql-datagen-users_1  |[0m 1701 --> ([ 1701 | 2 | 3 | 882 | 1508909397613 | 'iOS-test' | 'why is it so difficult to keep the bathrooms clean ?' ])
[36;1mksql-datagen-users_1  |[0m 1702 --> ([ 1702 | 9 | 3 | 1872 | 1508909397721 | 'ios' | 'meh' ])
Gracefully stopping... (press Ctrl+C again to force)
